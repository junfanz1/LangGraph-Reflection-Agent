{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "byPgKYhAE6gn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "os.environ['TAVILY_API_KEY'] = '' # Get a free key here: https://app.tavily.com"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gpt-researcher nest_asyncio"
      ],
      "metadata": {
        "id": "-rXET3OZLxwH",
        "outputId": "136b8dcc-ab48-432e-bafc-c25800411144",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpt-researcher\n",
            "  Downloading gpt_researcher-0.12.7-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gpt-researcher) (4.13.3)\n",
            "Collecting colorama (from gpt-researcher)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting md2pdf (from gpt-researcher)\n",
            "  Downloading md2pdf-1.0.1.tar.gz (6.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting python-dotenv (from gpt-researcher)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from gpt-researcher) (6.0.2)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from gpt-researcher) (2.10.6)\n",
            "Collecting python-multipart (from gpt-researcher)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (from gpt-researcher) (3.7)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (from gpt-researcher) (0.3.19)\n",
            "Collecting langchain-community (from gpt-researcher)\n",
            "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tiktoken (from gpt-researcher)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting arxiv (from gpt-researcher)\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting PyMuPDF (from gpt-researcher)\n",
            "  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gpt-researcher) (2.32.3)\n",
            "Collecting aiofiles (from gpt-researcher)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: mistune in /usr/local/lib/python3.11/dist-packages (from gpt-researcher) (3.1.2)\n",
            "Collecting python-docx (from gpt-researcher)\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting htmldocx (from gpt-researcher)\n",
            "  Downloading htmldocx-0.0.6-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting lxml-html-clean (from gpt-researcher)\n",
            "  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.11/dist-packages (from gpt-researcher) (14.2)\n",
            "Collecting unstructured (from gpt-researcher)\n",
            "  Downloading unstructured-0.16.23-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting json-repair (from gpt-researcher)\n",
            "  Downloading json_repair-0.39.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting json5 (from gpt-researcher)\n",
            "  Downloading json5-0.10.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting loguru (from gpt-researcher)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv->gpt-researcher)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gpt-researcher) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gpt-researcher) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gpt-researcher) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gpt-researcher) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gpt-researcher) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gpt-researcher) (4.12.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx->gpt-researcher) (5.3.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain->gpt-researcher) (0.3.40)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain->gpt-researcher) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain->gpt-researcher) (0.3.11)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain->gpt-researcher) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain->gpt-researcher) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain->gpt-researcher) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain->gpt-researcher) (1.26.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->gpt-researcher) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->gpt-researcher) (2.27.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community->gpt-researcher)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community->gpt-researcher)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community->gpt-researcher)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting markdown2 (from md2pdf->gpt-researcher)\n",
            "  Downloading markdown2-2.5.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting WeasyPrint (from md2pdf->gpt-researcher)\n",
            "  Downloading weasyprint-64.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting docopt (from md2pdf->gpt-researcher)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->gpt-researcher) (2024.11.6)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured->gpt-researcher) (5.2.0)\n",
            "Collecting filetype (from unstructured->gpt-researcher)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured->gpt-researcher)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured->gpt-researcher) (3.9.1)\n",
            "Collecting emoji (from unstructured->gpt-researcher)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting python-iso639 (from unstructured->gpt-researcher)\n",
            "  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langdetect (from unstructured->gpt-researcher)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rapidfuzz (from unstructured->gpt-researcher)\n",
            "  Downloading rapidfuzz-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting backoff (from unstructured->gpt-researcher)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting unstructured-client (from unstructured->gpt-researcher)\n",
            "  Downloading unstructured_client-0.30.6-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured->gpt-researcher) (1.17.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unstructured->gpt-researcher) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured->gpt-researcher) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured->gpt-researcher)\n",
            "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured->gpt-researcher) (1.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->gpt-researcher) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->gpt-researcher) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->gpt-researcher) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->gpt-researcher) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->gpt-researcher) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->gpt-researcher) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->gpt-researcher) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community->gpt-researcher)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community->gpt-researcher)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv->gpt-researcher)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain->gpt-researcher) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain->gpt-researcher) (24.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->gpt-researcher) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->gpt-researcher) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->gpt-researcher) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->gpt-researcher) (0.23.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain->gpt-researcher) (3.1.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured->gpt-researcher) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured->gpt-researcher) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured->gpt-researcher) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured->gpt-researcher) (1.4.2)\n",
            "Collecting olefile (from python-oxmsg->unstructured->gpt-researcher)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured->gpt-researcher) (43.0.3)\n",
            "Collecting eval-type-backport>=0.2.0 (from unstructured-client->unstructured->gpt-researcher)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pypdf>=4.0 (from unstructured-client->unstructured->gpt-researcher)\n",
            "  Downloading pypdf-5.3.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured->gpt-researcher) (2.8.2)\n",
            "Collecting pydyf>=0.11.0 (from WeasyPrint->md2pdf->gpt-researcher)\n",
            "  Downloading pydyf-0.11.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: cffi>=0.6 in /usr/local/lib/python3.11/dist-packages (from WeasyPrint->md2pdf->gpt-researcher) (1.17.1)\n",
            "Collecting tinyhtml5>=2.0.0b1 (from WeasyPrint->md2pdf->gpt-researcher)\n",
            "  Downloading tinyhtml5-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: tinycss2>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from WeasyPrint->md2pdf->gpt-researcher) (1.4.0)\n",
            "Collecting cssselect2>=0.1 (from WeasyPrint->md2pdf->gpt-researcher)\n",
            "  Downloading cssselect2-0.7.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting Pyphen>=0.9.1 (from WeasyPrint->md2pdf->gpt-researcher)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1.0 in /usr/local/lib/python3.11/dist-packages (from WeasyPrint->md2pdf->gpt-researcher) (11.1.0)\n",
            "Requirement already satisfied: fonttools>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from fonttools[woff]>=4.0.0->WeasyPrint->md2pdf->gpt-researcher) (4.56.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=0.6->WeasyPrint->md2pdf->gpt-researcher) (2.22)\n",
            "Collecting brotli>=1.0.1 (from fonttools[woff]>=4.0.0->WeasyPrint->md2pdf->gpt-researcher)\n",
            "  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting zopfli>=0.1.4 (from fonttools[woff]>=4.0.0->WeasyPrint->md2pdf->gpt-researcher)\n",
            "  Downloading zopfli-0.2.3.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->gpt-researcher) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->gpt-researcher) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->gpt-researcher) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain->gpt-researcher) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->gpt-researcher)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->gpt-researcher) (1.3.1)\n",
            "Downloading gpt_researcher-0.12.7-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.4/161.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading htmldocx-0.0.6-py3-none-any.whl (9.5 kB)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.39.1-py3-none-any.whl (20 kB)\n",
            "Downloading json5-0.10.0-py3-none-any.whl (34 kB)\n",
            "Downloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured-0.16.23-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading markdown2-2.5.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.30.6-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading weasyprint-64.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect2-0.7.0-py3-none-any.whl (15 kB)\n",
            "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydyf-0.11.0-py3-none-any.whl (8.1 kB)\n",
            "Downloading pypdf-5.3.0-py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tinyhtml5-2.0.0-py3-none-any.whl (39 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading zopfli-0.2.3.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (850 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m850.6/850.6 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: md2pdf, docopt, langdetect, sgmllib3k\n",
            "  Building wheel for md2pdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for md2pdf: filename=md2pdf-1.0.1-py2.py3-none-any.whl size=6003 sha256=a11a7fd006320e24ee587a1a8d2ed4b67c618f3e514773f95d9fcbf54a25f52c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/c3/29/09f3661b6b4eceb291241be520bb55b12b2a6714af18ae1510\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=0e17b4c3e9f8ab460fca4332d3c561688483ffa78e1a21c0c57545dc87709cfa\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=0b4c6a93ebcb4f0c0badf86632576bcee2daea22a852dbdc3462227627524d12\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=3952136d1476b1ab9693922b3ce9af66c758a75a708c0ab54550fe3c34ee8b27\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built md2pdf docopt langdetect sgmllib3k\n",
            "Installing collected packages: sgmllib3k, filetype, docopt, brotli, zopfli, tinyhtml5, rapidfuzz, python-multipart, python-magic, python-iso639, python-dotenv, python-docx, Pyphen, pypdf, PyMuPDF, pydyf, olefile, mypy-extensions, marshmallow, markdown2, lxml-html-clean, loguru, langdetect, json5, json-repair, httpx-sse, feedparser, eval-type-backport, emoji, colorama, backoff, aiofiles, typing-inspect, tiktoken, python-oxmsg, htmldocx, cssselect2, arxiv, WeasyPrint, unstructured-client, pydantic-settings, dataclasses-json, unstructured, md2pdf, langchain-community, gpt-researcher\n",
            "Successfully installed PyMuPDF-1.25.3 Pyphen-0.17.2 WeasyPrint-64.1 aiofiles-24.1.0 arxiv-2.1.3 backoff-2.2.1 brotli-1.1.0 colorama-0.4.6 cssselect2-0.7.0 dataclasses-json-0.6.7 docopt-0.6.2 emoji-2.14.1 eval-type-backport-0.2.2 feedparser-6.0.11 filetype-1.2.0 gpt-researcher-0.12.7 htmldocx-0.0.6 httpx-sse-0.4.0 json-repair-0.39.1 json5-0.10.0 langchain-community-0.3.18 langdetect-1.0.9 loguru-0.7.3 lxml-html-clean-0.4.1 markdown2-2.5.3 marshmallow-3.26.1 md2pdf-1.0.1 mypy-extensions-1.0.0 olefile-0.47 pydantic-settings-2.8.1 pydyf-0.11.0 pypdf-5.3.0 python-docx-1.1.2 python-dotenv-1.0.1 python-iso639-2025.2.18 python-magic-0.4.27 python-multipart-0.0.20 python-oxmsg-0.0.2 rapidfuzz-3.12.1 sgmllib3k-1.0.0 tiktoken-0.9.0 tinyhtml5-2.0.0 typing-inspect-0.9.0 unstructured-0.16.23 unstructured-client-0.30.6 zopfli-0.2.3.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain-openai"
      ],
      "metadata": {
        "id": "75ccbsjMrMSm",
        "outputId": "6809cb46-5e3f-40e8-932c-2a5fc792be9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.7-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.39 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.40)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.61.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (0.3.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (2.10.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.39->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.39->langchain-openai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.39->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.39->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.39->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.39->langchain-openai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
            "Downloading langchain_openai-0.3.7-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio # required for notebooks\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from gpt_researcher import GPTResearcher\n",
        "import asyncio\n",
        "\n",
        "async def get_report(query: str, report_type: str) -> str:\n",
        "    researcher = GPTResearcher(query, report_type)\n",
        "    research_result = await researcher.conduct_research()\n",
        "    report = await researcher.write_report()\n",
        "\n",
        "    # Get additional information\n",
        "    research_context = researcher.get_research_context()\n",
        "    research_costs = researcher.get_costs()\n",
        "    research_images = researcher.get_research_images()\n",
        "    research_sources = researcher.get_research_sources()\n",
        "\n",
        "    return report, research_context, research_costs, research_images, research_sources\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    query = \"Why DeepSeek's MoE and GRPO is a successful architecture in LLM research and application?\"\n",
        "    report_type = \"research_report\"\n",
        "\n",
        "    report, context, costs, images, sources = asyncio.run(get_report(query, report_type))\n",
        "\n",
        "    print(\"Report:\")\n",
        "    print(report)\n",
        "    print(\"\\nResearch Costs:\")\n",
        "    print(costs)\n",
        "    print(\"\\nResearch Images:\")\n",
        "    print(images)\n",
        "    print(\"\\nResearch Sources:\")\n",
        "    print(sources)"
      ],
      "metadata": {
        "id": "KWZe2InrL0ji",
        "outputId": "8c327e94-3335-4917-b039-442d743593a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     [22:28:35] 🔍 Starting the research task for 'Why DeepSeek's MoE and GRPO is a successful architecture in LLM research and application?'...\n",
            "INFO:     [22:28:35] 🤖 AI Research Agent\n",
            "INFO:     [22:28:35] 🌐 Browsing the web to learn more about the task: Why DeepSeek's MoE and GRPO is a successful architecture in LLM research and application?...\n",
            "INFO:     [22:28:39] 🤔 Planning the research strategy and subtasks...\n",
            "WARNING:gpt_researcher.actions.query_processing:Error with strategic LLM: Error code: 404 - {'error': {'message': 'The model `o3-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Retrying with max_tokens=4000.\n",
            "WARNING:gpt_researcher.actions.query_processing:See https://github.com/assafelovic/gpt-researcher/issues/1022\n",
            "WARNING:gpt_researcher.actions.query_processing:Retrying with max_tokens=4000 failed.\n",
            "WARNING:gpt_researcher.actions.query_processing:Error with strategic LLM: Error code: 404 - {'error': {'message': 'The model `o3-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}. Falling back to smart LLM.\n",
            "INFO:     [22:28:41] 🗂️ I will conduct my research based on the following queries: ['DeepSeek Mixture of Experts MoE architecture scalability and efficiency', 'Group Relative Policy Optimization GRPO reinforcement learning in DeepSeek LLMs', 'DeepSeek-V3 advances in MoE and GRPO for reasoning and cost-efficiency', 'How DeepSeek combines MoE and GRPO for successful LLM applications', \"Why DeepSeek's MoE and GRPO is a successful architecture in LLM research and application?\"]...\n",
            "INFO:     [22:28:41] \n",
            "🔍 Running research for 'DeepSeek Mixture of Experts MoE architecture scalability and efficiency'...\n",
            "INFO:     [22:28:41] \n",
            "🔍 Running research for 'Group Relative Policy Optimization GRPO reinforcement learning in DeepSeek LLMs'...\n",
            "INFO:     [22:28:41] \n",
            "🔍 Running research for 'DeepSeek-V3 advances in MoE and GRPO for reasoning and cost-efficiency'...\n",
            "INFO:     [22:28:41] \n",
            "🔍 Running research for 'How DeepSeek combines MoE and GRPO for successful LLM applications'...\n",
            "INFO:     [22:28:41] \n",
            "🔍 Running research for 'Why DeepSeek's MoE and GRPO is a successful architecture in LLM research and application?'...\n",
            "INFO:     [22:28:42] ✅ Added source url to research: https://medium.com/@sahin.samia/the-math-behind-deepseek-a-deep-dive-into-group-relative-policy-optimization-grpo-8a75007491ba\n",
            "\n",
            "INFO:     [22:28:42] ✅ Added source url to research: https://epichka.com/blog/2025/grpo/\n",
            "\n",
            "INFO:     [22:28:42] ✅ Added source url to research: https://blog.dataopslabs.com/deepseek-r1-efficient-reinforcement-learning-with-grpo\n",
            "\n",
            "INFO:     [22:28:42] ✅ Added source url to research: https://medium.com/@joshithagandra/understanding-group-relative-policy-optimization-grpo-powering-deepseekmath-and-deepseek-r1-df141c94c666\n",
            "\n",
            "INFO:     [22:28:42] ✅ Added source url to research: https://medium.com/@magalareuben60/group-relative-policy-optimisation-grpo-the-reinforcement-learning-algorithm-behind-deepseek-954588a0ba07\n",
            "\n",
            "INFO:     [22:28:42] 🤔 Researching for relevant information across multiple sources...\n",
            "\n",
            "INFO:     [22:28:42] 🌐 Scraping content from 5 URLs...\n",
            "INFO:     [22:28:43] ✅ Added source url to research: https://dev.to/sayed_ali_alkamel/deepseek-and-the-power-of-mixture-of-experts-moe-ham\n",
            "\n",
            "INFO:     [22:28:43] ✅ Added source url to research: https://medium.com/aimonks/deepseek-v3-efficient-and-scalable-ai-with-mixture-of-experts-8bd945b5ea3f\n",
            "\n",
            "INFO:     [22:28:43] ✅ Added source url to research: https://medium.com/@amirhossein_dehghaniazar/deepseek-and-mixture-of-experts-revolutionizing-ai-efficiency-and-speed-1ce1f931e45c\n",
            "\n",
            "INFO:     [22:28:43] ✅ Added source url to research: https://adasci.org/deepseek-v3-explained-optimizing-efficiency-and-scale/\n",
            "\n",
            "INFO:     [22:28:43] ✅ Added source url to research: https://www.modular.com/ai-resources/exploring-deepseek-r1-s-mixture-of-experts-model-architecture\n",
            "\n",
            "INFO:     [22:28:43] 🤔 Researching for relevant information across multiple sources...\n",
            "\n",
            "INFO:     [22:28:43] 🌐 Scraping content from 5 URLs...\n",
            "INFO:     [22:28:43] ✅ Added source url to research: https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/\n",
            "\n",
            "INFO:     [22:28:43] ✅ Added source url to research: https://medium.com/@yuvrajsagar117/understanding-grpo-key-ingredient-behind-deepseek-r1s-success-5f8f05494e6d\n",
            "\n",
            "INFO:     [22:28:43] ✅ Added source url to research: https://www.linkedin.com/pulse/grpo-game-changer-deepseeks-llms-sandeep-k-dzx2f\n",
            "\n",
            "INFO:     [22:28:43] 🤔 Researching for relevant information across multiple sources...\n",
            "\n",
            "INFO:     [22:28:43] 🌐 Scraping content from 3 URLs...\n",
            "INFO:     [22:28:43] ✅ Added source url to research: https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/\n",
            "\n",
            "INFO:     [22:28:43] ✅ Added source url to research: https://medium.com:8443/yugen-ai-technology-blog/understanding-the-math-behind-grpo-deepseek-r1-zero-9fb15e103a0a\n",
            "\n",
            "INFO:     [22:28:43] ✅ Added source url to research: https://opdeepseek.com/deepseek-moe/\n",
            "\n",
            "INFO:     [22:28:43] 🤔 Researching for relevant information across multiple sources...\n",
            "\n",
            "INFO:     [22:28:43] 🌐 Scraping content from 3 URLs...\n",
            "INFO:     [22:28:43] ✅ Added source url to research: https://www.deeplearning.ai/the-batch/deepseek-v3-redefines-llm-performance-and-cost-efficiency/\n",
            "\n",
            "INFO:     [22:28:43] ✅ Added source url to research: https://lzwjava.github.io/deepseek-v3-en\n",
            "\n",
            "INFO:     [22:28:43] ✅ Added source url to research: https://medium.com/yugen-ai-technology-blog/deepseek-v3-advances-in-moe-load-balancing-and-multi-token-prediction-training-f6d68c59749c\n",
            "\n",
            "INFO:     [22:28:43] ✅ Added source url to research: https://martinfowler.com/articles/deepseek-papers.html\n",
            "\n",
            "INFO:     [22:28:43] 🤔 Researching for relevant information across multiple sources...\n",
            "\n",
            "INFO:     [22:28:43] 🌐 Scraping content from 4 URLs...\n",
            "INFO:     [22:28:43] 📄 Scraped 5 pages of content\n",
            "INFO:     [22:28:43] 🖼️ Selected 0 new images from 0 total images\n",
            "INFO:     [22:28:43] 🌐 Scraping complete\n",
            "INFO:     [22:28:43] 📚 Getting relevant content based on query: Group Relative Policy Optimization GRPO reinforcement learning in DeepSeek LLMs...\n",
            "INFO:     [22:28:44] 📄 Scraped 4 pages of content\n",
            "INFO:     [22:28:44] 🖼️ Selected 0 new images from 0 total images\n",
            "INFO:     [22:28:44] 🌐 Scraping complete\n",
            "INFO:     [22:28:44] 📚 Getting relevant content based on query: DeepSeek-V3 advances in MoE and GRPO for reasoning and cost-efficiency...\n",
            "INFO:     [22:28:44] 📄 Scraped 5 pages of content\n",
            "INFO:     [22:28:44] 🖼️ Selected 4 new images from 7 total images\n",
            "INFO:     [22:28:44] 🌐 Scraping complete\n",
            "INFO:     [22:28:44] 📚 Getting relevant content based on query: DeepSeek Mixture of Experts MoE architecture scalability and efficiency...\n",
            "INFO:     [22:28:44] 📄 Scraped 3 pages of content\n",
            "INFO:     [22:28:44] 🖼️ Selected 3 new images from 3 total images\n",
            "INFO:     [22:28:44] 🌐 Scraping complete\n",
            "INFO:     [22:28:44] 📚 Getting relevant content based on query: How DeepSeek combines MoE and GRPO for successful LLM applications...\n",
            "INFO:     [22:28:46] 📃 Source: https://medium.com/yugen-ai-technology-blog/deepseek-v3-advances-in-moe-load-balancing-and-multi-token-prediction-training-f6d68c59749c\n",
            "Title: DeepSeek-V3 — Advances in MoE Load Balancing and Multi-Token Prediction Training | by Yugen.ai | Yugen.ai Technology Blog | Jan, 2025 | Medium\n",
            "Content: DeepSeek-V3 — Advances in MoE Load Balancing and Multi-Token Prediction Training | by Yugen.ai | Yugen.ai Technology Blog | Jan, 2025 | Medium\n",
            "Open in app\n",
            "Sign up\n",
            "Sign in\n",
            "Write\n",
            "Sign up\n",
            "Sign in\n",
            "Featured\n",
            "DeepSeek-V3 — Advances in MoE Load Balancing and Multi-Token Prediction Training\n",
            "Yugen.ai\n",
            "·\n",
            "Follow\n",
            "Published in\n",
            "Yugen.ai Technology Blog\n",
            "·\n",
            "8 min read\n",
            "·\n",
            "Jan 4, 2025\n",
            "--\n",
            "Listen\n",
            "Share\n",
            "DeepSeek-V3 benchmarks (\n",
            "source\n",
            ")\n",
            "By\n",
            "Soumanta Das\n",
            "To read Part II of our DeepSeek series, which focuses on Reinforcement Learning & Policy update algorithms such as PPO (Clip, Penalty) and GRPO, see\n",
            "Understanding the Math Behind GRPO — DeepSeek-R1-Zero\n",
            "The Policy Optimisation algo in DeepSeekMath & DeepSeek’s first gen reasoning models.\n",
            "medium.com\n",
            "Introduction\n",
            "\n",
            "Source: https://medium.com/yugen-ai-technology-blog/deepseek-v3-advances-in-moe-load-balancing-and-multi-token-prediction-training-f6d68c59749c\n",
            "Title: DeepSeek-V3 — Advances in MoE Load Balancing and Multi-Token Prediction Training | by Yugen.ai | Yugen.ai Technology Blog | Jan, 2025 | Medium\n",
            "Content: DeepSeek-V3's primary goal with MTP is to improve training. During inference, the additional MTP modules can be discarded, and the main model can operate independently. The MTP modules can be used for speculative decoding, which is similar to what has been proposed in the\n",
            "EAGLE paper\n",
            ".\n",
            "DeepSeek’s recent release is a huge win for the open source community and is a significant step in bridging the gap between open and closed source models.\n",
            "References\n",
            "DeepSeek-V3\n",
            "Github Repo\n",
            "DeepSeek-V3\n",
            "technical report\n",
            "Auxiliary-Loss-Free Load Balancing —\n",
            "ArXiv paper\n",
            "Understanding the Math Behind GRPO — DeepSeek-R1-Zero\n",
            "The Policy Optimisation algo in DeepSeekMath & DeepSeek’s first gen reasoning models.\n",
            "medium.com\n",
            "Hope you enjoyed the read. Our team keeps publishing such deep drives in the\n",
            "Yugen Tech Blog\n",
            "— do consider giving our publication a follow if you’re interested.\n",
            "Alternatively, I’m happy to connect on\n",
            "LinkedIn\n",
            "or\n",
            "X\n",
            "and talk.\n",
            "Deepseek\n",
            "Llm Architecture\n",
            "Mixture Of Experts\n",
            "Published in\n",
            "\n",
            "Source: https://martinfowler.com/articles/deepseek-papers.html\n",
            "Title: The DeepSeek Series: A Technical Overview\n",
            "Content: Multi-Head Latent Attention (MLA)\n",
            "DeepSeekMoE: Sparsely Activated FFNs\n",
            "Training & Outcomes\n",
            "DeepSeek-V3: HPC Co-Design\n",
            "Scaling MoE to 671B While Preserving Efficiency\n",
            "Refined MLA\n",
            "Refined DeepSeekMoE: Auxiliary-Loss-Free, Higher Capacity\n",
            "Co-Designed Frameworks: FP8, DualPipe, and PTX Optimizations\n",
            "Outcomes\n",
            "DeepSeek-R1: Reinforcement Learning for Deeper Reasoning\n",
            "Emergent Reasoning Behaviors Through RL-Only\n",
            "Refined Reasoning Through SFT + RL\n",
            "Connecting the Arcs: Efficiency & Emergence\n",
            "This article provides a cohesive overview of four technical reports from DeepSeek:\n",
            "DeepSeek-LLM\n",
            "(Jan '24)\n",
            ": an early investigation of scaling laws and data-model tradeoffs.\n",
            "DeepSeek-V2\n",
            "(Jun '24)\n",
            ": introducing Multi-Head Latent Attention (MLA) and DeepSeekMoE to improve memory and training efficiency.\n",
            "DeepSeek-V3\n",
            "(Dec '24)\n",
            ": scaling sparse MoE networks to 671B parameters, with FP8 mixed precision training and intricate HPC co-design\n",
            "DeepSeek-R1\n",
            "(Jan '25)\n",
            "\n",
            "Source: https://martinfowler.com/articles/deepseek-papers.html\n",
            "Title: The DeepSeek Series: A Technical Overview\n",
            "Content: 671B\n",
            "parameters (\n",
            "37B activated\n",
            "), training on 14.8T tokens in under\n",
            "2.8M H800 GPU hours\n",
            ". The authors credit extensive HPC co-design:\n",
            "Lastly, we emphasize again the economical training costs of DeepSeek-V3, summarized in Table 1, achieved through our optimized co-design of algorithms, frameworks, and hardware.\n",
            "--\n",
            "DeepSeek-V3 Tech. Report, p.5\n",
            "The major novelties are:\n",
            "Refined MLA\n",
            "Refined DeepSeekMoE\n",
            "Co-Designed Training & Inference Frameworks\n",
            "Refined MLA\n",
            "Multi-Head Latent Attention was introduced in V2 to reduce KV cache overhead. In V3, it is further refined with several new features:\n",
            "Dynamic Low-Rank Projection\n",
            ": Instead of a static compression dimension, MLA adjusts how strongly it compresses Key/Value vectors depending on sequence length. For shorter sequences, less compression preserves fidelity; for extremely long sequences (32K–128K tokens), deeper compression manages memory growth.\n",
            "Adaptive Query Compression\n",
            "\n",
            "Source: https://lzwjava.github.io/deepseek-v3-en\n",
            "Title: DeepSeek V3\n",
            "Content: DeepSeek V3\n",
            "DeepSeek V3 | Zhiwei Li\n",
            "DeepSeek V3\n",
            "Home\n",
            "PDF\n",
            "English\n",
            "中文\n",
            "日本語\n",
            "Español\n",
            "हिंदी\n",
            "Français\n",
            "Deutsch\n",
            "العربية\n",
            "繁體中文\n",
            "Your browser does not support the audio element.\n",
            "Overview and Key Highlights\n",
            "Model Name: DeepSeek-V3, a Mixture-of-Experts (MoE) language model with 671 billion parameters, of which 37 billion are activated per token.\n",
            "Training Dataset: Pre-trained on 14.8 trillion diverse, high-quality tokens.\n",
            "Core Innovations: Incorporates Multi-Head Latent Attention (MLA) and DeepSeekMoE architectures with auxiliary-loss-free load balancing for efficiency.\n",
            "Training Efficiency: Achieves full training with only 2.788 million H800 GPU hours.\n",
            "Cost Efficiency: Training cost is estimated at 5.576M USD, assuming 2 USD per GPU hour.\n",
            "Architectural Innovations\n",
            "Transformer-Based Framework: Retains the Transformer architecture for scalability and flexibility.\n",
            "Multi-Head Latent Attention (MLA): Reduces inference memory by compressing key-value caches without performance loss.\n",
            "\n",
            "Source: https://martinfowler.com/articles/deepseek-papers.html\n",
            "Title: The DeepSeek Series: A Technical Overview\n",
            "Content: Together, these MLA refinements ensure that while DeepSeek-V3 can attend across\n",
            "very\n",
            "long sequences, the memory overhead remains manageable.\n",
            "Refined DeepSeekMoE: Auxiliary-Loss-Free, Higher Capacity\n",
            "On the MoE side, DeepSeek-V3 drops the auxiliary-loss approach from V2. Instead of an explicit penalty term, each expert acquires a dynamic bias $b_i$. If an expert is overloaded at a step, $b_i$ decreases; if underloaded, $b_i$ increases. The gating decision then adds $b_i$ to the token's affinity:\n",
            "$$ s'_{i,t} = s_{i,t} + b_i $$\n",
            "Key Improvements:\n",
            "No Token Dropping\n",
            ": V2 occasionally dropped tokens if certain experts got overloaded, but the new bias-based method keeps everything.\n",
            "More Activated Experts\n",
            ": They raise the number of routed experts from 6 to 8 per token, improving representational power.\n",
            "Higher Stability\n",
            ": By removing auxiliary losses, they avoid potential interference with the main training objective, focusing purely on the\n",
            "intrinsic\n",
            "o gating signals plus bias adjustments.\n",
            "\n",
            "Source: https://www.deeplearning.ai/the-batch/deepseek-v3-redefines-llm-performance-and-cost-efficiency/\n",
            "Title: DeepSeek-V3 Redefines LLM Performance and Cost Efficiency\n",
            "Content: DeepSeek-V3 Redefines LLM Performance and Cost Efficiency\n",
            "✨ New course! Enroll in\n",
            "Build Apps with Windsurf’s AI Coding Agent\n",
            "Share\n",
            "A new model from Hangzhou upstart DeepSeek delivers outstanding performance and may change the equation for training costs.\n",
            "What’s new:\n",
            "DeepSeek-V3\n",
            "is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights are\n",
            "open\n",
            "except for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download them\n",
            "here\n",
            ".\n",
            "Mixture of experts (MoE) basics:\n",
            "\n",
            "Source: https://medium.com/yugen-ai-technology-blog/deepseek-v3-advances-in-moe-load-balancing-and-multi-token-prediction-training-f6d68c59749c\n",
            "Title: DeepSeek-V3 — Advances in MoE Load Balancing and Multi-Token Prediction Training | by Yugen.ai | Yugen.ai Technology Blog | Jan, 2025 | Medium\n",
            "Content: medium.com\n",
            "Introduction\n",
            "2024 has been a great year for Open Source LLMs. Towards the end of 2024, DeepSeek released DeepSeek-V3, a 671B parameter MOE (mixture-of-experts) language model. One of the reasons why this release came into the limelight was because of cost-effective training cost of ~$6M, which is easily an order of magnitude lower than known estimates for closed source models.\n",
            "Their technical report is a great read for those interested in understanding how architectural design decisions impact model performance. While the DeepSeek-V3’s impressive training cost has been most spoken of, I decided to share some aspects that have been lesser talked about, but are equally impressive. Hope you enjoy the read.\n",
            "Beyond their basic architecture of Multi-Head Latent Attention for inference and DeepSeekMOE for cost effective training, they implement the 2 following strategies to improve the model’s capabilities -\n",
            "An auxiliary-loss-free strategy for load balancing\n",
            "\n",
            "Source: https://www.deeplearning.ai/the-batch/deepseek-v3-redefines-llm-performance-and-cost-efficiency/\n",
            "Title: DeepSeek-V3 Redefines LLM Performance and Cost Efficiency\n",
            "Content: In language tasks, it performed neck-and-neck with Claude 3.5 Sonnet, achieving higher scores in some tasks and lower in others.\n",
            "Behind the news:\n",
            "OpenAI’s o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows.\n",
            "Why it matters:\n",
            "Open models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3’s shockingly low training cost.\n",
            "The team doesn’t explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits “meticulous engineering optimizations.”) But it’s likely that DeepSeek’s steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn’t employ MoE. In 2022,\n",
            "Microsoft\n",
            "\n",
            "Source: https://martinfowler.com/articles/deepseek-papers.html\n",
            "Title: The DeepSeek Series: A Technical Overview\n",
            "Content: The DeepSeek Series: A Technical Overview\n",
            "The DeepSeek Series: A Technical Overview\n",
            "The appearance of DeepSeek Large-Language Models has caused a lot of discussion and angst since their latest versions appeared at the beginning of 2025. But much of the value of DeepSeek's work comes from the papers they have published over the last year. This article provides an overview of these papers, highlighting three main arcs in this research: a focus on improving cost and memory efficiency, the use of HPC Co-Design to train large models on limited hardware, and the development of emergent reasoning from large-scale reinforcement learning\n",
            "06 February 2025\n",
            "Shayan Mohanty\n",
            "\n",
            "INFO:     [22:28:46] 📃 Source: https://medium.com/@magalareuben60/group-relative-policy-optimisation-grpo-the-reinforcement-learning-algorithm-behind-deepseek-954588a0ba07\n",
            "Title: Group Relative Policy Optimisation (GRPO): The Reinforcement learning algorithm behind deepseek. | by Magalareuben | Jan, 2025 | Medium\n",
            "Content: Group Relative Policy Optimisation (GRPO): The Reinforcement learning algorithm behind deepseek. | by Magalareuben | Jan, 2025 | Medium\n",
            "Open in app\n",
            "Sign up\n",
            "Sign in\n",
            "Write\n",
            "Sign up\n",
            "Sign in\n",
            "Group Relative Policy Optimisation (GRPO): The Reinforcement learning algorithm behind deepseek.\n",
            "Magalareuben\n",
            "·\n",
            "Follow\n",
            "7 min read\n",
            "·\n",
            "Jan 31, 2025\n",
            "--\n",
            "1\n",
            "Listen\n",
            "Share\n",
            "I\n",
            "ntroduced in April 2024 in the paper:\n",
            "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n",
            "from DeepSeek-AI, Tsinghua University and Peking University.\n",
            "Group Relative Policy Optimization (GRPO)\n",
            "\n",
            "Source: https://epichka.com/blog/2025/grpo/\n",
            "Title: Group Relative Policy Optimization (GRPO) Illustrated Breakdown | Ebrahim Pichka\n",
            "Content: Group Relative Policy Optimization (GRPO) Illustrated Breakdown | Ebrahim Pichka\n",
            "Group Relative Policy Optimization (GRPO) Illustrated Breakdown\n",
            "A simplified intro to GRPO, an efficient policy optimization method used for LLM reasoning training\n",
            "Introduction\n",
            "Reinforcement Learning (RL) has emerged as a powerful tool for enhancing Large Language Models (LLMs) after their initial training, particularly in reasoning-intensive tasks. DeepSeek’s recent breakthroughs with DeepSeek-Math [2] and DeepSeek-R1 [3] models have demonstrated the remarkable potential of RL in improving mathematical reasoning and problem-solving abilities of LLMs.\n",
            "These achievements were made possible through an innovative RL approach called Group Relative Policy Optimization (GRPO), which addresses the unique challenges of applying RL to language models. In this post, we’ll dive deep into how GRPO works and why it represents a significant advancement in LLM training.\n",
            "PPO vs GRPO\n",
            "\n",
            "Source: https://medium.com/@sahin.samia/the-math-behind-deepseek-a-deep-dive-into-group-relative-policy-optimization-grpo-8a75007491ba\n",
            "Title: The Math Behind DeepSeek: A Deep Dive into Group Relative Policy Optimization (GRPO) | by Sahin Ahmed, Data Scientist | Jan, 2025 | Medium\n",
            "Content: The Math Behind DeepSeek: A Deep Dive into Group Relative Policy Optimization (GRPO) | by Sahin Ahmed, Data Scientist | Jan, 2025 | Medium\n",
            "Open in app\n",
            "Sign up\n",
            "Sign in\n",
            "Write\n",
            "Sign up\n",
            "Sign in\n",
            "Mastodon\n",
            "The Math Behind DeepSeek: A Deep Dive into\n",
            "Group Relative Policy Optimization (GRPO)\n",
            "Sahin Ahmed, Data Scientist\n",
            "·\n",
            "Follow\n",
            "6 min read\n",
            "·\n",
            "Jan 26, 2025\n",
            "--\n",
            "8\n",
            "Listen\n",
            "Share\n",
            "This blog dives into the math behind\n",
            "Group Relative Policy Optimization (GRPO)\n",
            ", the core reinforcement learning algorithm that drives DeepSeek’s exceptional reasoning capabilities. We’ll break down how GRPO works, its key components, and why it’s a game-changer for training advanced Large Language Models (LLMs).\n",
            "The Foundation of GRPO\n",
            "What is GRPO?\n",
            "\n",
            "Source: https://medium.com/@joshithagandra/understanding-group-relative-policy-optimization-grpo-powering-deepseekmath-and-deepseek-r1-df141c94c666\n",
            "Title: Understanding GRPO: Powering DeepSeekMath and DeepSeek-R1 | Medium\n",
            "Content: Understanding GRPO: Powering DeepSeekMath and DeepSeek-R1 | Medium\n",
            "Open in app\n",
            "Sign up\n",
            "Sign in\n",
            "Write\n",
            "Sign up\n",
            "Sign in\n",
            "Understanding Group Relative Policy Optimization (GRPO): Powering DeepSeekMath and DeepSeek-R1\n",
            "Joshitha gandra\n",
            "·\n",
            "Follow\n",
            "4 min read\n",
            "·\n",
            "Jan 31, 2025\n",
            "--\n",
            "Listen\n",
            "Share\n",
            "Introduction\n",
            "Reinforcement learning (RL) has become essential in developing AI models for complex problem-solving, especially in tasks that require mathematical and logical reasoning. Traditional RL methods, such as Proximal Policy Optimization (PPO), have been widely adopted due to their simplicity and effectiveness. However, PPO comes with several limitations, such as its reliance on a critic model, which increases memory usage and computational cost.\n",
            "In response to these challenges,\n",
            "Group Relative Policy Optimization (GRPO)\n",
            "\n",
            "Source: https://blog.dataopslabs.com/deepseek-r1-efficient-reinforcement-learning-with-grpo\n",
            "Title: Efficient Learning: DeepSeek R1 with GRPO\n",
            "Content: Key Concepts\n",
            ":\n",
            "Agent\n",
            ": The decision-making system (e.g., DeepSeek R1).\n",
            "Environment\n",
            ": The system the agent interacts with (e.g., a customer query system).\n",
            "Reward\n",
            ": Feedback on how well the agent performs (e.g., customer satisfaction).\n",
            "Policy\n",
            ": A strategy the agent uses to decide its next action.\n",
            "How RL Works\n",
            ":\n",
            "The agent explores actions to maximize cumulative rewards over time.\n",
            "RL algorithms like\n",
            "Proximal Policy Optimization (PPO)\n",
            "are often used but require significant computational resources, especially for large models.\n",
            "What is GRPO?\n",
            "Group Relative Policy Optimization (GRPO)\n",
            "is a lightweight, efficient RL algorithm designed to optimize large models like DeepSeek R1. Unlike traditional RL methods (e.g., PPO) that rely on separate critic models to estimate value functions, GRPO avoids this additional overhead by comparing outputs within groups.\n",
            "Key Features of GRPO\n",
            ":\n",
            "Group-Level Baselines\n",
            ": Instead of using a critic model, GRPO calculates relative rewards within a group of outputs.\n",
            "\n",
            "Source: https://medium.com/@sahin.samia/the-math-behind-deepseek-a-deep-dive-into-group-relative-policy-optimization-grpo-8a75007491ba\n",
            "Title: The Math Behind DeepSeek: A Deep Dive into Group Relative Policy Optimization (GRPO) | by Sahin Ahmed, Data Scientist | Jan, 2025 | Medium\n",
            "Content: The Foundation of GRPO\n",
            "What is GRPO?\n",
            "Group Relative Policy Optimization (GRPO) is a reinforcement learning (RL) algorithm specifically designed to enhance reasoning capabilities in Large Language Models (LLMs). Unlike traditional RL methods, which rely heavily on external evaluators (critics) to guide learning, GRPO optimizes the model by evaluating groups of responses relative to one another. This approach enables more efficient training, making GRPO ideal for reasoning tasks that require complex problem-solving and long chains of thought.\n",
            "Why GRPO?\n",
            "Traditional RL methods like Proximal Policy Optimization (PPO) face significant challenges when applied to reasoning tasks in LLMs:\n",
            "Dependency on a Critic Model\n",
            ":\n",
            "PPO requires a separate critic model to estimate the value of each response, which doubles memory and computational requirements.\n",
            "Training the critic is complex and prone to errors, especially for tasks with subjective or nuanced evaluations.\n",
            "High Computational Cost\n",
            ":\n",
            "\n",
            "Source: https://blog.dataopslabs.com/deepseek-r1-efficient-reinforcement-learning-with-grpo\n",
            "Title: Efficient Learning: DeepSeek R1 with GRPO\n",
            "Content: Efficient Learning: DeepSeek R1 with GRPO\n",
            "DeepSeek R1: Efficient Reinforcement Learning with GRPO\n",
            "DataOps Labs\n",
            "·\n",
            "Jan 28, 2025\n",
            "·\n",
            "4\n",
            "min read\n",
            "Introduction\n",
            "In the evolving world of artificial intelligence (AI), efficient model training is crucial for achieving top-tier performance without spiraling hardware costs.\n",
            "DeepSeek R1\n",
            ", a state-of-the-art reasoning model, stands out for its innovative use of\n",
            "Reinforcement Learning (RL)\n",
            "and\n",
            "Group Relative Policy Optimization (GRPO)\n",
            ". This blog dives into what RL is, explains the GRPO technique, and demonstrates how DeepSeek R1 transforms reasoning tasks with unparalleled efficiency. We'll also explore a travel company use case to highlight its real-world applications.\n",
            "What is Reinforcement Learning (RL)?\n",
            "Reinforcement Learning is a machine learning paradigm where an agent learns to perform tasks by interacting with an environment and receiving rewards or penalties based on its actions.\n",
            "Key Concepts\n",
            ":\n",
            "Agent\n",
            "\n",
            "Source: https://epichka.com/blog/2025/grpo/\n",
            "Title: Group Relative Policy Optimization (GRPO) Illustrated Breakdown | Ebrahim Pichka\n",
            "Content: The key innovations of GRPO - group sampling, relative advantage estimation, and the elimination of the value network - provide a blueprint for future developments in LLM training. As we continue to push the boundaries of what language models can achieve, techniques like GRPO will be crucial in unlocking their full potential.\n",
            "References\n",
            "[1] Schulman, John, et al. Proximal Policy Optimization Algorithms. arXiv:1707.06347, arXiv, 28 Aug. 2017. arXiv.org, https://doi.org/10.48550/arXiv.1707.06347.\n",
            "[2] Shao, Zhihong, et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300, arXiv, 27 Apr. 2024. arXiv.org, https://doi.org/10.48550/arXiv.2402.03300.\n",
            "[3] DeepSeek-AI, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, arXiv, 22 Jan. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2501.12948.\n",
            "Please enable JavaScript to view the\n",
            "comments powered by giscus.\n",
            "\n",
            "Source: https://blog.dataopslabs.com/deepseek-r1-efficient-reinforcement-learning-with-grpo\n",
            "Title: Efficient Learning: DeepSeek R1 with GRPO\n",
            "Content: GRPO Optimization\n",
            ":\n",
            "The model compares the generated itineraries and updates its policy to prioritize itineraries with higher rewards.\n",
            "Outcome\n",
            ":\n",
            "The system delivers the best itinerary that balances cost, cultural experiences, and customer satisfaction.\n",
            "Cost Efficiency: How DeepSeek R1 Reduces Hardware Costs\n",
            "Critic-Free RL\n",
            ":\n",
            "GRPO eliminates the need for a separate critic model, cutting memory and compute costs by up to 50%.\n",
            "Group-Based Optimization\n",
            ":\n",
            "By focusing on relative rewards within small groups, GRPO minimizes computational overhead.\n",
            "Efficient Use of Resources\n",
            ":\n",
            "DeepSeek R1 uses FP8 mixed-precision training and memory-saving techniques, reducing GPU requirements for both training and inference.\n",
            "Training Time\n",
            ":\n",
            "Training with GRPO is faster and requires fewer iterations, further lowering hardware utilization.\n",
            "Conclusion\n",
            "\n",
            "Source: https://medium.com/@magalareuben60/group-relative-policy-optimisation-grpo-the-reinforcement-learning-algorithm-behind-deepseek-954588a0ba07\n",
            "Title: Group Relative Policy Optimisation (GRPO): The Reinforcement learning algorithm behind deepseek. | by Magalareuben | Jan, 2025 | Medium\n",
            "Content: is a reinforcement learning algorithm that builds upon Proximal Policy Optimization (PPO) algorithim, an algorithim that was introduced back in 2017 by openAI for reinforcement learning tasks but later proved to be helpful in LLMs through reinforcement learning with human feedback. GRPO is a more simple and efficient algorithm that was initially designed to improve mathematical reasoning capabilities while reducing memory consumption in large language models and in this article we will explore how this all comes to play but before we proceed for those not familiar with Reinforcement learning, in reinforcement learning we have something called the policy, in terms of LLMs, the policy is the model’s learned strategy that maps input text (states) to output text (actions). In simple terms the policy is like a brain that tells the model what to do given a certain state. Most of the methods in RL are geared towards optimizing this policy, i.e to a policy (optimal policy) that will give us\n",
            "\n",
            "INFO:     [22:28:46] 📃 Source: https://www.modular.com/ai-resources/exploring-deepseek-r1-s-mixture-of-experts-model-architecture\n",
            "Title: Exploring DeepSeek-R1's Mixture-of-Experts Model Architecture - AI Resources\n",
            "Content: Understanding Mixture-of-Experts Architecture\n",
            "The Mixture-of-Experts (MoE) architecture is a neural network design that incorporates multiple expert sub-models, each specializing in different aspects of data processing. A gating mechanism dynamically selects the most relevant experts for each input, enabling the model to allocate computational resources efficiently. This approach contrasts with traditional dense models, where all parameters are active during every computation, leading to higher resource consumption.\n",
            "DeepSeek-R1's MoE Implementation\n",
            "DeepSeek-R1 employs an MoE framework comprising 671 billion parameters. However, during any given forward pass, only 37 billion parameters are activated. This selective activation is achieved through a sophisticated gating mechanism that routes inputs to the most pertinent experts, thereby optimizing computational efficiency without compromising performance.\n",
            "Gating Mechanism\n",
            "\n",
            "Source: https://medium.com/aimonks/deepseek-v3-efficient-and-scalable-ai-with-mixture-of-experts-8bd945b5ea3f\n",
            "Title: DeepSeek-V3: Efficient and Scalable AI with Mixture-of-Experts | by My Social | 𝐀𝐈 𝐦𝐨𝐧𝐤𝐬.𝐢𝐨 | Dec, 2024 | Medium\n",
            "Content: The use of a Mixture-of-Experts (MoE AI models) has come out as one of the best solutions to this challenge. MoE models split one model into multiple specific, smaller sub-networks, known as ‘experts’ where the model can greatly enhance its capacity without experiencing destructive escalations in computational expense. However, these models are not without their problems such as; imbalance distribution of data among experts and highly demanding computational resources during the training phase.\n",
            "These challenges are solved by DeepSeek-V3 Advanced approaches such as improvements in gating for dynamic routing and less consumption of attention in this MoE. All these enhance the equality of distribution of the specialists and performant computing, thereby offering advanced intelligent systems for paramount application in different fields.\n",
            "What is DeepSeek-V3?\n",
            "\n",
            "Source: https://dev.to/sayed_ali_alkamel/deepseek-and-the-power-of-mixture-of-experts-moe-ham\n",
            "Title: DeepSeek and the Power of Mixture of Experts (MoE) - DEV Community\n",
            "Content: DeepSeek and the Power of Mixture of Experts (MoE) - DEV Community\n",
            "Add reaction\n",
            "Like\n",
            "Unicorn\n",
            "Exploding Head\n",
            "Raised Hands\n",
            "Fire\n",
            "Jump to Comments\n",
            "Save\n",
            "Boost\n",
            "Moderate\n",
            "Copy link\n",
            "Copied to Clipboard\n",
            "Share to X\n",
            "Share to LinkedIn\n",
            "Share to Facebook\n",
            "Share to Mastodon\n",
            "Report Abuse\n",
            "DeepSeek is causing a stir in the AI community with its open-source large language models (LLMs), and a key factor in its success is the\n",
            "Mixture of Experts (MoE)\n",
            "architecture. This approach allows DeepSeek to achieve impressive performance with remarkable efficiency, rivaling even giants like OpenAI's GPT series. But what exactly is MoE, and how does it work within DeepSeek?\n",
            "Understanding Mixture of Experts (MoE)\n",
            "\n",
            "Source: https://medium.com/@amirhossein_dehghaniazar/deepseek-and-mixture-of-experts-revolutionizing-ai-efficiency-and-speed-1ce1f931e45c\n",
            "Title: DeepSeek and Mixture of Experts (MoE): How AI is Becoming Faster and Smarter | Medium\n",
            "Content: What is Mixture of Experts (MoE)?\n",
            "MoE is a paradigm shift in neural network design. Instead of a monolithic model processing every input, MoE divides the workload among specialized subnetworks (“experts”) and a gating mechanism that dynamically routes tasks.\n",
            "Experts\n",
            ": Smaller neural networks, each excelling in specific data types or tasks (e.g., grammar, imagery, or numerical patterns).\n",
            "Gating Network\n",
            ": A smart router that selects the most relevant experts for each input.\n",
            "Imagine a conference where specialists deliver targeted talks instead of a single speaker covering everything. MoE works similarly, activating only the necessary experts, slashing computational costs while maintaining high accuracy.\n",
            "DeepSeek: Harnessing MoE for Breakthrough Performance\n",
            "DeepSeek integrates MoE with innovative engineering to optimize AI workflows. Here’s how it stands out:\n",
            "Dynamic Computation\n",
            "\n",
            "Source: https://dev.to/sayed_ali_alkamel/deepseek-and-the-power-of-mixture-of-experts-moe-ham\n",
            "Title: DeepSeek and the Power of Mixture of Experts (MoE) - DEV Community\n",
            "Content: MoE models can be implemented in various ways, including hierarchical structures. Hierarchical mixtures of experts use multiple levels of gating networks in a tree-like structure, with experts residing at the leaf nodes. This hierarchical approach allows for more complex and nuanced decision-making, further enhancing the model's ability to handle diverse tasks.\n",
            "Furthermore, MoE architectures enable large-scale models to reduce computation costs during pre-training and achieve faster performance during inference time. This efficiency stems from selectively activating only the specific experts needed for a given task, rather than activating the entire neural network for every task.\n",
            "MoE in DeepSeek\n",
            "\n",
            "Source: https://www.modular.com/ai-resources/exploring-deepseek-r1-s-mixture-of-experts-model-architecture\n",
            "Title: Exploring DeepSeek-R1's Mixture-of-Experts Model Architecture - AI Resources\n",
            "Content: Models\n",
            "ML Systems\n",
            "Exploring DeepSeek-R1's Mixture-of-Experts Model Architecture\n",
            "Introduction\n",
            "In the rapidly evolving field of artificial intelligence, model efficiency and scalability are paramount. DeepSeek-R1, introduced in January 2025 by the Chinese AI startup DeepSeek, exemplifies these principles through its innovative Mixture-of-Experts (MoE) architecture. This article delves into the intricacies of DeepSeek-R1's MoE design, exploring its structure, advantages, and the broader implications for AI development.\n",
            "Understanding Mixture-of-Experts Architecture\n",
            "\n",
            "Source: https://www.modular.com/ai-resources/exploring-deepseek-r1-s-mixture-of-experts-model-architecture\n",
            "Title: Exploring DeepSeek-R1's Mixture-of-Experts Model Architecture - AI Resources\n",
            "Content: Exploring DeepSeek-R1's Mixture-of-Experts Model Architecture - AI Resources\n",
            "AIÂ REsources Home\n",
            "Exploring DeepSeek-R1's Mixture-of-Experts Model Architecture\n",
            "Popular\n",
            "ML Compiler Technical Primer\n",
            "Quantization Technical Primer\n",
            "Mixtral of Experts\n",
            "Efficient Memory Management for LLM Serving with PagedAttention\n",
            "RoBERTa: A Robustly Optimized BERT Pretraining Approach\n",
            "+ View more\n",
            "Categories\n",
            "Mixture of Experts (MoE)\n",
            "DeepSeek-R1\n",
            "Test Time Compute\n",
            "AMD MI300X\n",
            "NVIDIA H100\n",
            "NVIDIA H200\n",
            "NVIDIA A100\n",
            "Embedding Models\n",
            "Offline Batch Inference\n",
            "Text Embedding\n",
            "Prometheus & Grafana\n",
            "Speculative Decoding\n",
            "Prefix Caching\n",
            "GGUF Models\n",
            "FP8 with LLMs\n",
            "LLM Serving\n",
            "Function Calling\n",
            "Structured JSON\n",
            "KV Cache\n",
            "AI Foundations\n",
            "Research\n",
            "Industry\n",
            "Agents\n",
            "Context Windows\n",
            "Models\n",
            "ML Systems\n",
            "Exploring DeepSeek-R1's Mixture-of-Experts Model Architecture\n",
            "Introduction\n",
            "\n",
            "Source: https://medium.com/aimonks/deepseek-v3-efficient-and-scalable-ai-with-mixture-of-experts-8bd945b5ea3f\n",
            "Title: DeepSeek-V3: Efficient and Scalable AI with Mixture-of-Experts | by My Social | 𝐀𝐈 𝐦𝐨𝐧𝐤𝐬.𝐢𝐨 | Dec, 2024 | Medium\n",
            "Content: What is DeepSeek-V3?\n",
            "The DeepSeek-V3 is a strong Mixture-of-Experts (MoE) large language model that was created by the DeepSeek AI. This architecture can make it achieve high performance with better efficiency and extensibility. It is available in varying sizes; it has the basic version in its list of offerings depending on the computation demands of the user.\n",
            "Key Features of DeepSeek-V3\n",
            "DeepSeek-V3 leverages its MoE architecture to achieve several key advantages:\n",
            "Efficiency\n",
            ": DeepSeek-V3 uses Mixture-of-Experts (MoE) by enabling a portion of its parameters say, 37B out of 671B, for any input. This selective activation reduces the computational costs considerably bringing out the ability to perform well while frugal with computation.\n",
            "Scalability\n",
            "\n",
            "Source: https://dev.to/sayed_ali_alkamel/deepseek-and-the-power-of-mixture-of-experts-moe-ham\n",
            "Title: DeepSeek and the Power of Mixture of Experts (MoE) - DEV Community\n",
            "Content: Scalability\n",
            ": DeepSeek can easily scale by adding more specialized experts without significantly impacting computational requirements. This modularity makes DeepSeek adaptable and future-proof, allowing it to accommodate new tasks and domains as they emerge.\n",
            "DeepSeek's MoE implementation involves some unique strategies to further enhance efficiency and performance:\n",
            "Fine-grained expert segmentation\n",
            ": Each expert is further divided into smaller experts, promoting specialization and preventing any single expert from becoming a generalist. This fine-grained approach ensures that each expert possesses highly focused knowledge, leading to more accurate and efficient processing.\n",
            "Shared expert isolation\n",
            ": Certain experts are designated as \"shared experts\" and are always active, capturing common knowledge applicable across various contexts. This strategy helps to reduce redundancy and improve the model's ability to generalize across different tasks.\n",
            "Expert Choice (EC) routing algorithm\n",
            "\n",
            "Source: https://medium.com/@amirhossein_dehghaniazar/deepseek-and-mixture-of-experts-revolutionizing-ai-efficiency-and-speed-1ce1f931e45c\n",
            "Title: DeepSeek and Mixture of Experts (MoE): How AI is Becoming Faster and Smarter | Medium\n",
            "Content: DeepSeek and Mixture of Experts (MoE): How AI is Becoming Faster and Smarter | Medium\n",
            "Open in app\n",
            "Sign up\n",
            "Sign in\n",
            "Write\n",
            "Sign up\n",
            "Sign in\n",
            "DeepSeek and Mixture of Experts: Revolutionizing AI Efficiency and Speed\n",
            "Amirhossein Dehghanazar\n",
            "·\n",
            "Follow\n",
            "3 min read\n",
            "·\n",
            "Feb 1, 2025\n",
            "--\n",
            "Listen\n",
            "Share\n",
            "Introduction: The Quest for Smarter, Faster AI\n",
            "As artificial intelligence models grow larger and more complex, a critical challenge emerges: balancing performance with efficiency. Traditional models, while powerful, often struggle with computational demands. Enter\n",
            "DeepSeek\n",
            "— a cutting-edge AI framework leveraging the\n",
            "Mixture of Experts (MoE)\n",
            "architecture to redefine speed and scalability. This article explores how DeepSeek and MoE are transforming AI, making it faster, leaner, and more adaptable than ever before.\n",
            "What is Mixture of Experts (MoE)?\n",
            "\n",
            "WARNING:gpt_researcher.scraper.scraper:Content too short or empty for https://medium.com:8443/yugen-ai-technology-blog/understanding-the-math-behind-grpo-deepseek-r1-zero-9fb15e103a0a\n",
            "INFO:     [22:28:47] 📄 Scraped 2 pages of content\n",
            "INFO:     [22:28:47] 🖼️ Selected 4 new images from 13 total images\n",
            "INFO:     [22:28:47] 🌐 Scraping complete\n",
            "INFO:     [22:28:47] 📚 Getting relevant content based on query: Why DeepSeek's MoE and GRPO is a successful architecture in LLM research and application?...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error! : HTTPSConnectionPool(host='medium.com', port=8443): Read timed out. (read timeout=4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     [22:28:48] 📃 Source: https://opdeepseek.com/deepseek-moe/\n",
            "Title: DeepSeek-MoE Power Of Mixture of Experts Framework\n",
            "Content: DeepSeek-MoE Power Of Mixture of Experts Framework\n",
            "Skip to content\n",
            "DeepSeek-MoE\n",
            "is a large language model (LLM) with 16.4B parameters. It is based on the Mixture-of-Experts (MoE) architecture, which allows it to achieve high performance with relatively low computational cost. Inshort, It’s a model that leverages the Mixture of Experts (MoE) framework, which allows it to perform complex tasks efficiently. Curious about how it works? Well, the Mixture of Experts framework assigns tasks to specialized expert networks, each one focuses on a specific area.To give the most accurate output A gating mechanism decides which expert is best suited for a specific task. To better understand the concept we will go through this page and simplify the basic concept.\n",
            "What is DeepSeek-MoE?\n",
            "DeepSeek\n",
            "is a large language model built on the Mixture of Experts framework. If we\n",
            "compare\n",
            "\n",
            "Source: https://opdeepseek.com/deepseek-moe/\n",
            "Title: DeepSeek-MoE Power Of Mixture of Experts Framework\n",
            "Content: DeepSeek Mixture-of-expert\n",
            "architecture applies the MoE framework in a way that maximizes both scalability and accuracy. By leveraging specialized experts, it can perform a diverse range of tasks with remarkable precision. Whether it’s language modeling, data analysis, or predictive analytics, DeepSeek intelligently assigns tasks to the most capable expert networks.\n",
            "The beauty of this system is its\n",
            "adaptability\n",
            ". It can continuously learn and adjust, so that as new data emerges, the right experts are engaged. This means that\n",
            "DeepSeek\n",
            "isn’t just efficient, it’s also highly dynamic and responsive to different types of input provided to it.\n",
            "Benefits of DeepSeek-MoE Architecture\n",
            "Enhanced Parameter Efficiency\n",
            "\n",
            "Source: https://opdeepseek.com/deepseek-moe/\n",
            "Title: DeepSeek-MoE Power Of Mixture of Experts Framework\n",
            "Content: Benefits of DeepSeek-MoE Architecture\n",
            "Enhanced Parameter Efficiency\n",
            "One of the standout benefits of this architecture is its ability to utilize parameters efficiently. Unlike traditional models that activate all parameters, MoE selectively engages only the necessary ones. This means that computational resources are used more effectively, leading to faster processing speeds and reduced energy consumption.\n",
            "Mitigation of Redundancy\n",
            "Because experts are specialized, there is far less redundancy in DeepSeek compared to conventional AI models. Each expert focuses on a specific area, eliminating unnecessary computations and making the system more streamlined. This results in faster execution and more accurate outcomes.\n",
            "Higher Expert Specialization\n",
            "\n",
            "Source: https://opdeepseek.com/deepseek-moe/\n",
            "Title: DeepSeek-MoE Power Of Mixture of Experts Framework\n",
            "Content: to use, so you won’t be charged for any additional feature.\n",
            "DeepSeek–MoE Architecture\n",
            "DeepSeek AI\n",
            "selectively activates only a few experts, significantly reducing computational overhead. The model also incorporates techniques to balance workload distribution across experts, preventing any single expert from being overburdened. This balance ensures that the system remains stable and doesn’t suffer from inefficiencies due to uneven task allocation.\n",
            "Applications of DeepSeek-MoE\n",
            "The applications of DeepSeek-MoE span multiple domains. It excels in natural language processing, offering better text generation, summarization, and translation capabilities. In finance, it aids in predictive analytics, risk assessment, and fraud detection. Healthcare also benefits from this technology, as it helps in diagnosing diseases by analyzing vast medical datasets.\n",
            "DeepSeek Mixture-of-expert\n",
            "\n",
            "Source: https://opdeepseek.com/deepseek-moe/\n",
            "Title: DeepSeek-MoE Power Of Mixture of Experts Framework\n",
            "Content: DeepSeek\n",
            "is a large language model built on the Mixture of Experts framework. If we\n",
            "compare\n",
            "it then It is not like other traditional models that activate all their parameters for every task, MoE selectively activates only the necessary experts for a given input. This makes it highly efficient in terms of computation and specialization. Essentially, DeepSeek optimizes performance while keeping resource consumption in check, a balance that’s difficult to achieve with conventional architectures.\n",
            "The backend working based on this division makes work faster, even smarter and capable of handling a number of inputs in a more efficient way.\n",
            "Basic Concept Of Mixture Of Expert Framework\n",
            "\n",
            "Source: https://opdeepseek.com/deepseek-moe/\n",
            "Title: DeepSeek-MoE Power Of Mixture of Experts Framework\n",
            "Content: The Future of DeepSeek-MoE\n",
            "Looking ahead,\n",
            "DeepSeek-MoE\n",
            "has the potential to redefine AI applications. As technology advances, we can expect even more efficient architectures, further reducing computational costs while improving accuracy. AI models like DeepSeek will likely become more integrated into everyday applications, making advanced AI accessible to businesses and individuals alike.\n",
            "The challenge will be refining MoE frameworks to further minimize latency and ensure fair expert utilization. But with continuous research and development, DeepSeek is poised to remain at the forefront of AI innovation.\n",
            "You can also explore the\n",
            "key features of DeepSeek\n",
            "and if you are curious to know\n",
            "is DeepSeek safe\n",
            ".\n",
            "FAQs\n",
            "How does DeepSeek differ from traditional AI models?\n",
            "DeepSeek uses a Mixture of Experts framework, selectively activating only relevant experts instead of processing every input with a full model, making it more efficient.\n",
            "\n",
            "Source: https://opdeepseek.com/deepseek-moe/\n",
            "Title: DeepSeek-MoE Power Of Mixture of Experts Framework\n",
            "Content: Higher Expert Specialization\n",
            "Specialization is what makes DeepSeek so powerful. Each expert network is trained on specific types of data, ensuring that they become highly proficient in their domain. This focused learning approach leads to better performance, as opposed to models that try to do everything at once without true mastery in any area.\n",
            "Improved Load Balancing\n",
            "With a well-structured gating network, DeepSeek ensures that computational loads are distributed evenly across experts. This prevents bottlenecks and optimizes performance, making the model both scalable and reliable.\n",
            "Flexibility in Knowledge Acquisition\n",
            "DeepSeek isn’t static, it evolves. The model can integrate new knowledge seamlessly, allowing it to adapt to changing information landscapes. Whether it’s learning new languages, adapting to new research findings, or improving predictive capabilities, DeepSeek MoE remains flexible and future-proof.\n",
            "The Future of DeepSeek-MoE\n",
            "Looking ahead,\n",
            "DeepSeek-MoE\n",
            "\n",
            "Source: https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/\n",
            "Title: Why GRPO is Important and How it Works\n",
            "Content: Why GRPO is Important and How it Works\n",
            "Last week on Arxiv Dives we dug into research\n",
            "behind DeepSeek-R1\n",
            ", and uncovered that one of the techniques they use in the their training pipeline is called Group Relative Policy Optimization (GRPO).\n",
            "At it’s core, GRPO is a Reinforcement Learning (RL) algorithm that is aimed at improving the model’s reasoning ability. It was first introduced in their paper\n",
            "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\n",
            ", but was also used in the post-training of\n",
            "DeepSeek-R1\n",
            ".\n",
            "The process to go from DeepSeek’s base pre-trained language model to a reasoning model was laid out in detail in the\n",
            "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\n",
            "paper.\n",
            "Last week we didn’t get too deep into the math or process behind GRPO or look at any code, so today the goal is to fully understand what is going on in GRPO and help apply it to your own work.\n",
            "💡\n",
            "Note\n",
            "\n",
            "Source: https://opdeepseek.com/deepseek-moe/\n",
            "Title: DeepSeek-MoE Power Of Mixture of Experts Framework\n",
            "Content: Basic Concept Of Mixture Of Expert Framework\n",
            "To put it simply, Mixture of Experts is like having a team of specialists instead of a jack-of-all-trades. Instead of relying on a single model to process all kinds of information, MoE breaks tasks into smaller parts and assigns them to specialized sub-models called experts. A gating network acts as a decision-maker, directing inputs to the most relevant experts. This results in a more efficient and precise output.\n",
            "Think of it like visiting a hospital. Instead of one doctor handling everything, you have cardiologists, neurologists, and orthopedic specialists who focus on their respective fields. This targeted approach ensures better diagnosis and treatment, just as MoE enhances AI performance. And the best part?\n",
            "DeepSeek is free\n",
            "to use, so you won’t be charged for any additional feature.\n",
            "DeepSeek–MoE Architecture\n",
            "DeepSeek AI\n",
            "\n",
            "Source: https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/\n",
            "Title: Why GRPO is Important and How it Works\n",
            "Content: Not only is this computationally expensive, there are a lot of moving parts and multiple models you are optimizing. The more moving parts, typically the harder it is to optimize. GRPO helps simplify things.\n",
            "For fun, I decided to try a bunch of different model sizes on an H100 and see how easy it was to fine tune with GRPO\n",
            "If you want all the technical details check them out here:\n",
            "🧠 GRPO VRAM Requirements For the GPU Poor | Oxen.ai\n",
            "Since the release of DeepSeek-R1, Group Relative Policy Optimization (GRPO) has become the talk of the town for Reinforcement Learning in Large Language Models due to its effectiveness and ease of training. The R1 paper demonstrated how you can use GRPO to go from a base instruction following LLM (DeepSeek-v3) to a reasoning model (DeepSeek-R1). To learn more about instruction following, reasoning models, and the full DeepSeek-R1 model, I suggest you checkout some of our other deep dives. How\n",
            "Oxen.ai\n",
            "\n",
            "INFO:     [22:29:10] 📃 Source: https://www.linkedin.com/pulse/grpo-game-changer-deepseeks-llms-sandeep-k-dzx2f\n",
            "Title: GRPO: A Game-Changer for DeepSeek’s LLMs\n",
            "Content: GRPO: A Game-Changer for DeepSeek’s LLMs\n",
            "Agree & Join LinkedIn\n",
            "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
            "User Agreement\n",
            ",\n",
            "Privacy Policy\n",
            ", and\n",
            "Cookie Policy\n",
            ".\n",
            "Sign in to view more content\n",
            "Create your free account or sign in to continue your search\n",
            "Sign in\n",
            "Welcome back\n",
            "Email or phone\n",
            "Password\n",
            "Show\n",
            "Forgot password?\n",
            "Sign in\n",
            "or\n",
            "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
            "User Agreement\n",
            ",\n",
            "Privacy Policy\n",
            ", and\n",
            "Cookie Policy\n",
            ".\n",
            "New to LinkedIn?\n",
            "Join now\n",
            "or\n",
            "New to LinkedIn?\n",
            "Join now\n",
            "By clicking Continue to join or sign in, you agree to LinkedIn’s\n",
            "User Agreement\n",
            ",\n",
            "Privacy Policy\n",
            ", and\n",
            "Cookie Policy\n",
            ".\n",
            "LinkedIn\n",
            "LinkedIn is better on the app\n",
            "Don’t have the app? Get it in the Microsoft Store.\n",
            "Open the app\n",
            "Skip to main content\n",
            "Introduction\n",
            "Artificial Intelligence (AI) has made incredible strides in recent years, particularly in the field of\n",
            "Large Language Models (LLMs)\n",
            "\n",
            "Source: https://medium.com/@yuvrajsagar117/understanding-grpo-key-ingredient-behind-deepseek-r1s-success-5f8f05494e6d\n",
            "Title: Understanding GRPO: Key ingredient behind Deepseek-R1’s Success | by Yuvraj Sagar | Feb, 2025 | Medium\n",
            "Content: Understanding GRPO: Key ingredient behind Deepseek-R1’s Success | by Yuvraj Sagar | Feb, 2025 | Medium\n",
            "Open in app\n",
            "Sign up\n",
            "Sign in\n",
            "Write\n",
            "Sign up\n",
            "Sign in\n",
            "Understanding GRPO: Key ingredient behind Deepseek-R1’s Success\n",
            "Yuvraj Sagar\n",
            "·\n",
            "Follow\n",
            "9 min read\n",
            "·\n",
            "Feb 12, 2025\n",
            "--\n",
            "Listen\n",
            "Share\n",
            "In my previous blog\n",
            "Deepseek-R1 Explained\n",
            ", I explored How\n",
            "Deepseek-R1\n",
            "was developed, their innovation and techniques they utilized to train such a performant model. One of the key breakthroughs they made was\n",
            "Group Relative Policy Optimization (GRPO) —\n",
            "a large-scale reinforcement learning (RL) algorithm\n",
            "specifically designed to enhance reasoning capabilities in Large Language Models (LLMs). In this blog, we will be covering how GRPO works and how we can implement it for any given LLM to\n",
            "enhance their reasoning capabilities\n",
            ". If you want to find out how Deepseek team utilized this technique for their model, you can visit my\n",
            "Deepseek-R1 Explained\n",
            "blog to find out more. So, let’s get started with GRPO.\n",
            "\n",
            "Source: https://www.linkedin.com/pulse/grpo-game-changer-deepseeks-llms-sandeep-k-dzx2f\n",
            "Title: GRPO: A Game-Changer for DeepSeek’s LLMs\n",
            "Content: How Much Does GRPO Improve Model Performance?\n",
            "DeepSeek's models trained with GRPO\n",
            "achieve industry-leading results\n",
            "in reasoning tasks. Here are some notable performance improvements:\n",
            "On the AIME 2024 math competition benchmark, DeepSeek-R1-Zero increased accuracy from\n",
            "15.6% to 71.0%\n",
            ", and after additional training, DeepSeek-R1 reached\n",
            "79.8% accuracy\n",
            ".\n",
            "On the MATH-500 benchmark, DeepSeek-R1 achieved\n",
            "97.3% accuracy\n",
            ", surpassing most other open models.\n",
            "In competitive coding, DeepSeek-R1 reached a\n",
            "96.3 percentile ranking\n",
            "on Codeforces, a leading competitive programming platform.\n",
            "For general knowledge tasks like MMLU, DeepSeek-R1 achieved\n",
            "90.8% accuracy\n",
            ", making it one of the top-performing open models.\n",
            "These improvements are\n",
            "game-changing\n",
            ". For example, GRPO boosted math performance from\n",
            "15.6% to 71.0%\n",
            ", a nearly\n",
            "5x increase\n",
            "in accuracy.\n",
            "Why GRPO is a Game-Changer for AI and LLMs\n",
            "Enables Self-Learning AI Models\n",
            "GRPO allows models to\n",
            "self-improve\n",
            "\n",
            "Source: https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/\n",
            "Title: A Deep Dive into LLM Optimization: From Policy Gradient to GRPO\n",
            "Content: A Deep Dive into LLM Optimization: From Policy Gradient to GRPO\n",
            "Master Generative AI with 10+ Real-world Projects in 2025!\n",
            "Download Projects\n",
            "Interview Prep\n",
            "Career\n",
            "GenAI\n",
            "Prompt Engg\n",
            "ChatGPT\n",
            "LLM\n",
            "Langchain\n",
            "RAG\n",
            "AI Agents\n",
            "Machine Learning\n",
            "Deep Learning\n",
            "GenAI Tools\n",
            "LLMOps\n",
            "Python\n",
            "NLP\n",
            "SQL\n",
            "AIML Projects\n",
            "From RL to LLMs: Optimizing AI with GRPO, PPO, and DPO for Better Fine-Tuning\n",
            "Neil D\n",
            "Last Updated : 18 Feb, 2025\n",
            "22\n",
            "min read\n",
            "\n",
            "Source: https://www.linkedin.com/pulse/grpo-game-changer-deepseeks-llms-sandeep-k-dzx2f\n",
            "Title: GRPO: A Game-Changer for DeepSeek’s LLMs\n",
            "Content: Large Language Models (LLMs)\n",
            ". These models are trained on massive datasets and fine-tuned with sophisticated algorithms to improve reasoning, problem-solving, and understanding. However, a major challenge has been improving\n",
            "reasoning capabilities\n",
            "efficiently while minimizing training costs.\n",
            "DeepSeek, a leading AI research team, has introduced a groundbreaking technique called\n",
            "Group Relative Policy Optimization (GRPO)\n",
            ", which dramatically enhances LLM reasoning ability through reinforcement learning. GRPO powers\n",
            "DeepSeek-R1-Zero\n",
            "and\n",
            "DeepSeek-R1\n",
            ", two advanced reasoning models that rival industry leaders like OpenAI’s\n",
            "o1-1217\n",
            "series.\n",
            "This article will break down:\n",
            "What GRPO is and why it matters\n",
            "How GRPO differs from other LLM training methods\n",
            "How GRPO impacts DeepSeek's model performance\n",
            "Why GRPO could change the future of AI model training\n",
            "What is Group Relative Policy Optimization (GRPO)?\n",
            "\n",
            "Source: https://medium.com/@yuvrajsagar117/understanding-grpo-key-ingredient-behind-deepseek-r1s-success-5f8f05494e6d\n",
            "Title: Understanding GRPO: Key ingredient behind Deepseek-R1’s Success | by Yuvraj Sagar | Feb, 2025 | Medium\n",
            "Content: Deepseek-R1 Explained\n",
            "blog to find out more. So, let’s get started with GRPO.\n",
            "Quick Note:\n",
            "For the first part, I will be providing a quick recap about how Preference Optimizations are applied to any LLM, if you know about these, you can skip this part, and directly move to GRPO section.\n",
            "How LLMs are trained?\n",
            "Generally, there are three major stages in which LLMs are trained, as:\n",
            "Pretraining\n",
            "— To educate the model from raw text data capturing patterns, grammar and general knowledge. At this stage, we only have base models serving as next-token predictors used for Chat-Completion purposes only.\n",
            "Supervised Finetuning (SFT)\n",
            "— Making base-models to perform well on downstream tasks using a labeled dataset — Helpful assistants.\n",
            "Preference Alignment\n",
            "— To shape these assistants’ responses to follow a desired format of answers to avoid bias, toxicity and harmfullness.\n",
            "Fig: How LLMs are trained —\n",
            "Source\n",
            "\n",
            "Source: https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/\n",
            "Title: A Deep Dive into LLM Optimization: From Policy Gradient to GRPO\n",
            "Content: GRPO – Group Relative Policy Optimization (DeepSeek’s Approach)\n",
            "DeepSeek’s Group Relative Policy Optimization (GRPO) is an advanced preference optimization technique that extends Direct Preference Optimization (DPO) while incorporating elements from Proximal Policy Optimization (PPO). Unlike traditional policy optimization methods that operate on single preference pairs, GRPO leverages group-wise preference ranking, enabling better alignment with human feedback in large-scale LLM fine-tuning.\n",
            "\n",
            "Source: https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/\n",
            "Title: A Deep Dive into LLM Optimization: From Policy Gradient to GRPO\n",
            "Content: Mathematical Foundation of GRPO\n",
            "Data for GRPO Fine-Tuning\n",
            "Code Implementation: Group-Based Preference Optimization\n",
            "Training Loop for GRPO\n",
            "Expected Outcome and Results\n",
            "Final Model Insights: Why GRPO Excels in LLM Fine-Tuning\n",
            "Conclusion\n",
            "Frequently Asked Questions\n",
            "Primer on Policy Optimization Techniques\n",
            "Before diving into DeepSeek’s GRPO, it’s crucial to understand the policy optimization techniques that form the foundation of reinforcement learning (RL) in both traditional control tasks and\n",
            "LLM fine-tuning\n",
            ". Policy optimization refers to the process of improving an AI agent’s decision-making strategy (policy) to maximize expected rewards. While early methods like vanilla\n",
            "policy gradient (PG)\n",
            "laid the groundwork, more sophisticated techniques such as TRPO, PPO, DPO, and GRPO evolved to address issues like stability, efficiency, and preference alignment.\n",
            "What is Policy Optimization?\n",
            "At its core, policy optimization is about learning the optimal policy\n",
            "π_θ(a∣s)\n",
            ", which maps a state\n",
            "s\n",
            "\n",
            "Source: https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/\n",
            "Title: A Deep Dive into LLM Optimization: From Policy Gradient to GRPO\n",
            "Content: However, standard PPO-based RLHF had inefficiencies, requiring expensive reward modeling and iterative training. Enter DeepSeek’s Group Relative Policy Optimization (GRPO)—a breakthrough that eliminated the need for explicit reward modeling by directly optimizing preference rankings. To fully grasp the significance of GRPO, we must first explore the fundamental policy optimization techniques (LLM optimization) that power modern reinforcement learning.\n",
            "Source:\n",
            "Link\n",
            "Learning Objectives\n",
            "Understand why RL-based techniques are crucial for optimizing LLMs like ChatGPT, DeepSeek, Claude, and Gemini.\n",
            "Learn the fundamentals of policy optimization, including PG, TRPO, and PPO.Explore DPO and GRPO for preference-based LLM training without explicit reward models.\n",
            "Compare PG, TRPO, PPO, DPO, and GRPO to determine the best approach for RL and LLM fine-tuning.\n",
            "Gain hands-on experience with Python implementations of policy optimization algorithms.\n",
            "\n",
            "Source: https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/\n",
            "Title: A Deep Dive into LLM Optimization: From Policy Gradient to GRPO\n",
            "Content: Q4. Why do LLMs need preference optimization techniques like DPO and GRPO?\n",
            "Ans. Traditional RL methods focus on maximizing numerical rewards, which do not always align with human expectations in language models. DPO and GRPO fine-tune LLMs based on human preference data, ensuring responses are helpful, honest, and harmless. Unlike Reinforcement Learning with Human Feedback (RLHF), these methods eliminate the need for a separate reward model, making fine-tuning more efficient and reducing potential biases from reward misalignment.\n",
            "Neil D\n",
            "Advancing language model research by day and writing about my work online by night. I explore AI breakthroughs and transform complex studies into clear, engaging insights that empower professionals and enthusiasts alike.\n",
            "Thanks for stopping by my profile!\n",
            "Advanced\n",
            "Best of Tech\n",
            "LLMs\n",
            "Reinforcement Learning\n",
            "Free Courses\n",
            "4.7\n",
            "Generative AI - A Way of Life\n",
            "\n",
            "INFO:     [22:29:10] Finalized research step.\n",
            "💸 Total Research Costs: $0.016939199999999998\n",
            "INFO:     [22:29:10] ✍️ Writing report for 'Why DeepSeek's MoE and GRPO is a successful architecture in LLM research and application?'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m# Why DeepSeek's MoE and GRPO is a Successful Architecture in LLM Research and Application\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m## Introduction\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32mThe field of artificial intelligence (AI) has witnessed significant advancements in recent years, particularly in the development of large language models (LLMs). Among the most noteworthy innovations are DeepSeek's Mixture of Experts (MoE) architecture and Group Relative Policy Optimization (GRPO) algorithm. These two innovations have not only enhanced the efficiency and scalability of LLMs but also redefined their performance in reasoning and computational tasks. This report delves into the reasons behind the success of DeepSeek's MoE and GRPO architectures in LLM research and application, supported by facts, figures, and insights from credible sources.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m---\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m## Mixture of Experts (MoE): A Paradigm Shift in Neural Network Design\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m### Overview of MoE Architecture\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32mThe Mixture of Experts (MoE) architecture represents a significant departure from traditional dense neural networks. Instead of activating all parameters for every computation, MoE selectively activates only a subset of specialized sub-networks, known as \"experts,\" for each input. A gating mechanism dynamically routes inputs to the most relevant experts, optimizing computational efficiency while maintaining high performance ([Modular AI Resources](https://www.modular.com/ai-resources/exploring-deepseek-r1-s-mixture-of-experts-model-architecture)).\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32mDeepSeek's implementation of MoE is particularly noteworthy. For instance, the DeepSeek-V3 model comprises 671 billion parameters, but only 37 billion are activated during any given forward pass. This selective activation significantly reduces computational costs without compromising accuracy ([Yugen.ai Technology Blog](https://medium.com/yugen-ai-technology-blog/deepseek-v3-advances-in-moe-load-balancing-and-multi-token-prediction-training-f6d68c59749c)).\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m### Key Advantages of MoE in DeepSeek\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m1. **Enhanced Parameter Efficiency**:  \n",
            "\u001b[0m\n",
            "\u001b[32m   Traditional models activate all parameters for every input, leading to redundancy and inefficiency. In contrast, MoE selectively engages only the necessary experts, resulting in faster processing speeds and reduced energy consumption ([OpDeepSeek](https://opdeepseek.com/deepseek-moe/)).\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m2. **Higher Expert Specialization**:  \n",
            "\u001b[0m\n",
            "\u001b[32m   Each expert in the MoE framework is trained on specific types of data, ensuring proficiency in its domain. This focused learning approach leads to better performance compared to models that attempt to generalize across all tasks ([OpDeepSeek](https://opdeepseek.com/deepseek-moe/)).\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m3. **Improved Load Balancing**:  \n",
            "\u001b[0m\n",
            "\u001b[32m   DeepSeek's MoE incorporates techniques to distribute computational loads evenly across experts, preventing bottlenecks and ensuring scalability. For example, DeepSeek-V3 introduced auxiliary-loss-free load balancing, which dynamically adjusts expert utilization without dropping tokens ([Martin Fowler](https://martinfowler.com/articles/deepseek-papers.html)).\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m4. **Cost Efficiency**:  \n",
            "\u001b[0m\n",
            "\u001b[32m   The training cost of DeepSeek-V3 was approximately $5.576 million, achieved through meticulous engineering optimizations and the use of FP8 mixed-precision training. This is an order of magnitude lower than the costs associated with comparable closed-source models ([DeepLearning.AI](https://www.deeplearning.ai/the-batch/deepseek-v3-redefines-llm-performance-and-cost-efficiency/)).\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m### Applications of MoE in DeepSeek\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32mDeepSeek's MoE architecture has demonstrated exceptional versatility across various domains, including:\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m- **Natural Language Processing (NLP)**: Improved text generation, summarization, and translation capabilities.  \n",
            "\u001b[0m\n",
            "\u001b[32m- **Finance**: Enhanced predictive analytics, risk assessment, and fraud detection.  \n",
            "\u001b[0m\n",
            "\u001b[32m- **Healthcare**: Advanced diagnostic capabilities through the analysis of large medical datasets ([OpDeepSeek](https://opdeepseek.com/deepseek-moe/)).\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m---\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m## Group Relative Policy Optimization (GRPO): Revolutionizing Reinforcement Learning\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m### Overview of GRPO\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32mGroup Relative Policy Optimization (GRPO) is a reinforcement learning (RL) algorithm designed to enhance the reasoning capabilities of LLMs. Unlike traditional RL methods such as Proximal Policy Optimization (PPO), GRPO eliminates the need for a separate critic model by comparing outputs within groups. This approach reduces memory and computational costs while improving training efficiency ([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/)).\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32mGRPO was first introduced in the \"DeepSeekMath\" paper and later applied to DeepSeek-R1, a state-of-the-art reasoning model. By leveraging GRPO, DeepSeek-R1 achieved remarkable improvements in reasoning tasks, including mathematical problem-solving and competitive coding ([Oxen.ai](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/)).\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m### Key Advantages of GRPO in DeepSeek\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m1. **Critic-Free RL**:  \n",
            "\u001b[0m\n",
            "\u001b[32m   Traditional RL methods rely on a critic model to estimate value functions, doubling memory and computational requirements. GRPO eliminates this overhead, reducing GPU memory usage by up to 50% ([DataOps Labs](https://blog.dataopslabs.com/deepseek-r1-efficient-reinforcement-learning-with-grpo)).\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m2. **Group-Based Optimization**:  \n",
            "\u001b[0m\n",
            "\u001b[32m   GRPO calculates relative rewards within groups of outputs, enabling more efficient training and better alignment with human preferences. This approach is particularly effective for tasks requiring subjective or nuanced evaluations ([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/)).\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m3. **Enhanced Reasoning Capabilities**:  \n",
            "\u001b[0m\n",
            "\u001b[32m   GRPO has significantly improved DeepSeek's performance in reasoning-intensive benchmarks. For example:\n",
            "\u001b[0m\n",
            "\u001b[32m   - On the AIME 2024 math competition benchmark, DeepSeek-R1-Zero increased accuracy from 15.6% to 71.0%, and DeepSeek-R1 reached 79.8% accuracy after additional training.  \n",
            "\u001b[0m\n",
            "\u001b[32m   - On the MATH-500 benchmark, DeepSeek-R1 achieved 97.3% accuracy, surpassing most other open models ([LinkedIn](https://www.linkedin.com/pulse/grpo-game-changer-deepseeks-llms-sandeep-k-dzx2f)).\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m4. **Cost and Time Efficiency**:  \n",
            "\u001b[0m\n",
            "\u001b[32m   Training with GRPO requires fewer iterations, further lowering hardware utilization. For instance, DeepSeek-R1 completed training on 14.8 trillion tokens in under 2.8 million H800 GPU hours ([Martin Fowler](https://martinfowler.com/articles/deepseek-papers.html)).\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m### Applications of GRPO in DeepSeek\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32mGRPO has been instrumental in transforming DeepSeek's base models into advanced reasoning systems. Key applications include:\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m- **Mathematical Reasoning**: DeepSeekMath demonstrated the potential of GRPO in solving complex mathematical problems.  \n",
            "\u001b[0m\n",
            "\u001b[32m- **Competitive Coding**: DeepSeek-R1 achieved a 96.3 percentile ranking on Codeforces, a leading competitive programming platform ([LinkedIn](https://www.linkedin.com/pulse/grpo-game-changer-deepseeks-llms-sandeep-k-dzx2f)).  \n",
            "\u001b[0m\n",
            "\u001b[32m- **General Knowledge Tasks**: DeepSeek-R1 achieved 90.8% accuracy on the MMLU benchmark, making it one of the top-performing open models ([LinkedIn](https://www.linkedin.com/pulse/grpo-game-changer-deepseeks-llms-sandeep-k-dzx2f)).\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m---\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m## The Synergy Between MoE and GRPO\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32mThe combination of MoE and GRPO has been a game-changer for DeepSeek's LLMs. While MoE optimizes computational efficiency and scalability, GRPO enhances reasoning capabilities through efficient reinforcement learning. Together, these innovations have enabled DeepSeek to achieve industry-leading performance at a fraction of the cost associated with traditional architectures.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32mFor instance, DeepSeek-V3's MoE framework ensures that only the most relevant experts are activated for each input, minimizing computational overhead. Meanwhile, GRPO fine-tunes the model's reasoning capabilities, allowing it to excel in complex tasks such as mathematical problem-solving and competitive coding. This synergy has positioned DeepSeek as a leader in open-source LLM research and application ([DeepLearning.AI](https://www.deeplearning.ai/the-batch/deepseek-v3-redefines-llm-performance-and-cost-efficiency/)).\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m---\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m## Conclusion\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32mDeepSeek's Mixture of Experts (MoE) architecture and Group Relative Policy Optimization (GRPO) algorithm represent groundbreaking advancements in LLM research and application. By addressing the challenges of computational efficiency, scalability, and reasoning capabilities, these innovations have set new benchmarks for performance and cost-effectiveness in the AI industry. As technology continues to evolve, DeepSeek's MoE and GRPO architectures are likely to play a pivotal role in shaping the future of AI, making advanced language models more accessible and versatile than ever before.\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m---\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m## References\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[32m1. Modular AI Resources. (n.d.). Exploring DeepSeek-R1's Mixture-of-Experts Model Architecture. Retrieved from https://www.modular.com/ai-resources/exploring-deepseek-r1-s-mixture-of-experts-model-architecture  \n",
            "\u001b[0m\n",
            "\u001b[32m2. Yugen.ai Technology Blog. (2025, January). DeepSeek-V3 — Advances in MoE Load Balancing and Multi-Token Prediction Training. Retrieved from https://medium.com/yugen-ai-technology-blog/deepseek-v3-advances-in-moe-load-balancing-and-multi-token-prediction-training-f6d68c59749c  \n",
            "\u001b[0m\n",
            "\u001b[32m3. OpDeepSeek. (n.d.). DeepSeek-MoE Power Of Mixture of Experts Framework. Retrieved from https://opdeepseek.com/deepseek-moe/  \n",
            "\u001b[0m\n",
            "\u001b[32m4. Martin Fowler. (2025, February 6). The DeepSeek Series: A Technical Overview. Retrieved from https://martinfowler.com/articles/deepseek-papers.html  \n",
            "\u001b[0m\n",
            "\u001b[32m5. DeepLearning.AI. (2025, February). DeepSeek-V3 Redefines LLM Performance and Cost Efficiency. Retrieved from https://www.deeplearning.ai/the-batch/deepseek-v3-redefines-llm-performance-and-cost-efficiency/  \n",
            "\u001b[0m\n",
            "\u001b[32m6. Analytics Vidhya. (2025, February). A Deep Dive into LLM Optimization: From Policy Gradient to GRPO. Retrieved from https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/  \n",
            "\u001b[0m\n",
            "\u001b[32m7. LinkedIn. (2025, February). GRPO: A Game-Changer for DeepSeek’s LLMs. Retrieved from https://www.linkedin.com/pulse/grpo-game-changer-deepseeks-llms-sandeep-k-dzx2f  \n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     [22:29:37] 📝 Report written for 'Why DeepSeek's MoE and GRPO is a successful architecture in LLM research and application?'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m8. Oxen.ai. (n.d.). Why GRPO is Important and How it Works. Retrieved from https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/  \u001b[0m\n",
            "Report:\n",
            "# Why DeepSeek's MoE and GRPO is a Successful Architecture in LLM Research and Application\n",
            "\n",
            "## Introduction\n",
            "\n",
            "The field of artificial intelligence (AI) has witnessed significant advancements in recent years, particularly in the development of large language models (LLMs). Among the most noteworthy innovations are DeepSeek's Mixture of Experts (MoE) architecture and Group Relative Policy Optimization (GRPO) algorithm. These two innovations have not only enhanced the efficiency and scalability of LLMs but also redefined their performance in reasoning and computational tasks. This report delves into the reasons behind the success of DeepSeek's MoE and GRPO architectures in LLM research and application, supported by facts, figures, and insights from credible sources.\n",
            "\n",
            "---\n",
            "\n",
            "## Mixture of Experts (MoE): A Paradigm Shift in Neural Network Design\n",
            "\n",
            "### Overview of MoE Architecture\n",
            "\n",
            "The Mixture of Experts (MoE) architecture represents a significant departure from traditional dense neural networks. Instead of activating all parameters for every computation, MoE selectively activates only a subset of specialized sub-networks, known as \"experts,\" for each input. A gating mechanism dynamically routes inputs to the most relevant experts, optimizing computational efficiency while maintaining high performance ([Modular AI Resources](https://www.modular.com/ai-resources/exploring-deepseek-r1-s-mixture-of-experts-model-architecture)).\n",
            "\n",
            "DeepSeek's implementation of MoE is particularly noteworthy. For instance, the DeepSeek-V3 model comprises 671 billion parameters, but only 37 billion are activated during any given forward pass. This selective activation significantly reduces computational costs without compromising accuracy ([Yugen.ai Technology Blog](https://medium.com/yugen-ai-technology-blog/deepseek-v3-advances-in-moe-load-balancing-and-multi-token-prediction-training-f6d68c59749c)).\n",
            "\n",
            "### Key Advantages of MoE in DeepSeek\n",
            "\n",
            "1. **Enhanced Parameter Efficiency**:  \n",
            "   Traditional models activate all parameters for every input, leading to redundancy and inefficiency. In contrast, MoE selectively engages only the necessary experts, resulting in faster processing speeds and reduced energy consumption ([OpDeepSeek](https://opdeepseek.com/deepseek-moe/)).\n",
            "\n",
            "2. **Higher Expert Specialization**:  \n",
            "   Each expert in the MoE framework is trained on specific types of data, ensuring proficiency in its domain. This focused learning approach leads to better performance compared to models that attempt to generalize across all tasks ([OpDeepSeek](https://opdeepseek.com/deepseek-moe/)).\n",
            "\n",
            "3. **Improved Load Balancing**:  \n",
            "   DeepSeek's MoE incorporates techniques to distribute computational loads evenly across experts, preventing bottlenecks and ensuring scalability. For example, DeepSeek-V3 introduced auxiliary-loss-free load balancing, which dynamically adjusts expert utilization without dropping tokens ([Martin Fowler](https://martinfowler.com/articles/deepseek-papers.html)).\n",
            "\n",
            "4. **Cost Efficiency**:  \n",
            "   The training cost of DeepSeek-V3 was approximately $5.576 million, achieved through meticulous engineering optimizations and the use of FP8 mixed-precision training. This is an order of magnitude lower than the costs associated with comparable closed-source models ([DeepLearning.AI](https://www.deeplearning.ai/the-batch/deepseek-v3-redefines-llm-performance-and-cost-efficiency/)).\n",
            "\n",
            "### Applications of MoE in DeepSeek\n",
            "\n",
            "DeepSeek's MoE architecture has demonstrated exceptional versatility across various domains, including:\n",
            "\n",
            "- **Natural Language Processing (NLP)**: Improved text generation, summarization, and translation capabilities.  \n",
            "- **Finance**: Enhanced predictive analytics, risk assessment, and fraud detection.  \n",
            "- **Healthcare**: Advanced diagnostic capabilities through the analysis of large medical datasets ([OpDeepSeek](https://opdeepseek.com/deepseek-moe/)).\n",
            "\n",
            "---\n",
            "\n",
            "## Group Relative Policy Optimization (GRPO): Revolutionizing Reinforcement Learning\n",
            "\n",
            "### Overview of GRPO\n",
            "\n",
            "Group Relative Policy Optimization (GRPO) is a reinforcement learning (RL) algorithm designed to enhance the reasoning capabilities of LLMs. Unlike traditional RL methods such as Proximal Policy Optimization (PPO), GRPO eliminates the need for a separate critic model by comparing outputs within groups. This approach reduces memory and computational costs while improving training efficiency ([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/)).\n",
            "\n",
            "GRPO was first introduced in the \"DeepSeekMath\" paper and later applied to DeepSeek-R1, a state-of-the-art reasoning model. By leveraging GRPO, DeepSeek-R1 achieved remarkable improvements in reasoning tasks, including mathematical problem-solving and competitive coding ([Oxen.ai](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/)).\n",
            "\n",
            "### Key Advantages of GRPO in DeepSeek\n",
            "\n",
            "1. **Critic-Free RL**:  \n",
            "   Traditional RL methods rely on a critic model to estimate value functions, doubling memory and computational requirements. GRPO eliminates this overhead, reducing GPU memory usage by up to 50% ([DataOps Labs](https://blog.dataopslabs.com/deepseek-r1-efficient-reinforcement-learning-with-grpo)).\n",
            "\n",
            "2. **Group-Based Optimization**:  \n",
            "   GRPO calculates relative rewards within groups of outputs, enabling more efficient training and better alignment with human preferences. This approach is particularly effective for tasks requiring subjective or nuanced evaluations ([Analytics Vidhya](https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/)).\n",
            "\n",
            "3. **Enhanced Reasoning Capabilities**:  \n",
            "   GRPO has significantly improved DeepSeek's performance in reasoning-intensive benchmarks. For example:\n",
            "   - On the AIME 2024 math competition benchmark, DeepSeek-R1-Zero increased accuracy from 15.6% to 71.0%, and DeepSeek-R1 reached 79.8% accuracy after additional training.  \n",
            "   - On the MATH-500 benchmark, DeepSeek-R1 achieved 97.3% accuracy, surpassing most other open models ([LinkedIn](https://www.linkedin.com/pulse/grpo-game-changer-deepseeks-llms-sandeep-k-dzx2f)).\n",
            "\n",
            "4. **Cost and Time Efficiency**:  \n",
            "   Training with GRPO requires fewer iterations, further lowering hardware utilization. For instance, DeepSeek-R1 completed training on 14.8 trillion tokens in under 2.8 million H800 GPU hours ([Martin Fowler](https://martinfowler.com/articles/deepseek-papers.html)).\n",
            "\n",
            "### Applications of GRPO in DeepSeek\n",
            "\n",
            "GRPO has been instrumental in transforming DeepSeek's base models into advanced reasoning systems. Key applications include:\n",
            "\n",
            "- **Mathematical Reasoning**: DeepSeekMath demonstrated the potential of GRPO in solving complex mathematical problems.  \n",
            "- **Competitive Coding**: DeepSeek-R1 achieved a 96.3 percentile ranking on Codeforces, a leading competitive programming platform ([LinkedIn](https://www.linkedin.com/pulse/grpo-game-changer-deepseeks-llms-sandeep-k-dzx2f)).  \n",
            "- **General Knowledge Tasks**: DeepSeek-R1 achieved 90.8% accuracy on the MMLU benchmark, making it one of the top-performing open models ([LinkedIn](https://www.linkedin.com/pulse/grpo-game-changer-deepseeks-llms-sandeep-k-dzx2f)).\n",
            "\n",
            "---\n",
            "\n",
            "## The Synergy Between MoE and GRPO\n",
            "\n",
            "The combination of MoE and GRPO has been a game-changer for DeepSeek's LLMs. While MoE optimizes computational efficiency and scalability, GRPO enhances reasoning capabilities through efficient reinforcement learning. Together, these innovations have enabled DeepSeek to achieve industry-leading performance at a fraction of the cost associated with traditional architectures.\n",
            "\n",
            "For instance, DeepSeek-V3's MoE framework ensures that only the most relevant experts are activated for each input, minimizing computational overhead. Meanwhile, GRPO fine-tunes the model's reasoning capabilities, allowing it to excel in complex tasks such as mathematical problem-solving and competitive coding. This synergy has positioned DeepSeek as a leader in open-source LLM research and application ([DeepLearning.AI](https://www.deeplearning.ai/the-batch/deepseek-v3-redefines-llm-performance-and-cost-efficiency/)).\n",
            "\n",
            "---\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "DeepSeek's Mixture of Experts (MoE) architecture and Group Relative Policy Optimization (GRPO) algorithm represent groundbreaking advancements in LLM research and application. By addressing the challenges of computational efficiency, scalability, and reasoning capabilities, these innovations have set new benchmarks for performance and cost-effectiveness in the AI industry. As technology continues to evolve, DeepSeek's MoE and GRPO architectures are likely to play a pivotal role in shaping the future of AI, making advanced language models more accessible and versatile than ever before.\n",
            "\n",
            "---\n",
            "\n",
            "## References\n",
            "\n",
            "1. Modular AI Resources. (n.d.). Exploring DeepSeek-R1's Mixture-of-Experts Model Architecture. Retrieved from https://www.modular.com/ai-resources/exploring-deepseek-r1-s-mixture-of-experts-model-architecture  \n",
            "2. Yugen.ai Technology Blog. (2025, January). DeepSeek-V3 — Advances in MoE Load Balancing and Multi-Token Prediction Training. Retrieved from https://medium.com/yugen-ai-technology-blog/deepseek-v3-advances-in-moe-load-balancing-and-multi-token-prediction-training-f6d68c59749c  \n",
            "3. OpDeepSeek. (n.d.). DeepSeek-MoE Power Of Mixture of Experts Framework. Retrieved from https://opdeepseek.com/deepseek-moe/  \n",
            "4. Martin Fowler. (2025, February 6). The DeepSeek Series: A Technical Overview. Retrieved from https://martinfowler.com/articles/deepseek-papers.html  \n",
            "5. DeepLearning.AI. (2025, February). DeepSeek-V3 Redefines LLM Performance and Cost Efficiency. Retrieved from https://www.deeplearning.ai/the-batch/deepseek-v3-redefines-llm-performance-and-cost-efficiency/  \n",
            "6. Analytics Vidhya. (2025, February). A Deep Dive into LLM Optimization: From Policy Gradient to GRPO. Retrieved from https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/  \n",
            "7. LinkedIn. (2025, February). GRPO: A Game-Changer for DeepSeek’s LLMs. Retrieved from https://www.linkedin.com/pulse/grpo-game-changer-deepseeks-llms-sandeep-k-dzx2f  \n",
            "8. Oxen.ai. (n.d.). Why GRPO is Important and How it Works. Retrieved from https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/  \n",
            "\n",
            "Research Costs:\n",
            "0.11340420000000001\n",
            "\n",
            "Research Images:\n",
            "['https://adasci.org/wp-content/uploads/2023/01/happy-indian-university-student-walking-with-mobil-2021-08-27-16-35-34-utc-scaled.jpg', 'https://adasci.org/wp-content/uploads/2023/01/indian-man-in-office-portrait-2022-11-06-23-14-38-utc-scaled.jpg', 'https://adasci.org/wp-content/uploads/2024/02/handsome-man-smiling-wearing-a-suit-in-a-conversat-2023-11-27-05-15-51-utc-scaled.jpg', 'https://adasci.org/wp-content/uploads/2022/12/ADASCI-15-1-1.png', 'https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/image-7-2.webp', 'https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/DPO_OP4.webp', 'https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/GRPO_Eq4.webp', 'https://opdeepseek.com/wp-content/uploads/2025/02/Add-a-subheading-9.webp', 'https://ghost.oxen.ai/content/images/2025/02/2.png', 'https://ghost.oxen.ai/content/images/2025/02/5.png']\n",
            "\n",
            "Research Sources:\n",
            "[{'url': 'https://medium.com/@joshithagandra/understanding-group-relative-policy-optimization-grpo-powering-deepseekmath-and-deepseek-r1-df141c94c666', 'raw_content': 'Understanding GRPO: Powering DeepSeekMath and DeepSeek-R1 | Medium\\nOpen in app\\nSign up\\nSign in\\nWrite\\nSign up\\nSign in\\nUnderstanding Group Relative Policy Optimization (GRPO): Powering DeepSeekMath and DeepSeek-R1\\nJoshitha gandra\\n·\\nFollow\\n4 min read\\n·\\nJan 31, 2025\\n--\\nListen\\nShare\\nIntroduction\\nReinforcement learning (RL) has become essential in developing AI models for complex problem-solving, especially in tasks that require mathematical and logical reasoning. Traditional RL methods, such as Proximal Policy Optimization (PPO), have been widely adopted due to their simplicity and effectiveness. However, PPO comes with several limitations, such as its reliance on a critic model, which increases memory usage and computational cost.\\nIn response to these challenges,\\nGroup Relative Policy Optimization (GRPO)\\noffers a smarter and more efficient alternative. By eliminating the critic model and utilizing group-based relative rewards, GRPO optimizes policies with reduced memory overhead and improved stability.\\nIn this post, we will:\\nExplore the PPO algorithm and its limitations.\\nIntroduce GRPO and highlight its key improvements.\\nPresent the mathematical formulations and algorithms as detailed in the paper.\\nLet’s dive in!\\n1. Proximal Policy Optimization (PPO): How AI Learns Through Reinforcement Learning\\nPPO is one of the most commonly used\\npolicy gradient algorithms\\nin reinforcement learning. It is an\\nactor-critic method\\n, meaning it has two components:\\nPolicy Model (Actor):\\nGenerates actions based on the current state.\\nValue Model (Critic):\\nEstimates the advantage (expected future reward) to improve the policy.\\n1.1 PPO Objective Function\\nPPO optimizes the policy by maximizing the following function:\\n1.2 How Rewards Are Calculated in PPO\\nRewards are calculated using a\\nreward model\\nwhich assigns a score to an output based on its quality. In PPO:\\nThe\\ncritic network\\nestimates a baseline value V(s).\\nThe\\nadvantage function\\nmeasures how much better an action is compared to the baseline.\\nThe\\npolicy loss function\\nupdates the model by maximizing the expected advantage.\\n1.3 Issues with PPO\\nWhile effective, PPO suffers from\\nseveral drawbacks\\n:\\nRequires a critic model\\nV(s), increasing memory usage.\\nSensitive to value function errors\\n, leading to unstable training.\\nComputationally expensive\\n, limiting scalability.\\n2. Group Relative Policy Optimization (GRPO): A Smarter Alternative\\nGRPO\\nremoves the critic model\\nby comparing generated responses\\nwithin a group\\ninstead of relying on an absolute value function.\\n2.1 Why GRPO?\\nUnlike PPO, which estimates rewards using a critic network, GRPO:\\nEliminates the critic\\n, reducing memory overhead.\\nUses group-based relative rewards\\n, improving stability.\\nOptimizes policy updates\\nby leveraging multiple sampled outputs per query.\\n2.2 How Rewards Are Calculated in GRPO\\nInstead of relying on a single estimated value, GRPO\\ncompares multiple generated solutions\\nwithin a batch:\\n2.3 GRPO Objective Function\\nGRPO optimizes the policy πθ\\u200b by maximizing the following objective function:\\nKey Components\\n:\\nProbability Ratio Calculation\\n:\\nRelative Advantage Calculation\\n:\\nClipping Mechanism for Stability\\n:\\nKL Regularization to Prevent Overfitting\\n:\\n2.4 Algorithm for GRPO (from the DeepseekMath Paper)\\n3. PPO vs GRPO\\nValue Model\\n: PPO incorporates a Value Model, whereas GRPO uses group scores to estimate baselines.\\nEfficiency\\n: GRPO’s group computation significantly reduces training resources and enhances efficiency compared to PPO.\\nRewards Handling\\n: PPO deals with single outputs and rewards, while GRPO efficiently aggregates multiple outputs and rewards.\\nKL Divergence\\n: In PPO, KL divergence is part of the reward signal, but in GRPO, it is directly included in the loss function.\\nConclusion\\nGRPO is an advanced\\nreinforcement learning\\ntechnique that optimizes AI models\\nwithout a critic model\\n, making it\\nfaster, more stable, and computationally efficient\\n. It’s particularly useful for training AI models for\\nmathematical and logical reasoning\\n.\\nLlm\\nReinforcement Learning\\nDeepseek\\nDeep Learning\\nLarge Language Models\\nFollow\\nWritten by\\nJoshitha gandra\\n2 Followers\\n·\\n5 Following\\nFollow\\nNo responses yet\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams', 'image_urls': [], 'title': 'Understanding GRPO: Powering DeepSeekMath and DeepSeek-R1 | Medium'}, {'url': 'https://blog.dataopslabs.com/deepseek-r1-efficient-reinforcement-learning-with-grpo', 'raw_content': 'Efficient Learning: DeepSeek R1 with GRPO\\nDeepSeek R1: Efficient Reinforcement Learning with GRPO\\nDataOps Labs\\n·\\nJan 28, 2025\\n·\\n4\\nmin read\\nIntroduction\\nIn the evolving world of artificial intelligence (AI), efficient model training is crucial for achieving top-tier performance without spiraling hardware costs.\\nDeepSeek R1\\n, a state-of-the-art reasoning model, stands out for its innovative use of\\nReinforcement Learning (RL)\\nand\\nGroup Relative Policy Optimization (GRPO)\\n. This blog dives into what RL is, explains the GRPO technique, and demonstrates how DeepSeek R1 transforms reasoning tasks with unparalleled efficiency. We\\'ll also explore a travel company use case to highlight its real-world applications.\\nWhat is Reinforcement Learning (RL)?\\nReinforcement Learning is a machine learning paradigm where an agent learns to perform tasks by interacting with an environment and receiving rewards or penalties based on its actions.\\nKey Concepts\\n:\\nAgent\\n: The decision-making system (e.g., DeepSeek R1).\\nEnvironment\\n: The system the agent interacts with (e.g., a customer query system).\\nReward\\n: Feedback on how well the agent performs (e.g., customer satisfaction).\\nPolicy\\n: A strategy the agent uses to decide its next action.\\nHow RL Works\\n:\\nThe agent explores actions to maximize cumulative rewards over time.\\nRL algorithms like\\nProximal Policy Optimization (PPO)\\nare often used but require significant computational resources, especially for large models.\\nWhat is GRPO?\\nGroup Relative Policy Optimization (GRPO)\\nis a lightweight, efficient RL algorithm designed to optimize large models like DeepSeek R1. Unlike traditional RL methods (e.g., PPO) that rely on separate critic models to estimate value functions, GRPO avoids this additional overhead by comparing outputs within groups.\\nKey Features of GRPO\\n:\\nGroup-Level Baselines\\n: Instead of using a critic model, GRPO calculates relative rewards within a group of outputs.\\nSimplified Training\\n: Reduces computational complexity by focusing on optimizing the best-performing responses in a group.\\nClipping Mechanism\\n: Stabilizes updates and prevents overfitting to specific outputs.\\nEfficiency\\n: GRPO significantly reduces the hardware requirements for RL, making it ideal for large-scale models like DeepSeek R1.\\nRole of RL in DeepSeek R1\\nDeepSeek R1 leverages RL, powered by GRPO, to enhance its reasoning capabilities across diverse tasks. Here’s how RL contributes:\\nIncentivizing Reasoning\\n:\\nDeepSeek R1 learns reasoning strategies through RL, improving its ability to handle complex tasks like math problems, coding, and logical queries.\\nOptimized Exploration\\n:\\nGRPO allows the model to focus on exploring better solutions within smaller groups, increasing efficiency.\\nAdaptation to Real-World Scenarios\\n:\\nRL enables DeepSeek R1 to refine its responses to align with specific objectives, such as customer satisfaction or problem-solving accuracy.\\nTravel Company Use Case: Personalized Itinerary Generation\\nImagine a travel company that uses DeepSeek R1 to automate customer itinerary planning. Here\\'s how RL and GRPO make this possible:\\nScenario\\n:\\nA customer queries:\\n\"Plan a 5-day trip to Japan, including Kyoto and Tokyo, focusing on cultural experiences and staying under $2,000.\"\\nDeepSeek R1 Process\\n:\\nResponse Sampling\\n: The model generates multiple itineraries with varying details.\\nItinerary 1: Emphasizes Kyoto’s temples and Tokyo’s museums.\\nItinerary 2: Includes a balance of cultural landmarks and local cuisines.\\nItinerary 3: Focuses on budget-friendly travel options.\\nReward Assignment\\n: Each itinerary is evaluated based on criteria like cost, cultural richness, and adherence to customer preferences.\\nGRPO Optimization\\n:\\nThe model compares the generated itineraries and updates its policy to prioritize itineraries with higher rewards.\\nOutcome\\n:\\nThe system delivers the best itinerary that balances cost, cultural experiences, and customer satisfaction.\\nCost Efficiency: How DeepSeek R1 Reduces Hardware Costs\\nCritic-Free RL\\n:\\nGRPO eliminates the need for a separate critic model, cutting memory and compute costs by up to 50%.\\nGroup-Based Optimization\\n:\\nBy focusing on relative rewards within small groups, GRPO minimizes computational overhead.\\nEfficient Use of Resources\\n:\\nDeepSeek R1 uses FP8 mixed-precision training and memory-saving techniques, reducing GPU requirements for both training and inference.\\nTraining Time\\n:\\nTraining with GRPO is faster and requires fewer iterations, further lowering hardware utilization.\\nConclusion\\nDeepSeek R1 represents a significant leap in reasoning AI by combining the power of RL with the efficiency of GRPO. Its applications, from automated reasoning tasks to real-world scenarios like travel planning, demonstrate its versatility and impact. By optimizing resource usage, DeepSeek R1 makes high-performance AI accessible to organizations looking to innovate without breaking the bank.\\nIf you\\'re exploring efficient AI solutions for reasoning tasks, DeepSeek R1 and GRPO offer a transformative approach to achieving exceptional performance at a fraction of the cost.\\nDeepSeekR1\\nShare this', 'image_urls': [], 'title': 'Efficient Learning: DeepSeek R1 with GRPO'}, {'url': 'https://medium.com/@sahin.samia/the-math-behind-deepseek-a-deep-dive-into-group-relative-policy-optimization-grpo-8a75007491ba', 'raw_content': 'The Math Behind DeepSeek: A Deep Dive into Group Relative Policy Optimization (GRPO) | by Sahin Ahmed, Data Scientist | Jan, 2025 | Medium\\nOpen in app\\nSign up\\nSign in\\nWrite\\nSign up\\nSign in\\nMastodon\\nThe Math Behind DeepSeek: A Deep Dive into\\nGroup Relative Policy Optimization (GRPO)\\nSahin Ahmed, Data Scientist\\n·\\nFollow\\n6 min read\\n·\\nJan 26, 2025\\n--\\n8\\nListen\\nShare\\nThis blog dives into the math behind\\nGroup Relative Policy Optimization (GRPO)\\n, the core reinforcement learning algorithm that drives DeepSeek’s exceptional reasoning capabilities. We’ll break down how GRPO works, its key components, and why it’s a game-changer for training advanced Large Language Models (LLMs).\\nThe Foundation of GRPO\\nWhat is GRPO?\\nGroup Relative Policy Optimization (GRPO) is a reinforcement learning (RL) algorithm specifically designed to enhance reasoning capabilities in Large Language Models (LLMs). Unlike traditional RL methods, which rely heavily on external evaluators (critics) to guide learning, GRPO optimizes the model by evaluating groups of responses relative to one another. This approach enables more efficient training, making GRPO ideal for reasoning tasks that require complex problem-solving and long chains of thought.\\nWhy GRPO?\\nTraditional RL methods like Proximal Policy Optimization (PPO) face significant challenges when applied to reasoning tasks in LLMs:\\nDependency on a Critic Model\\n:\\nPPO requires a separate critic model to estimate the value of each response, which doubles memory and computational requirements.\\nTraining the critic is complex and prone to errors, especially for tasks with subjective or nuanced evaluations.\\nHigh Computational Cost\\n:\\nRL pipelines often demand significant computational resources to evaluate and optimize responses iteratively.\\nScaling these methods to large LLMs exacerbates these costs.\\nScalability Issues\\n:\\nAbsolute reward evaluations struggle with diverse tasks, making it hard to generalize across reasoning domains.\\nHow GRPO Addresses These Challenges\\n:\\nCritic-Free Optimization\\n: GRPO removes the need for a critic model by comparing responses within a group, significantly reducing computational overhead.\\nRelative Evaluation\\n: Instead of relying on an external evaluator, GRPO uses group dynamics to assess how well a response performs relative to others in the same batch.\\nEfficient Training\\n: By focusing on group-based advantages, GRPO simplifies the reward estimation process, making it faster and more scalable for large models.\\nKey Idea\\nAt the heart of GRPO is the concept of\\nrelative evaluation\\n:\\nFor each input query, the model generates a group of potential responses.\\nThese responses are scored based on how they compare to others in the group, rather than being evaluated in isolation.\\nThe\\nadvantage\\nof a response reflects how much better or worse it is relative to the group’s average performance.\\nThis approach eliminates the need for a separate critic, making GRPO both efficient and robust. By fostering competition within groups, GRPO drives the model to consistently improve its reasoning capabilities. It’s this innovation that powers DeepSeek’s ability to achieve exceptional results in reasoning tasks.\\nUnderstanding the GRPO Objective Function\\nThe objective function in Group Relative Policy Optimization (GRPO) defines how the model learns to improve its policy, driving its ability to generate high-quality responses. Let’s break it down step by step.\\nThe GRPO Objective Function\\nEquations are from the paper and explanation was created with the help from GPT-4o\\nUnderstanding the GRPO Objective Function in Simple Terms\\nThe GRPO (Group Relative Policy Optimization) objective function is like a recipe for teaching a model to get better at generating answers by comparing its own responses and improving step by step. Let’s break it down into an easy-to-follow explanation:\\nThe Goal\\nImagine you’re teaching a group of students to solve a math problem. Instead of just telling them who got the right or wrong answer, you compare all their answers to figure out who did the best (and why). Then, you help them learn by rewarding better approaches and improving weaker ones. This is exactly what GRPO does — only it teaches AI models, not students.\\nStep-by-Step Breakdown\\nStep 1: Start with a Query\\nPick a query (q) from the training dataset P(Q)\\nExample\\n: Let’s say the query is\\n“What is the sum of 8 + 5?”\\nStep 2: Generate a Group of Responses\\nThe model generates a group of GGG responses to the query.\\nExample\\n: The model generates these responses:\\no1\\u200b: “The answer is 13.”\\no2\\u200b: “Thirteen.”\\no3\\u200b: “It’s 12.”\\no4: “The sum is 13.”\\nStep 3: Calculate Rewards for Each Response\\nWhat are Rewards?\\n:\\nRewards guide the model’s learning by quantifying the quality of its responses.\\nTypes of Rewards in GRPO\\n:\\nAccuracy Rewards\\n: Based on the correctness of the response (e.g., solving a math problem).\\nFormat Rewards\\n: Ensures the response adheres to structural guidelines (e.g., reasoning enclosed in\\n<think>\\ntags).\\nLanguage Consistency Rewards\\n: Penalizes language mixing or incoherent formatting.\\nAssign a\\nreward (ri)\\nto each response based on how good it is. For example Rewards could depend on:\\nAccuracy\\n: Is the answer correct?\\nFormat\\n: Is the response well-structured?\\nExample\\n:\\nr1=1.0 (correct and well-formatted).\\nr2=0.9 (correct but less formal).\\nr3=0.0 (incorrect answer).\\nr4=1.0 (correct and well-formatted).\\nStep 4: Compare Responses (Group Advantage)\\nCalculate the\\nadvantage (Ai\\u200b)\\nof each response relative to the group:\\nEquations are from the paper and explanation was created with the help from GPT-4o\\nIn simple way you can understand it like this\\nEquations are from the paper and explanation was created with the help from GPT-4o\\nResponses better than the group’s average get positive scores, while worse responses get negative scores.\\nEncourages competition within the group, driving the model to generate better responses.\\nStep 5: Update Policy with Clipping\\nEquations are from the paper and explanation was created with the help from GPT-4o\\nExample\\n: If the new policy starts assigning too much probability to o1, clipping ensures it doesn’t overemphasize this response.\\nEnables steady and reliable policy optimization, even in complex tasks like reasoning.\\nStep 6: Penalize Deviations with KL Divergence\\nEquations are from the paper and explanation was created with the help from GPT-4o\\nPutting It All Together\\nThe GRPO objective works as follows:\\nGenerate a group of responses\\nfor a query.\\nCalculate rewards\\nfor each response based on predefined criteria (e.g., accuracy, format).\\nCompare responses within the group\\nto calculate their relative advantage (AiA_iAi\\u200b).\\nUpdate the policy\\nto favor responses with higher advantages, ensuring stability with clipping.\\nRegularize the updates\\nto prevent the model from drifting too far from its baseline.\\nWhy GRPO is Effective\\nNo Critic Needed\\n: GRPO avoids the need for a separate evaluator by relying on group comparisons, reducing computational costs.\\nStable Learning\\n: Clipping and KL regularization ensure the model improves steadily without wild fluctuations.\\nEfficient Training\\n: By focusing on relative performance, GRPO is ideal for tasks like reasoning, where absolute scoring is difficult.\\nReal-Life Analogy\\nImagine a group of students solving a problem. Instead of a teacher grading each student individually, the students compare their answers among themselves. Those with better answers get encouraged, while others learn from their mistakes. Over time, the group collectively improves, becoming more accurate and consistent. GRPO applies this principle to train AI models, enabling them to learn effectively and efficiently.\\nComparison of GRPO and PPO\\nGRPO in Action: DeepSeek’s Success\\nGRPO drives DeepSeek’s exceptional performance by enabling efficient and scalable training for reasoning tasks. Here’s how it translates into success:\\nEnhanced Reasoning\\n: GRPO allowed\\nDeepSeek-R1-Zero\\nto achieve a\\nPass@1 score of 71.0%\\non AIME 2024, increasing to\\n86.7%\\nwith majority voting. It rivaled proprietary models like OpenAI in solving math and logic problems.\\nEmergent Capabilities\\n: Through GRPO, DeepSeek models developed advanced reasoning behaviors like\\nself-verification\\n,\\nreflection\\n, and\\nlong chains of thought\\n, critical for tackling complex tasks.\\nScalability\\n: GRPO’s group-based optimization removed the need for a critic model, reducing computational overhead and enabling training at scale.\\nDistillation Success\\n: Smaller models, distilled from GRPO-trained checkpoints, retained high reasoning capabilities, making AI more accessible and cost-effective.\\nBy focusing on relative performance within groups, GRPO empowers DeepSeek to set new benchmarks in reasoning, long-context understanding, and general AI tasks, all while maintaining efficiency and scalability\\nReference: https://arxiv.org/abs/2501.12948\\nDeep Learning\\nReinforcement Learning\\nArtificial Intelligence\\nLarge Language Models\\nData Science\\nFollow\\nWritten by\\nSahin Ahmed, Data Scientist\\n732 Followers\\n·\\n171 Following\\nLifelong learner passionate about AI, LLMs, Machine Learning, Deep Learning, NLP, and Statistical Modeling to make a meaningful impact. MSc in Data Science.\\nFollow\\nResponses (\\n8\\n)\\nSee all responses\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams', 'image_urls': [], 'title': 'The Math Behind DeepSeek: A Deep Dive into Group Relative Policy Optimization (GRPO) | by Sahin Ahmed, Data Scientist | Jan, 2025 | Medium'}, {'url': 'https://epichka.com/blog/2025/grpo/', 'raw_content': 'Group Relative Policy Optimization (GRPO) Illustrated Breakdown | Ebrahim Pichka\\nGroup Relative Policy Optimization (GRPO) Illustrated Breakdown\\nA simplified intro to GRPO, an efficient policy optimization method used for LLM reasoning training\\nIntroduction\\nReinforcement Learning (RL) has emerged as a powerful tool for enhancing Large Language Models (LLMs) after their initial training, particularly in reasoning-intensive tasks. DeepSeek’s recent breakthroughs with DeepSeek-Math [2] and DeepSeek-R1 [3] models have demonstrated the remarkable potential of RL in improving mathematical reasoning and problem-solving abilities of LLMs.\\nThese achievements were made possible through an innovative RL approach called Group Relative Policy Optimization (GRPO), which addresses the unique challenges of applying RL to language models. In this post, we’ll dive deep into how GRPO works and why it represents a significant advancement in LLM training.\\nPPO vs GRPO\\nProximal Policy Optimization (PPO) [1] has been the go-to algorithm for RL fine-tuning of language models. At its core, PPO is a policy gradient method that uses clipping to limit policy updates (gradients), preventing destructive large policy changes. The objective function for PPO can be written as:\\n\\\\[J_{PPO}(\\\\theta) = \\\\mathbb{E}[s \\\\sim P(S), a \\\\sim \\\\pi_{\\\\theta_{old}}(A\\\\mid s)] \\\\left[\\\\min\\\\left(\\\\frac{\\\\pi_\\\\theta(a\\\\mid s)}{\\\\pi_{\\\\theta_{old}}(a\\\\mid s)}A(s,a), \\\\text{clip}\\\\left(\\\\frac{\\\\pi_\\\\theta(a\\\\mid s)}{\\\\pi_{\\\\theta_{old}}(a\\\\mid s)}, 1-\\\\epsilon, 1+\\\\epsilon\\\\right)A(s,a)\\\\right)\\\\right]\\\\]\\nWhere GRPO, introduced in [2] builds upon PPO’s foundation but introduces several key innovations that make it more\\nefficient\\nand better suited for language models:\\nEliminates the need for a value network, hence\\nless memory/compute\\nusage\\nUses group sampling for more efficient stable\\nadvantage estimation\\nUses a more conservative update mechanism through further penalizing both the objective and the rewards\\nGRPO: A Closer Look\\nLLM as a Policy\\nIn GRPO, the language model serves as the policy network (actor), taking a question \\\\(q\\\\) as input observation \\\\(s\\\\) and producing a sequence of tokens as actions. The policy distribution factors across tokens:\\n\\\\[\\\\pi_\\\\theta(a\\\\mid q) = \\\\prod_{t=1}^N \\\\pi_\\\\theta(a_t \\\\mid q, a_{ < t})\\\\]\\nNote:\\nIn the original paper [2], they use \\\\(o_t\\\\) to denote the output token at timestep \\\\(t\\\\). Whereas we use \\\\(a_t\\\\) instead to conform with standard RL notation of\\naction\\n.\\nSequential Token Generation\\nThe generation process is inherently sequential because of auto-regressive nature of transformers/LLMs:\\nEach token is generated conditionally on previous tokens\\nThe policy network (LLM) maintains a running context\\nEach token generation step can be viewed as an action \\\\(a_t\\\\) in the RL framework\\nReward and Advantage Calculation\\nFor each generated sequence, GRPO computes per-token rewards as follows:\\n\\\\[r_t = r_\\\\phi(q,a_{\\\\leq t}) - \\\\beta \\\\log\\\\frac{\\\\pi_\\\\theta(a_t \\\\mid q,a_{ < t})}{\\\\pi_{ref}(a_t \\\\mid q, a_{ < t})}\\\\]\\nInstead of using a value network, GRPO estimates baseline advantages \\\\(A\\\\) by normalizing a group (batch) of\\nrewards\\nobtained from\\nsampling multiple different outputs\\nfrom the\\nreference policy\\nproduced in response to the same question as input. :\\n\\\\[\\\\hat{A}_{i,t} = \\\\tilde{r}_i = \\\\frac{r_i - \\\\text{mean}(r)}{\\\\text{std}(r)}\\\\]\\nThe GRPO Objective\\nfor each question 𝑞, GRPO samples a group of outputs {𝑜1, 𝑜2, · · · , 𝑜𝐺} from the old policy 𝜋𝜃𝑜𝑙𝑑 and then optimizes the policy model by maximizing the GRPO objective. The complete GRPO objective brings everything together:\\n\\\\[J_{GRPO}(\\\\theta) = \\\\frac{1}{G}\\\\sum_{i=1}^G \\\\frac{1}{\\\\mid a_i \\\\mid}\\\\sum_{t=1}^{\\\\mid a_i \\\\mid} \\\\left\\\\{\\\\min\\\\left[\\\\frac{\\\\pi_\\\\theta(a_{i,t} \\\\mid s,a_{i, < t})}{\\\\pi_{\\\\theta_{old}}(a_{i, t}\\\\mid s, a_{i, < t})}\\\\hat{A}_{i,t}, \\\\text{clip}\\\\left(\\\\frac{\\\\pi_\\\\theta(a_{i,t}\\\\mid s, a_{ i, < t })}{\\\\pi_{\\\\theta_{old}}(a_{i, t} \\\\mid s, a_{i, < t})}, 1-\\\\epsilon, 1+\\\\epsilon\\\\right)\\\\hat{A}_{i,t}\\\\right] - \\\\beta D_{KL}[\\\\pi_\\\\theta \\\\mid \\\\mid \\\\pi_{ref}]\\\\right\\\\}\\\\]\\nThis objective:\\nAverages over both groups and sequence lengths\\nUses clipping for conservative updates\\nIncludes an\\nestimate\\nof the KL divergence as a penalty to prevent large deviations from the reference model\\nConclusion\\nGRPO represents a significant advancement in applying RL to language models. By eliminating the need for a value network and introducing group-relative advantage estimation, it provides a more efficient and stable training process. The success of DeepSeek-Math and DeepSeek-R1 demonstrates the practical benefits of this approach.\\nThe key innovations of GRPO - group sampling, relative advantage estimation, and the elimination of the value network - provide a blueprint for future developments in LLM training. As we continue to push the boundaries of what language models can achieve, techniques like GRPO will be crucial in unlocking their full potential.\\nReferences\\n[1] Schulman, John, et al. Proximal Policy Optimization Algorithms. arXiv:1707.06347, arXiv, 28 Aug. 2017. arXiv.org, https://doi.org/10.48550/arXiv.1707.06347.\\n[2] Shao, Zhihong, et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300, arXiv, 27 Apr. 2024. arXiv.org, https://doi.org/10.48550/arXiv.2402.03300.\\n[3] DeepSeek-AI, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, arXiv, 22 Jan. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2501.12948.\\nPlease enable JavaScript to view the\\ncomments powered by giscus.', 'image_urls': [], 'title': 'Group Relative Policy Optimization (GRPO) Illustrated Breakdown | Ebrahim Pichka'}, {'url': 'https://medium.com/@magalareuben60/group-relative-policy-optimisation-grpo-the-reinforcement-learning-algorithm-behind-deepseek-954588a0ba07', 'raw_content': 'Group Relative Policy Optimisation (GRPO): The Reinforcement learning algorithm behind deepseek. | by Magalareuben | Jan, 2025 | Medium\\nOpen in app\\nSign up\\nSign in\\nWrite\\nSign up\\nSign in\\nGroup Relative Policy Optimisation (GRPO): The Reinforcement learning algorithm behind deepseek.\\nMagalareuben\\n·\\nFollow\\n7 min read\\n·\\nJan 31, 2025\\n--\\n1\\nListen\\nShare\\nI\\nntroduced in April 2024 in the paper:\\nDeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\\nfrom DeepSeek-AI, Tsinghua University and Peking University.\\nGroup Relative Policy Optimization (GRPO)\\nis a reinforcement learning algorithm that builds upon Proximal Policy Optimization (PPO) algorithim, an algorithim that was introduced back in 2017 by openAI for reinforcement learning tasks but later proved to be helpful in LLMs through reinforcement learning with human feedback. GRPO is a more simple and efficient algorithm that was initially designed to improve mathematical reasoning capabilities while reducing memory consumption in large language models and in this article we will explore how this all comes to play but before we proceed for those not familiar with Reinforcement learning, in reinforcement learning we have something called the policy, in terms of LLMs, the policy is the model’s learned strategy that maps input text (states) to output text (actions). In simple terms the policy is like a brain that tells the model what to do given a certain state. Most of the methods in RL are geared towards optimizing this policy, i.e to a policy (optimal policy) that will give us the best actions given a certain state.\\nWhat did GRPO actually solve\\nEarlier RL algorithms like PPO heavily relied on an external evaluator known as the critic i.e a separate value function model that estimates the value\\n(a total sum of rewards the actor will get given it takes actions in a given state)\\nin our case the LLM generating text is the actor, this critic in short provided feedback on how good a particular response is. However this method required more memory and computational resources since a critic is a separate model. On top of that, the values estimated by the critic in PPO are just approximations, not exact calculated values. The critic network learns to estimate the expected return (all future rewards) from a given state, but this estimation is sometimes noisy and uncertain. This makes training the critic very complex and also error prone. So when you put this in terms of training costs, GRPO actually reduces them in such a way that you aren’t going to need significant computational requirements to evaluate responses and also in training the critic. And also since the values by the critic in PPO are just estimates, LLLMs trained on this method might struggle to generalize across reasoning domains.\\nSo in short:\\nEarlier RL algorithms like PPO relied on a separate critic model to estimate the value of an action, providing feedback to the actor (LLM), but this approach required significant memory and computation resources.\\nThe critic’s value estimates in PPO were approximations, often noisy and uncertain, making training complex and error-prone.\\nGRPO reduces training costs by eliminating the need for a separate critic model, leading to lower computational requirements and better generalization across reasoning domain\\nAdvantages of GRPO\\nNo value function estimator (critic) required\\n: this reduces memory and computational requirements.\\nGroup-Based Advantage Calculation\\n: Remember LLMs basically predict the next word and in order to evaluate how good the next word is in GRPO we don’t rely on a critic but rather we calculate the reward\\n(Remember a reward in RL is like a numerical value given to an agent after it takes an action, signifying how well that action performed in a particular state in our case the LLM generates text as the action and reward tells it how good or how bad is the text it generated)\\nbasing on average score of the group. i.e all the previous words generated as we will see later, this makes GRPO speed up scale up reward estimation by using group-based advantages\\nEfficient Training\\n: In previous methods, the\\nKL divergence term\\nwas added directly to the reward, you can think of the KL divergence term as a way to measure how much a new policy’s probability distribution differs from an older established policy. It helps the RL agent to smoothly transition between different behaviors while avoiding drastic changes that could lead to instability. GRPO integrates this term directly into the loss function. This adjustment helps stabilize the training process and improve performance in the long run\\nHow GRPO works on a simpler level\\nFor each input of the question, the agent generates various outputs using the current policy.\\nEach of these outputs is scored relative to the other outputs in a similar group i.e each output is given rewards basing on how well it performs compared to the others rather than being scored individually. (e.g which of the output in all the outputs is the best solution to a math question, which algorithm runs faster on leet code in a coding question e.t.c)\\nThe average of these rewards is used as a baseline to compute the advantages i.e We use this average to calculate the relative advantage of each response in the group compared to another. This helps us get rid of the critic and rather use the entire group to calculate the advantage and rewards.\\nUnderstanding how GRPO works on a deeper level\\nThis section is more of a mathematical section, so if you are not interested in it, you can directly jump to the section of understanding GRPO using a real life example.\\nUnderstanding GRPO Through a real life example\\nImagine you are coaching a team for a debate competition though in our case we are training an LLM to give the best responses based on the given queries. You want to train your team members to improve their debating capabilities over time. However, instead of just telling them who won like the critic does in PPO, you follow a structured approach, similar to how GRPO optimizes a language model.\\nStep 1: The Debate Question (Query Selection)\\nJust like in training a language model, we need a prompt to respond to.\\nExample:\\nThe debate topic is: “Should schools switch to a four day school week?”\\nThis is like selecting a query (q) from the training dataset P(Q) that the model will train on.\\nStep 2: Multiple Responses from the Team (Generating a Group of Responses)\\nEach student on the team provides their own answer or reasoning in response to the debate question.\\nResponses:\\nStudent A:\\n“A four day school week improves student focus and reduces burnout.”\\nStudent B:\\n“It may reduce costs for schools but could also lead to longer school days.”\\nStudent C:\\n“It’s a bad idea because students need consistent learning schedules.”\\nStudent D:\\n“Studies show that a four-day week can improve attendance rates.”\\nSimilarly, in GRPO, the language model generates multiple responses to a given query.\\nStep 3: Evaluating Responses (Reward Calculation)\\nNow, we measure how good each response is based on clarity, correctness, relevance and other factors we may put in place. This is like assigning a reward to each response.\\nRewards Example (on a scale from 0 to 1):\\nStudent A:\\n1.0 (Strong argument, well-explained)\\nStudent B:\\n0.9 (Balanced perspective, but lacks depth)\\nStudent C:\\n0.0 (Too vague, not well-supported)\\nStudent D:\\n1.0 (Uses research-based evidence)\\nIn GRPO, rewards could be based on accuracy, how well an answer is formatted, how fast or accurate code runs on a certain problem, language consistency. each response within the group is assessed based on already predefined criteria e.g\\nAccuracy Reward:\\nDetermined by the correctness of the final answer. e.g in mathematical tasks, if the model’s boxed answer is correct, it receives a reward. otherwise, it does not.\\nFormat Reward:\\nEvaluated by how well the response adheres to a specified format. DeepSeek enforced a structure where the model’s reasoning process is enclosed within tags, and the final answer within tags. Responses that were of this structure received additional rewards.\\nLanguage Consistency Reward:\\nAssessed by the percentage of tokens in the target language, ensuring that the model’s output remains in a single language without mixing.\\nStep 4: Comparing Responses (Advantage Calculation)\\nInstead of evaluating responses independently, we will compare them relative to the entire group of the generated responses\\nStep 5: Updating the Policy with Clipping (Controlled Learning)\\nAfter getting the scores we would want our team to improve without making drastic changes. Instead of forcing everyone to copy the best student, you encourage gradual improvement by adjusting their responses slightly.\\nStep 6: Penalizing Overconfident Changes (KL Divergence)\\nIf a student suddenly starts dominating all debates with their somehow similar responses, it might mean they’re overfitting to a specific style. To prevent this, you introduce a penalty that ensures balanced learning. In GRPO, this is done using\\nKL Divergence\\n, which prevents the model from moving too far from previous policies.\\nConclusion\\nThis algorithm drastically reduced the costs of training for deepseek, improved it’s reasoning abilities, context understanding and other AI tasks. This was novel in that deepseek took the AI world by storm. Here is how the model performed compared to previous models.\\nReinforcement Learning\\nLarge Language Models\\nArtificial Intelligence\\nMachine Learning\\nData Science\\nFollow\\nWritten by\\nMagalareuben\\n1 Follower\\n·\\n11 Following\\nAI enginner and Data Scientist.\\nhttps://x.com/ReubenCosmos\\nFollow\\nResponses (\\n1\\n)\\nSee all responses\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams', 'image_urls': [], 'title': 'Group Relative Policy Optimisation (GRPO): The Reinforcement learning algorithm behind deepseek. | by Magalareuben | Jan, 2025 | Medium'}, {'url': 'https://medium.com/yugen-ai-technology-blog/deepseek-v3-advances-in-moe-load-balancing-and-multi-token-prediction-training-f6d68c59749c', 'raw_content': \"DeepSeek-V3 — Advances in MoE Load Balancing and Multi-Token Prediction Training | by Yugen.ai | Yugen.ai Technology Blog | Jan, 2025 | Medium\\nOpen in app\\nSign up\\nSign in\\nWrite\\nSign up\\nSign in\\nFeatured\\nDeepSeek-V3 — Advances in MoE Load Balancing and Multi-Token Prediction Training\\nYugen.ai\\n·\\nFollow\\nPublished in\\nYugen.ai Technology Blog\\n·\\n8 min read\\n·\\nJan 4, 2025\\n--\\nListen\\nShare\\nDeepSeek-V3 benchmarks (\\nsource\\n)\\nBy\\nSoumanta Das\\nTo read Part II of our DeepSeek series, which focuses on Reinforcement Learning & Policy update algorithms such as PPO (Clip, Penalty) and GRPO, see\\nUnderstanding the Math Behind GRPO — DeepSeek-R1-Zero\\nThe Policy Optimisation algo in DeepSeekMath & DeepSeek’s first gen reasoning models.\\nmedium.com\\nIntroduction\\n2024 has been a great year for Open Source LLMs. Towards the end of 2024, DeepSeek released DeepSeek-V3, a 671B parameter MOE (mixture-of-experts) language model. One of the reasons why this release came into the limelight was because of cost-effective training cost of ~$6M, which is easily an order of magnitude lower than known estimates for closed source models.\\nTheir technical report is a great read for those interested in understanding how architectural design decisions impact model performance. While the DeepSeek-V3’s impressive training cost has been most spoken of, I decided to share some aspects that have been lesser talked about, but are equally impressive. Hope you enjoy the read.\\nBeyond their basic architecture of Multi-Head Latent Attention for inference and DeepSeekMOE for cost effective training, they implement the 2 following strategies to improve the model’s capabilities -\\nAn auxiliary-loss-free strategy for load balancing\\nA multi-token prediction training objective\\nLet’s take a deeper look at what these mean and try to develop an intuition as to why these have been helpful.\\nWhat is MOE?\\nThe MOE (Mixture-of-Experts) architecture uses multiple expert networks for prediction. This involves replacing the Feed-Forward Network (FFN) in a standard Transformer architecture with an MoE layer. The underlying idea here is to make each expert get trained on a different & specialised distributions of tokens. This is achieved by using a gating network that determines the distribution of each expert’s relevance given an input. The output of 1 MoE layer is the sum of outputs from each layer weighted by the output of the gating network.\\nThe MoE architectures are computationally less expensive than non-MoE/dense architectures. This is because only the relevant experts for the given task are activated while training as well as running inference.\\nWhile MoEs have gained popularity in language model architectures in the last few years, its foundations can be traced back to the Learning Factored Representations in a Deep Mixture of Experts\\npaper\\n. The diagram below illustrates a simple mixture-of-experts network, where g(x) acts as the gating network, fi(x) are the experts and z is the output of the system. It’s important to note that recent LLMs such as Mixtral, DBRX and DeepSeek have significant improvements on top of this.\\nFig 1 — A simple MOE system. Source:\\nLearning Factored Representations in a Deep Mixture of Experts\\nBefore we move forward, it’s important to remember that DeepSeekMOE architecture (see original paper\\nhere\\n) had introduced a few architectural improvements over traditional MOE architectures.\\nFine grained expert segmentation\\n— Compared to a typical MoE architecture, each expert FFN is segmented into m experts and the hidden dimension size is reduced to 1/m of their original size. This results in more experts, each of which are discouraged from covering diverse knowledge types. Additionally, the compute cost remains unchanged because the increase in no. of experts is offset by the reduced size.\\nShared expert isolation\\n— Certain knowledge types can be common or shared and therefore do not necessitate separate experts. In addition to the fine grained expert segmentation strategy,\\nKs\\nexperts serve as shared experts. Again, to maintain computation costs, the no. of activated non-shared experts are decreased by\\nKs\\n, while each token is deterministically routed to each shared expert.\\nChallenges with MoE\\nAn (arguably oversimplified) analogy for MoE is to imagine experts as translators and inputs as incoming documents that need to be translated, where the no. of documents are more than the no. of available translators. The router’s job is to ensure that each document is introduced to the most relevant translators (based on a translators’ expertise to review aspects such as technical review, grammar, cultural context, etc).\\nMoEs are prone to routing collapses i.e. a model can continuously select the same set of experts during training, thereby hindering the learning process of the others. Also, when experts are distributed across different training nodes, load imbalance can lead to computational bottlenecks.\\nA commonly accepted solution to tackle this problem is an auxiliary loss. However, this loss, while improving load-balancing, interferes with training — the loss is a regularisation term after all. A small auxiliary loss co-efficient causes insufficient load balancing, whereas a larger value impairs the model performance. This tradeoff is illustrated in the diagram below.\\nDilemma between load balancing and model performance in auxiliary-loss-controlled training\\nLoss-free Load balancing\\nDeepSeek V3 introduces a novel way to make the routing more efficient. Simply put, it separates “\\nWhich expert gets selected”\\nand “\\nHow much of their expertise counts”,\\nthereby making these independent.\\nLet’s take an example to understand this better. Assume that the\\nInput is “\\nQuantum computing in Financial Fraud Detection\\n” and\\nWe have 3 translators/experts —\\nTechnical, Grammar, Cultural\\nWe’re selecting the Top-k (k=2) experts\\nEach input has a natural affinity towards the experts. In this case quantum computing & fraud detection is a technical concept, so the natural affinity of the Technical expert is the highest. Let’s assume some baseline affinity scores as a starting point -\\nBaseline affinity across experts (example)\\nNow, let’s say, the Technical expert is being overworked. A bias term is added to the affinity scores in order to balance the utilisation.\\nUsing Bias to Load Balance across experts\\nThe new selection score diminishes the gap between Grammar and Technical — meaning for new inputs, the distribution of workload is more equal than before. Even if we get inputs that are extremely technical in nature, the Grammar expert is utilised more than before and the Cultural expert isn’t used (since we’re selecting the top 2 experts).\\nThe bias impacts only the expert selection. The gating value is still derived using the original affinity scores (Technical = 0.7, Grammar = 0.4, Cultural=0.1). This ensures that the expertise of each expert is respected.\\nThis process in mathematically represented in the DeepSeek V3 paper as follows\\nFig 2 — Auxiliary-Loss-Free Load Balancing\\nwhere,\\nbi\\n= bias for each expert\\nsi,t\\n= natural affinity scores\\nMulti-token prediction\\nStandard Language models, based on the transformer architecture, learn from a corpus of tokens and attempt to predict the next token. The objective function is formulated as the negative log likelihood of the next token, given prior input tokens. Multi-token prediction, as the name suggests, extends the former objective function to include the next n tokens instead of just the next one.\\nFig 3 — Cross Entropy loss for next token prediction\\nFig 4 — Cross Entropy loss for multi-token (next n tokens) prediction\\nInspiration from Gloeckle et al. (2024)\\nFig 5 — Overview of Multi-token prediction. Gloeckle et al. (2024)\\nThe DeepSeek-V3 paper builds on the MTP work proposed in the\\nBetter & Faster Large Language Models via Multi-token Prediction\\npaper. The MTP architecture is as follows -\\nInput tokens\\nx1\\n,\\nx2\\n, …\\nxt\\ni.e.\\nxt:1\\nare fed to a shared transformer trunk to produce a hidden representation\\nzt:1\\n. The hidden representation can be thought of as something that contains a shared understanding of input tokens.\\nn independent output heads, each head responsible for predicting a position-specific token. The first head predicts\\nxt+1\\n, the second\\nxt+2\\nand so on.\\nA unembedding matrix (\\nfu\\n) that is shared by all prediction heads\\nFig 6 — Prediction ’n’ future tokens\\nMTP implementation in DeepSeek-V3\\nInstead of parallel heads for multi-token prediction, DeepSeek-V3 uses a sequential prediction approach. MTP helps the model learn by giving it a “peek ahead” at the next tokens in the sequence during training. This encourages the model to create internal representations (its understanding of the text) that consider a wider context than just the preceding words. The model is only given access to future tokens within the same training sequence, so this should not be misinterpreted as leakage during training.\\nD\\nsequential MTP modules are used to predict\\nD\\nadditional tokens. Each MTP module consists of the following\\nA shared embedding layer\\nA shared output head\\nA transformer block and,\\nA projection matrix\\nLet’s take a quick look at the MTP illustration in the paper and then get into the finer details of the implementation.\\nFig 7 — MTP Implementation in DeepSeek-V3\\nThe 2 MTP Modules are sequentially arranged as shown above. The core idea here is combine the model’s representation of the current token at the previous depth (\\nk-1\\n) and the embedding of the future token.\\nA Root Mean Square Layer Normalisation is applied to both these representations to normalise them before they are concatenated — this ensures training stability.\\nThis combination is then multiplied by a projection matrix\\nMk\\n— this matrix is specific to the prediction depth\\nk\\nThe output from the Linear projection is then fed to a transformer block (similar to the projection matrix, this block is also specific to the depth\\nk\\n)\\nThe output from the Transformer block is passed on to an Output head, which calculates the probability distribution for the\\nk\\nth additional token. This output head is shared with the main model and isn’t specific to the MTP module.\\nDeepSeek-V3's primary goal with MTP is to improve training. During inference, the additional MTP modules can be discarded, and the main model can operate independently. The MTP modules can be used for speculative decoding, which is similar to what has been proposed in the\\nEAGLE paper\\n.\\nDeepSeek’s recent release is a huge win for the open source community and is a significant step in bridging the gap between open and closed source models.\\nReferences\\nDeepSeek-V3\\nGithub Repo\\nDeepSeek-V3\\ntechnical report\\nAuxiliary-Loss-Free Load Balancing —\\nArXiv paper\\nUnderstanding the Math Behind GRPO — DeepSeek-R1-Zero\\nThe Policy Optimisation algo in DeepSeekMath & DeepSeek’s first gen reasoning models.\\nmedium.com\\nHope you enjoyed the read. Our team keeps publishing such deep drives in the\\nYugen Tech Blog\\n— do consider giving our publication a follow if you’re interested.\\nAlternatively, I’m happy to connect on\\nLinkedIn\\nor\\nX\\nand talk.\\nDeepseek\\nLlm Architecture\\nMixture Of Experts\\nPublished in\\nYugen.ai Technology Blog\\n27 Followers\\n·\\nLast published\\n3 days ago\\nStories on Engineering, Machine Learning & AI from the team at\\nYugen.ai\\nFollow\\nWritten by\\nYugen.ai\\n73 Followers\\n·\\n17 Following\\nAn early-stage, full stack MLOps and AI Agents startup.\\nFollow\\nNo responses yet\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams\", 'image_urls': [], 'title': 'DeepSeek-V3 — Advances in MoE Load Balancing and Multi-Token Prediction Training | by Yugen.ai | Yugen.ai Technology Blog | Jan, 2025 | Medium'}, {'url': 'https://lzwjava.github.io/deepseek-v3-en', 'raw_content': 'DeepSeek V3\\nDeepSeek V3 | Zhiwei Li\\nDeepSeek V3\\nHome\\nPDF\\nEnglish\\n中文\\n日本語\\nEspañol\\nहिंदी\\nFrançais\\nDeutsch\\nالعربية\\n繁體中文\\nYour browser does not support the audio element.\\nOverview and Key Highlights\\nModel Name: DeepSeek-V3, a Mixture-of-Experts (MoE) language model with 671 billion parameters, of which 37 billion are activated per token.\\nTraining Dataset: Pre-trained on 14.8 trillion diverse, high-quality tokens.\\nCore Innovations: Incorporates Multi-Head Latent Attention (MLA) and DeepSeekMoE architectures with auxiliary-loss-free load balancing for efficiency.\\nTraining Efficiency: Achieves full training with only 2.788 million H800 GPU hours.\\nCost Efficiency: Training cost is estimated at 5.576M USD, assuming 2 USD per GPU hour.\\nArchitectural Innovations\\nTransformer-Based Framework: Retains the Transformer architecture for scalability and flexibility.\\nMulti-Head Latent Attention (MLA): Reduces inference memory by compressing key-value caches without performance loss.\\nDeepSeekMoE: Utilizes a combination of shared and routed experts for cost-effective training and high computational efficiency.\\nAuxiliary-Loss-Free Load Balancing: Introduces bias terms to maintain balanced expert loads without compromising performance.\\nMulti-Token Prediction (MTP): Sequentially predicts multiple tokens per position, improving data efficiency and representation pre-planning.\\nTraining Framework\\nFP8 Mixed Precision Training: Leverages fine-grained quantization and low-precision storage to optimize memory and computation.\\nDualPipe Algorithm: Overlaps computation and communication phases, reducing pipeline bubbles and improving parallelism.\\nEfficient Cross-Node Communication: Employs optimized kernels for all-to-all operations, utilizing NVLink and InfiniBand bandwidths.\\nLow-Precision Optimizer States: Stores optimizer states in BF16, reducing memory consumption without performance loss.\\nMemory Optimization Techniques: Recomputes certain operations (e.g., RMSNorm) during back-propagation to save memory.\\nPre-Training Details\\nStable Training Process: No irrecoverable loss spikes or rollbacks occurred during pre-training.\\nContext Length Extension: Extended context length to 32K and subsequently to 128K in two stages.\\nTraining Costs: Pre-training required 2.664M GPU hours, context extension 119K GPU hours, and post-training 5K GPU hours.\\nToken Efficiency: Training efficiency ensured by minimizing GPU hours per trillion tokens.\\nHigh-Quality Data: Pre-training dataset curated for diversity and relevance.\\nPost-Training Enhancements\\nSupervised Fine-Tuning (SFT): Aligns model outputs with human preferences.\\nReinforcement Learning (RL): Employs Group Relative Policy Optimization for fine-tuning.\\nKnowledge Distillation: Integrates reasoning capabilities from DeepSeek-R1 models.\\nOutput Style Control: Balances accuracy with generation length and style.\\nPerformance Refinement: Post-training further improves benchmark results.\\nBenchmark Performance\\nMMLU (Educational Benchmarks): Achieves 88.5, surpassing other open-source models.\\nGPQA (General Knowledge): Scores 59.1, comparable to GPT-4o and Claude-3.5-Sonnet.\\nMath Benchmarks: State-of-the-art performance in mathematical reasoning tasks.\\nCode Competitions: Excels in coding benchmarks such as LiveCodeBench.\\nFactual Knowledge: Demonstrates superior results in English and Chinese factuality benchmarks.\\nInference and Deployment\\nPrefilling Stage: Combines tensor parallelism (TP4), sequence parallelism (SP), and expert parallelism (EP32) for efficiency.\\nDecoding Stage: Utilizes EP320 with IBGDA for low-latency communication.\\nDynamic Redundancy: Adjusts expert loads dynamically to optimize resource utilization.\\nSeparation of Stages: Prefilling and decoding stages are separated to enhance throughput.\\nHardware Utilization: Optimized for H800 GPUs with NVLink and InfiniBand interconnects.\\nInnovations in Load Balancing and Decoding\\nBias-Based Routing: Introduces bias terms to ensure balanced expert loads dynamically.\\nSpeculative Decoding: Enhances generation latency using MTP modules.\\nRedundant Experts: Duplicates high-load experts to balance GPU workloads.\\nNode-Limited Routing: Restricts token routing to a maximum of 4 nodes to reduce communication overhead.\\nNo Token Dropping: Ensures all tokens are retained during training and inference.\\nTechnical Details\\nCluster Configuration: Trained on a cluster with 2048 NVIDIA H800 GPUs.\\nPipeline Parallelism: Employs a 16-way parallelism scheme for scalability.\\nMemory Footprint: Avoids costly tensor parallelism by optimizing memory usage.\\nCustom Kernels: Develops specialized communication kernels to handle cross-node operations efficiently.\\nMixed Precision Optimization: Combines FP8 and BF16 formats for optimal training dynamics.\\nEvaluation and Results\\nComprehensive Benchmarks: Evaluated across diverse domains including education, coding, and reasoning.\\nOpen-Source Leadership: Emerges as the strongest open-source base model in its category.\\nComparison with Closed-Source Models: Performance comparable to GPT-4o and Claude-3.5-Sonnet.\\nStrength in Chinese Knowledge: Outperforms leading models in Chinese factuality benchmarks.\\nLong-Context Handling: Excels in tasks requiring extended context processing.\\nFuture Directions\\nDynamic Redundancy Exploration: Investigating more adaptive redundancy strategies.\\nSpeculative Decoding Expansion: Exploring further uses of MTP for inference acceleration.\\nHardware Co-Design: Adapting to next-generation GPUs for enhanced performance.\\nBroader Benchmark Coverage: Expanding evaluations to more diverse tasks.\\nSustainability: Reducing training costs further through algorithmic and hardware optimizations.\\nThis document provides a comprehensive summary of DeepSeek-V3, encapsulating its architecture, training methodologies, benchmark performance, and future prospects. Let me know if you need further elaboration on specific sections or additional points!\\nBack\\n2025.03.01\\nDonate', 'image_urls': [], 'title': 'DeepSeek V3'}, {'url': 'https://www.deeplearning.ai/the-batch/deepseek-v3-redefines-llm-performance-and-cost-efficiency/', 'raw_content': 'DeepSeek-V3 Redefines LLM Performance and Cost Efficiency\\n✨ New course! Enroll in\\nBuild Apps with Windsurf’s AI Coding Agent\\nShare\\nA new model from Hangzhou upstart DeepSeek delivers outstanding performance and may change the equation for training costs.\\nWhat’s new:\\nDeepSeek-V3\\nis an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights are\\nopen\\nexcept for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download them\\nhere\\n.\\nMixture of experts (MoE) basics:\\nThe MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input.\\nHow it works:\\nDeepSeek-V3 is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The team trained the model in 2.79 million GPU hours — less than 1/10 the\\ntime required to train Llama 3.1 405B\\n, which DeepSeek-V3 substantially outperforms — at an extraordinarily low cost of $5.6 million.\\nThe developers trained it on roughly 15 trillion tokens, including a larger percentage of coding and math data relative to DeepSeek-V2. They fine-tuned it on a wide variety of tasks using output generated by\\nDeepSeek-R1\\nand DeepSeek-V2.5. They further sharpened its performance across diverse domains using the reinforcement learning algorithm known as\\ngroup relative policy optimization\\n.\\nEarlier\\nwork\\nshowed that training to predict the next two tokens would improve performance over learning to predict just one. The authors implemented this procedure. The model learned to predict the first token as usual and used an additional set of layers to learn to predict the second token. The additional layers aren’t used at inference.\\nFollowing\\nDeepSeek-V2\\n, DeepSeek-V3 uses multi-head latent attention, which saves memory during execution relative to other variants of attention.\\nAlso like DeepSeek-V2, the new model combines dedicated (routed) and shared experts. The model chooses eight of 256 experts for a particular input, but it also uses a shared expert that processes all inputs.\\nResults:\\nIn DeepSeek’s tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o.\\nDeepSeek-V3 showed exceptional performance in coding and math tasks. In coding, DeepSeek-V3 dominated in five of the seven benchmarks tested. However, DeepSeek-V3 lost to o1 on one of the five, according to a public leaderboard. Specifically, on\\nPolyglot\\n, which tests a model’s ability to generate code in response to difficult requests in multiple programming languages, DeepSeek-V3 (48.5 percent accuracy) beat Claude Sonnet 3.5 (45.3 percent accuracy), though it lost to o1 (61.7 percent accuracy).\\nIn language tasks, it performed neck-and-neck with Claude 3.5 Sonnet, achieving higher scores in some tasks and lower in others.\\nBehind the news:\\nOpenAI’s o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows.\\nWhy it matters:\\nOpen models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3’s shockingly low training cost.\\nThe team doesn’t explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits “meticulous engineering optimizations.”) But it’s likely that DeepSeek’s steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn’t employ MoE. In 2022,\\nMicrosoft\\nfound that MoE cost five times less in training for equal performance compared to a dense model, and\\nGoogle\\nand\\nMeta\\nreported that MoE achieved better performance than dense models trained on the same numbers of tokens.\\nWe’re thinking:\\nIf they can be replicated, DeepSeek’s results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically.\\nSubscribe to The Batch\\nStay updated with weekly AI News and Insights delivered to your inbox', 'image_urls': [], 'title': 'DeepSeek-V3 Redefines LLM Performance and Cost Efficiency'}, {'url': 'https://martinfowler.com/articles/deepseek-papers.html', 'raw_content': \"The DeepSeek Series: A Technical Overview\\nThe DeepSeek Series: A Technical Overview\\nThe appearance of DeepSeek Large-Language Models has caused a lot of discussion and angst since their latest versions appeared at the beginning of 2025. But much of the value of DeepSeek's work comes from the papers they have published over the last year. This article provides an overview of these papers, highlighting three main arcs in this research: a focus on improving cost and memory efficiency, the use of HPC Co-Design to train large models on limited hardware, and the development of emergent reasoning from large-scale reinforcement learning\\n06 February 2025\\nShayan Mohanty\\nShayan Mohanty is the Head of AI Research at Thoughtworks, where his group focuses on foundational research to bridge the gap between AI development and production. Previously, he was CEO and Co-Founder of Watchful, a startup that built software to automate the process of data labeling for AI. Shayan has a decade leading data engineering teams at various companies including Facebook, where he led the stream processing team responsible for processing 100% of the ads metrics data for all FB products. He is also a Guest Scientist at Los Alamos National Laboratory and has given talks on topics ranging from Automata Theory to Machine Teaching.\\nContents\\nDeepSeek-LLM: Laying the Foundation\\nMotivation & Overview\\nTraining Instability\\nData Quality & Model Scale\\nKey Takeaways\\nDeepSeek-V2: Multi-Head Latent Attention & MoE\\nExpanding the Model While Reducing Memory\\nMulti-Head Latent Attention (MLA)\\nDeepSeekMoE: Sparsely Activated FFNs\\nTraining & Outcomes\\nDeepSeek-V3: HPC Co-Design\\nScaling MoE to 671B While Preserving Efficiency\\nRefined MLA\\nRefined DeepSeekMoE: Auxiliary-Loss-Free, Higher Capacity\\nCo-Designed Frameworks: FP8, DualPipe, and PTX Optimizations\\nOutcomes\\nDeepSeek-R1: Reinforcement Learning for Deeper Reasoning\\nEmergent Reasoning Behaviors Through RL-Only\\nRefined Reasoning Through SFT + RL\\nConnecting the Arcs: Efficiency & Emergence\\nThis article provides a cohesive overview of four technical reports from DeepSeek:\\nDeepSeek-LLM\\n(Jan '24)\\n: an early investigation of scaling laws and data-model tradeoffs.\\nDeepSeek-V2\\n(Jun '24)\\n: introducing Multi-Head Latent Attention (MLA) and DeepSeekMoE to improve memory and training efficiency.\\nDeepSeek-V3\\n(Dec '24)\\n: scaling sparse MoE networks to 671B parameters, with FP8 mixed precision training and intricate HPC co-design\\nDeepSeek-R1\\n(Jan '25)\\n: building upon the efficiency foundations of the previous papers and using\\nlarge-scale reinforcement learning\\nto incentivize emergent chain-of-thought capabilities, including a â\\x80\\x9czero-SFTâ\\x80\\x9d variant.\\nFor additional context on DeepSeek itself and the market backdrop that has caused claims made by the DeepSeek team to be taken out of context and spread widely, please take a look at my colleague Prasanna Pendse's post:\\nDemystifying Deepseek\\n. For the purposes of this article, we'll be focusing analysis and commentary on the technical work itself, its merits, and what it may signal for the future.\\nMuch of this article assumes significant knowledge of the terminology and concepts of building LLMs, more so than is typical for articles on this site. In future weeks we hope to expand this article to provide explanations of these concepts to make this article easier to follow for those not familiar with this world. We shall post any such updates on this site's\\nusual channels\\n.\\nAll four papers revolve around a\\nsingular challenge\\n: building ever-larger language models with minimal cost, memory overhead, and training instability. In each iteration, the authors refine both\\narchitecture\\nand\\ninfrastructure\\n- a strategy often referred to as\\nHPC co-design\\n.\\nKey arcs in this series include:\\nCost and Memory Efficiency:\\nMethods like Multi-Head Latent Attention (MLA) compression, mixture-of-experts (MoE), and FP8-based optimizations all aim to make massive-scale training and inference feasible.\\nSparsity + HPC Co-Design:\\nFrom V2 to V3, we see mixture-of-experts architecture evolve alongside specialized HPC scheduling—allowing 671B-parameter models to be trained on H800 clusters without blowing up the budget.\\nEmergent Reasoning:\\nIn R1, large-scale Reinforcement Learning (RL) unlocks advanced chain-of-thought capabilities, culminating in “R1-Zero” and its purely RL-driven approach to reasoning tasks.\\nDeepSeek-LLM: Laying the Foundation\\nMotivation & Overview\\nThe authors set out to answer an important question: Given a fixed compute budget for pre-training, how do we choose the scale of the model and how much training data to use? Prior studies (e.g. Chinchilla vs. GPT-3) differed on the ratio between these two factors. DeepSeek-LLM addresses that by measuring scale in a different way. Earlier work measured scale in terms of how many parameters were in the model, DeepSeek-LLM instead measured scale as\\nnon-embedding FLOPs/token\\n1\\nThey then found they could predict computation with:\\n1:\\nNon-embedding FLOPs are the amount of FLOPs (Floating Point Operations per Second) used for pre-training certain layers of the transformer (non-embedding). The authors found only some layers contributed to the scaling formula.\\n$$ C = M \\\\times D $$\\nwhere $C$ is the compute budget, $M$ is non-embedding FLOPs/token, and $D$ is data size.\\nThis more granular representation helps them predict how a 7B or 67B model might train on 2T tokens of bilingual data.\\nTraining Instability\\nA central concern they grapple with is training instability (sudden irrecoverable divergences in the training process), which can often manifest in large-scale language models—especially those with mixture-of-experts or very long contexts.\\nBy carefully tuning learning rates, batch sizes, and other hyperparameters\\n2\\n, DeepSeek-LLM demonstrates that stable large-scale training is achievable, but it requires meticulous design of the architecture of the transformer model together with the infrastructure of the High Performance Computing (HPC) data center used to train it. This interwoven design of both architecture and infrastructure is called\\nHPC Co-Design\\n.\\n2:\\nA model consists of billions on internal variables, which are called its\\nparameters\\n. These parameters gain their values (weights) during training. Before training, developers will set a number of different variables that control the training process itself, these are called\\nhyperparameters\\nData Quality & Model Scale\\nA point the authors make is about how data quality shifts the optimal ratio—i.e.,\\nhigher-quality data can justify a bigger model for the same number of tokens\\n. You can intuit this by imagining two scenarios:\\nScenario A: You have a 100-billion-token corpus full of duplicates, spammy text, or incomplete sentences. The model might not glean much new knowledge because the data is partly redundant or low-value.\\nScenario B: You have a carefully curated 100-billion-token corpus with broad coverage of code, math, multi-lingual dialogues, factual text, etc. Each token is more “information-rich,” so the model can “afford” to use more parameters without hitting diminishing returns prematurely.\\nIn other words, when data is denser in useful information, scaling the model further pays off because each parameter can learn from richer signals.\\nKey Takeaways\\nHyperparameter Scaling: They propose simple power-law fits to pick batch size and learning rate as compute $C$ grows.\\nBilingual Data: They train two base sizes (7B, 67B) on 2T tokens covering English/Chinese, then do Supervised Fine Tuning (SFT) and a simpler preference-based alignment called\\nDirect Preference Optimization (DPO)\\n.\\nResults: The resulting DeepSeek-LLM67B â\\x80\\x9cOutperforms LLaMA-2 70Bâ\\x80\\x9d on math/coding tasks, illustrating how HPC co-designed approaches can keep training stable while efficiently pushing scale.\\nThe seeds planted here - scaling laws and infrastructure for extremely large training - will reappear in subsequent works.\\nDeepSeek-V2: Multi-Head Latent Attention & MoE\\nExpanding the Model While Reducing Memory\\nWhere DeepSeek-LLM mostly explored high-level scale tradeoffs, DeepSeek-V2 dives into specifics of Transformer architecture overhead. Two big obstacles in large LLMs are:\\nAttention KV Cache: Storing Key/Value vectors for thousands of tokens is memory-intensive.\\nFeed-Forward Computation: Typically the largest consumption of FLOPs in a Transformer.\\nTo tame both, they propose:\\nMulti-Head Latent Attention (MLA): compresses Key/Value vectors to reduce memory.\\nDeepSeekMoE: a sparse Mixture-of-Experts approach that activates a fraction of the feed-forward capacity per token.\\nMulti-Head Latent Attention (MLA)\\nIn standard attention, each token's Q/K/V can be as large as $d_{model}$ times the number of heads. MLA folds them into smaller â\\x80\\x9clatentâ\\x80\\x9d vectors:\\n$$ \\\\quad \\\\mathbf{c}_{t}^{KV} = W^{DKV}\\\\mathbf{h}_t, \\\\quad \\\\mathbf{k}_{t}^{C} = W^{UK}\\\\mathbf{c}_t^{KV}, \\\\quad \\\\mathbf{v}_{t}^{C} = W^{UV}\\\\mathbf{c}_t^{KV}, \\\\quad $$\\nWhere $c_{t}^{KV}$ is the compressed latent vector for keys and values. $W^{DKV}$ is the down-projection matrix, and $W^{UK}, W^{UV}$ are the up-projection matrices for keys and values, respectively. In simpler terms:\\nReplaces the standard QKV computation by using low rank factorization to turn one matrix of dim (\\nin, out\\n) into two matrices of (\\nin, rank\\n) and (\\nrank, out\\n)\\nProject the compressed KV latent vector for each head to get the full K and V head corresponding to each Q head\\nCache the compressed KV latent vector instead of each of the KV heads in full, and compute the KV heads on the fly from the latent vector.\\nDeepSeekMoE: Sparsely Activated FFNs\\nNext, they adopt a Mixture-of-Experts (MoE) in the feed-forward blocks:\\nShared Experts\\nhandle universal patterns for every token.\\nRouted Experts\\nhandle specialized sub-problems, chosen dynamically via gating.\\nAuxiliary Loss\\nensures balanced usage so no expert collapses (i.e. is never used).\\nThey further limit cross-device routing with a â\\x80\\x9cdevice-limited routingâ\\x80\\x9d scheme - instead of allowing any token to access any expert, DeepSeekMoE selects a limited number of devices ($M$) per token, and performs expert selection only within these devices. The basic process is as follows:\\nIdentify top $M$ devices that contain experts with the highest affinity to the token\\nPerform top $K_r$ expert selection within these $M$ devices\\nAssign the selected experts to process the token\\nWithout device-limited routing, MoE models can generate excessive communication overhead which is incompatible with the hardware limitations imposed on the DeepSeek team. In addition, MoE models typically risk uneven expert utilization, where some experts are overused while others remain inactive. To prevent this, DeepSeekMoE introduces three balancing loss functions:\\nExpert-level Balance Loss ($L_{ExpBal}$):\\nEnsures uniform distribution of tokens across experts to prevent expert collapse\\nUses a loss function based on softmax scores of token-expert affinity\\nDevice-level Balance Loss ($L_{DevBal}$):\\nEnsures workload is evenly distributed across devices\\nCommunication Balance Loss ($L_{CommBal}$):\\nBalances incoming and outgoing token routing to each device\\nTraining & Outcomes\\nDeepSeek-V2, with ~236B total params (21B activated), is pre-trained on 8.1T tokens. They do Supervised Fine Tuning (SFT) on 1.5M instruction samples, then reinforcement learning (RL) for alignment. The end result:\\nInference and training are both faster and cheaper (MLA + sparse experts)\\nThey remain stable at scale\\nThis paper is really when iteration gains due to HPC Co-Design start to become apparent. By designing the model architecture with the training infrastructure in mind, and implementing a training regime that considers the realities of the hardware (e.g. low interconnect speeds on H800s), the team was able to lay the foundation for their most notable breakthrough.\\nDeepSeek-V3: HPC Co-Design\\nScaling MoE to 671B While Preserving Efficiency\\nBuilding on V2, DeepSeek-V3 further extends sparse models to\\n671B\\nparameters (\\n37B activated\\n), training on 14.8T tokens in under\\n2.8M H800 GPU hours\\n. The authors credit extensive HPC co-design:\\nLastly, we emphasize again the economical training costs of DeepSeek-V3, summarized in Table 1, achieved through our optimized co-design of algorithms, frameworks, and hardware.\\n--\\nDeepSeek-V3 Tech. Report, p.5\\nThe major novelties are:\\nRefined MLA\\nRefined DeepSeekMoE\\nCo-Designed Training & Inference Frameworks\\nRefined MLA\\nMulti-Head Latent Attention was introduced in V2 to reduce KV cache overhead. In V3, it is further refined with several new features:\\nDynamic Low-Rank Projection\\n: Instead of a static compression dimension, MLA adjusts how strongly it compresses Key/Value vectors depending on sequence length. For shorter sequences, less compression preserves fidelity; for extremely long sequences (32K–128K tokens), deeper compression manages memory growth.\\nAdaptive Query Compression\\n: Where V2 used a fixed $d_c$ dimension, V3 employs an adaptive scaling of the query up/down at different layer depths. Early layers use higher-dimensional queries for expressiveness; deeper layers more aggressively compress to save activation memory.\\nImproved RoPE Handling\\n: V2 only partially decoupled keys, but V3 extends the concept for more stable 128K context. They track a “decoupled shared key” that reduces numerical drift in extremely long generations.\\nJoint KV Storage\\n: V2 stored compressed keys and values separately. V3 merges them into a shared compressed representation to further reduce memory traffic during multi-node inference.\\nLayer-Wise Adaptive Cache\\n: Instead of caching\\nall\\npast tokens for all layers, V3 prunes older KV entries at deeper layers. This helps keep memory usage in check when dealing with 128K context windows.\\nTogether, these MLA refinements ensure that while DeepSeek-V3 can attend across\\nvery\\nlong sequences, the memory overhead remains manageable.\\nRefined DeepSeekMoE: Auxiliary-Loss-Free, Higher Capacity\\nOn the MoE side, DeepSeek-V3 drops the auxiliary-loss approach from V2. Instead of an explicit penalty term, each expert acquires a dynamic bias $b_i$. If an expert is overloaded at a step, $b_i$ decreases; if underloaded, $b_i$ increases. The gating decision then adds $b_i$ to the token's affinity:\\n$$ s'_{i,t} = s_{i,t} + b_i $$\\nKey Improvements:\\nNo Token Dropping\\n: V2 occasionally dropped tokens if certain experts got overloaded, but the new bias-based method keeps everything.\\nMore Activated Experts\\n: They raise the number of routed experts from 6 to 8 per token, improving representational power.\\nHigher Stability\\n: By removing auxiliary losses, they avoid potential interference with the main training objective, focusing purely on the\\nintrinsic\\no gating signals plus bias adjustments.\\nHence, the final feed-forward module is a combination of a small set of shared experts plus up to 8 specialized experts chosen adaptively.\\nCo-Designed Frameworks: FP8, DualPipe, and PTX Optimizations\\nScaling an MoE model to 671B demanded HPC-level solutions for training and inference. The authors emphasize:\\nThrough the co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, achieving near-full computation- communication overlap.\\n--\\nDeepSeek-V3 Tech. Report, p.5\\nFP8 Mixed Precision\\nThey adopt an FP8 data format for General Matrix Multiplications (GEMMs), halving memory. The risk is\\nreduced numeric range\\nso they offset it with:\\nBlock-wise scaling (e.g., 1x128 or 128x128 tiles).\\nPeriodic â\\x80\\x9cpromotionâ\\x80\\x9d to FP32 after short accumulation intervals to avoid overflow/underflow.\\nDualPipe Parallelism\\nThey propose\\nDualPipe\\nto overlap forward/backward computation with the MoE all-to-all dispatch. It rearranges pipeline stages to ensure that network communication (particularly across InfiniBand) is hidden behind local matrix multiplications.\\nPTX-Level & Warp Specialization\\nTo fully exploit InifiniBand(IB) and NVLink:\\nThey tune\\nwarp-level\\ninstructions in PTX (a level lower than CUDA), auto-tuning the chunk size for all-to-all dispatch.\\nDynamically partition Streaming Microcontrollers into communication vs. compute tasks so that token dispatch never stalls local GEMM.\\nAs a result, training costs were cut to 2.8M H800 GPU hours per run - low for a 14.8T token corpus.\\nOutcomes\\nThe resulting DeepSeek-V3 excels at code, math, and some multilingual tasks, outperforming other open-source LLMs of similar scale. Deep HPC co-design (FP8, DualPipe, PTX-level optimization) plus refined MLA/MoE implementation achieve\\nextreme\\nscale with stable training.\\nDeepSeek-R1: Reinforcement Learning for Deeper Reasoning\\nIt's worth noting that both DeepSeek R1 and DeepSeek R1-Zero are architecturally identical to DeepSeek V3 (but uses the â\\x80\\x9conly-pretrainedâ\\x80\\x9d base version). The only difference in these models is how post-training is handled.\\nEmergent Reasoning Behaviors Through RL-Only\\nAll prior DeepSeek releases used SFT (plus occasional RL). By contrast, DeepSeek-R1-Zero tries an extreme:\\nno supervised warmup\\n, just RL from the base model. They adopt Group Relative Policy Optimization (GRPO), which:\\nSamples a group of old-policy outputs ${o_1, ..., o_G}$\\nScores each with a reward (in this case, rule-based)\\nNormalizes the advantage $A_i$ by group mean/stdev\\nOptimizes a clipped PPO-like objective\\nThe reward function for the R1 models is rule-based - a simple weighted sum between 2 components\\nAccuracy Reward\\n- if the task has an objective correct answer (e.g. a math problem, coding task, etc.), correctness is verified using mathematical equation solvers for step-by-step proof checking, and code execution & test cases for code correctness verification\\nFormat Reward\\n- the model is rewarded for following a structured reasoning process using explicit reasoning markers\\n<think></think>\\nand\\n<answer></answer>\\nThe relative advantage $A_i$ for a given output is calculated as:\\n$$ A_i = \\\\frac{r_i - mean(\\\\{r_1, r_2, ..., r_G\\\\})}{std(\\\\{r_1, r_2, ..., r_G\\\\})} $$\\nwhere $r_i$ is the reward calculated for the given output. The model's policy is updated to favor responses with higher rewards while constraining changes using a clipping function which ensures that the new policy remains close to the old.\\nIn so many words: the authors created a testing/verification harness around the model which they exercised using reinforcement learning, and gently guided the model using simple Accuracy and Format rewards. In doing so, emergent reasoning behaviors were observed:\\nSelf-verification\\nwhere the model double-checks its own answers\\nExtended chain-of-thought\\nwhere the model learns to explain its reasoning more thoroughly\\nExploratory reasoning\\n- the model tries different approaches before converging on an answer\\nReflection\\n- the model starts questioning its own solutions and adjusting reasoning paths dynamically\\nR1-Zero is probably the most interesting outcome of the R1 paper for researchers because it learned complex chain-of-thought patterns from raw reward signals alone. However, the model exhibited notable issues:\\nReadability Problems\\n: Because it never saw any human-curated language style, its outputs were sometimes jumbled or mix multiple languages.\\nInstability in Non-Reasoning Tasks\\n: Lacking SFT data for general conversation, R1-Zero would produce valid solutions for math or code but be awkward on simpler Q&A or safety prompts.\\nLimited Domain\\n: Rule-based rewards worked well for verifiable tasks (math/coding), but handling creative/writing tasks demanded broader coverage.\\nHence, the authors concluded that while “pure RL” yields strong reasoning\\nin verifiable tasks\\n, the model’s overall user-friendliness was lacking. This led them to\\nDeepSeek-R1\\n: an alignment pipeline combining small cold-start data, RL, rejection sampling, and more RL, to “fill in the gaps” from R1-Zero’s deficits.\\nRefined Reasoning Through SFT + RL\\nDeepSeek-R1 addresses R1-Zero's limitations by injecting a small amount of supervised data before RL and weaving in additional alignment steps.\\nStage 1: “Cold-Start” SFT\\nThey gather a small number (~thousands) of curated, “human-friendly” chain-of-thought data covering common sense Q&A, basic math, standard instruction tasks, etc. Then, they do a short SFT pass on the base model. This ensures the model acquires:\\nBetter readability: Polished language style and formatting.\\nNon-reasoning coverage: Some conversation, factual QA, or creative tasks not easily rewarded purely by rule-based checks.\\nIn essence, the authors realized you can avoid the “brittleness” of a zero-SFT approach by giving the model a\\nseed\\nof user-friendly behaviors.\\nStage 2: Reasoning-Oriented RL\\nNext, as in R1-Zero, they apply large-scale RL for tasks like math and code. The difference is that now the model starts from a “cold-start SFT” checkpoint—so it retains decent language style while still learning verifiable tasks from a rule-based or tool-based reward. This RL stage fosters the same emergent chain-of-thought expansions but\\nwithout\\nthe random “language mixing” or bizarre structure.\\nStage 3: Rejection Sampling + Additional SFT\\nOnce that RL converges, they generate multiple completions per prompt from the RL checkpoint. Using a combination of automatic verifiers and some human checks, they pick the best outputs (“rejection sampling”) and build a new SFT dataset. They also incorporate standard writing/factual/safety data from DeepSeek-V3 to keep the model balanced in non-verifiable tasks. Finally, they re-fine-tune the base model on this curated set.\\nThis step addresses the “spotty coverage” problem even further: The best RL answers become training targets, so the model improves at chain-of-thought\\nand\\nclarity.\\nStage 4: RL for “All Scenarios”\\nLastly, they do another RL pass on diverse prompts—not just math/code but general helpfulness, safety, or role-playing tasks. Rewards may come from a combination of rule-based checks and large “preference” models (trained from user preference pairs). The final result is a model that:\\nRetains strong chain-of-thought for verifiable tasks,\\nAligns to broad user requests in everyday usage,\\nMaintains safer, more controlled outputs.\\nConnecting the Arcs: Efficiency & Emergence\\nDespite covering different angles - scaling laws, MoE, HPC scheduling, and large-scale RL - DeepSeek's work consistently follows these arcs:\\nCost and Memory Efficiency\\nThey systematically design methods (MLA, MoE gating, device-limited routing, FP8 training, DualPipe) to maximize hardware utilization even in constrained environments\\nHPC-level scheduling (PTX instructions, warp specialization) hides communication overhead and overcomes the limitations imposed by limited interconnect speeds on H800s\\nSparse + HPC Co-Design\\nFrom V2 to V3, we see an evolving mixture-of-experts approach, culminating in a 671B-parameter model feasible on H800 clusters.\\nThe authors repeatedly stress that HPC co-design is the only path to cheaply train multi-hundred-billion-parameter LLMs.\\nEmergent Reasoning\\nR1 pushes beyond standard supervised training, letting RL signals shape deep chain-of-thought. The synergy between\\npre-trained\\nscale and\\ntargeted post-training\\nyields advanced reasoning patterns like reflection or multi-step verification.\\nTaken as a whole, the DeepSeek series highlights how architecture, algorithms, frameworks, and hardware must be co-designed to handle LLM training at trillion-token scales. Looking to the future, it indicates that toolchain builders may want to find ways to capture some of these HPC optimizations as part of the model compilation path or training apparatus, and AI research teams may want to work closely with HPC expertise even in the early days of architecture ideation.\\nFootnotes\\n1:\\nNon-embedding FLOPs are the amount of FLOPs (Floating Point Operations per Second) used for pre-training certain layers of the transformer (non-embedding). The authors found only some layers contributed to the scaling formula.\\n2:\\nA model consists of billions on internal variables, which are called its\\nparameters\\n. These parameters gain their values (weights) during training. Before training, developers will set a number of different variables that control the training process itself, these are called\\nhyperparameters\\nSignificant Revisions\\n06 February 2025:\\nFirst published\", 'image_urls': [], 'title': 'The DeepSeek Series: A Technical Overview'}, {'url': 'https://dev.to/sayed_ali_alkamel/deepseek-and-the-power-of-mixture-of-experts-moe-ham', 'raw_content': 'DeepSeek and the Power of Mixture of Experts (MoE) - DEV Community\\nAdd reaction\\nLike\\nUnicorn\\nExploding Head\\nRaised Hands\\nFire\\nJump to Comments\\nSave\\nBoost\\nModerate\\nCopy link\\nCopied to Clipboard\\nShare to X\\nShare to LinkedIn\\nShare to Facebook\\nShare to Mastodon\\nReport Abuse\\nDeepSeek is causing a stir in the AI community with its open-source large language models (LLMs), and a key factor in its success is the\\nMixture of Experts (MoE)\\narchitecture. This approach allows DeepSeek to achieve impressive performance with remarkable efficiency, rivaling even giants like OpenAI\\'s GPT series. But what exactly is MoE, and how does it work within DeepSeek?\\nUnderstanding Mixture of Experts (MoE)\\nImagine a complex problem that requires a team of specialists with diverse expertise to solve. This collaborative approach is the essence of MoE. Instead of relying on a single massive model to handle every aspect of a problem, MoE divides the task among smaller, specialized expert networks, each focusing on a specific domain or sub-task.\\nThink of these experts as individual neural networks, each trained on different datasets or for specific tasks. For example, in a language model, one expert might specialize in grammar, another in factual knowledge, and yet another in generating different creative text formats. This specialization allows each expert to become highly proficient in its designated area, leading to improved overall performance.\\nA crucial component of MoE is the\\ngating network\\n. This acts like a manager or dispatcher, deciding which expert is best suited for a given input. It analyzes the input and intelligently routes it to the most relevant expert(s), ensuring efficient and accurate processing.\\nMoE offers a significant advantage through its\\nsparsity\\n. Unlike traditional models that activate all parameters for every input, MoE activates only the necessary experts for a given task. This selective activation significantly reduces computational cost and improves efficiency, allowing MoE models to scale to massive sizes without requiring a proportional increase in computing power.\\nMoE models can be implemented in various ways, including hierarchical structures. Hierarchical mixtures of experts use multiple levels of gating networks in a tree-like structure, with experts residing at the leaf nodes. This hierarchical approach allows for more complex and nuanced decision-making, further enhancing the model\\'s ability to handle diverse tasks.\\nFurthermore, MoE architectures enable large-scale models to reduce computation costs during pre-training and achieve faster performance during inference time. This efficiency stems from selectively activating only the specific experts needed for a given task, rather than activating the entire neural network for every task.\\nMoE in DeepSeek\\nDeepSeek leverages MoE to achieve remarkable efficiency and performance. Despite having hundreds of billions of parameters, DeepSeek activates only a small fraction (around 37 billion) for any given task. This selective activation, combined with other architectural innovations, leads to several benefits:\\nEfficient Resource Use\\n: DeepSeek significantly reduces computational costs by activating only the necessary experts. This efficiency is crucial for making large-scale AI models more accessible and affordable.\\nTask-Specific Precision:\\nDeepSeek handles various inputs with accuracy tailored to each task. This specialization allows the model to excel in diverse domains, from code generation to mathematical problem-solving.\\nScalability\\n: DeepSeek can easily scale by adding more specialized experts without significantly impacting computational requirements. This modularity makes DeepSeek adaptable and future-proof, allowing it to accommodate new tasks and domains as they emerge.\\nDeepSeek\\'s MoE implementation involves some unique strategies to further enhance efficiency and performance:\\nFine-grained expert segmentation\\n: Each expert is further divided into smaller experts, promoting specialization and preventing any single expert from becoming a generalist. This fine-grained approach ensures that each expert possesses highly focused knowledge, leading to more accurate and efficient processing.\\nShared expert isolation\\n: Certain experts are designated as \"shared experts\" and are always active, capturing common knowledge applicable across various contexts. This strategy helps to reduce redundancy and improve the model\\'s ability to generalize across different tasks.\\nExpert Choice (EC) routing algorithm\\n: DeepSeek utilizes the Expert Choice routing algorithm to achieve optimal load balancing among experts. This algorithm ensures that each expert receives an appropriate amount of data, preventing under-utilization or overload, and maximizing the overall efficiency of the model.\\nReplacing dense feed-forward network (FFN) layers with sparse MoE layers\\n: DeepSeek replaces traditional dense FFN layers with sparse MoE layers, enabling it to achieve higher capacity with lower computational costs. This architectural optimization contributes significantly to DeepSeek\\'s efficiency and scalability.\\nMitigating knowledge hybridity and knowledge redundancy\\n: DeepSeekMoE addresses the challenges of knowledge hybridity and knowledge redundancy by finely segmenting experts and introducing shared experts. This approach ensures that each expert acquires non-overlapping and focused knowledge, maximizing specialization and efficiency.\\nDeepSeek\\'s Training and Architecture\\nDeepSeek\\'s training data is sampled from a large-scale multilingual corpus, primarily focusing on English and Chinese but also encompassing other languages. This corpus is derived from diverse sources, including web text, mathematical material, coding scripts, published literature, and various other textual materials.\\nFor tokenization, DeepSeek utilizes byte pair encoding (BPE) tokenizers trained on a subset of the training corpus. This tokenization process allows the model to efficiently represent and process text data.\\nApplications of DeepSeek with MoE\\nDeepSeek\\'s powerful MoE architecture enables a wide range of applications across various domains:\\nCode Generation\\n: DeepSeek can automate coding tasks, including code generation, debugging, and review. This capability can significantly improve developer productivity and code quality.\\nBusiness Processes\\n: DeepSeek can streamline workflows, analyze data, and generate reports. This can help businesses automate repetitive tasks, gain insights from data, and make more informed decisions.\\nEducation\\n: DeepSeek can personalize learning, provide feedback, and assist with complex problem-solving. This can revolutionize education by providing students with tailored learning experiences and support\\nScientific Research\\n: DeepSeek\\'s focus on reasoning and problem-solving makes it particularly well-suited for applications in scientific research. It can assist scientists in analyzing data, formulating hypotheses, and exploring new avenues of inquiry.\\nBenefits of MoE in DeepSeek\\nThe use of MoE in DeepSeek brings several advantages that contribute to its overall effectiveness and impact:\\nImproved Performance\\n: DeepSeek achieves state-of-the-art results on various benchmarks, including coding, problem-solving, and language understanding. This high performance is a testament to the effectiveness of the MoE architecture and DeepSeek\\'s unique implementation.\\nReduced Training Costs\\n: DeepSeek requires significantly less training time and resources compared to other large models. This cost-effectiveness makes DeepSeek a more accessible and sustainable option for AI development.\\nFaster Inference\\n: DeepSeek\\'s selective activation of experts leads to faster response times. This speed is crucial for real-time applications and interactive AI systems.\\nEnhanced Scalability\\n: DeepSeek can easily accommodate new tasks and domains by adding more experts. This adaptability ensures that DeepSeek can continue to evolve and improve over time.\\nDeepSeek\\'s MoE implementation allows it to achieve comparable performance to larger models while using significantly fewer resources. For example, DeepSeek-V3 outperforms Llama 3.1 while requiring 11 times less training compute. This efficiency translates into practical benefits like shorter development cycles and more reliable outputs for complex projects.\\nChallenges of MoE in DeepSeek\\nWhile MoE offers significant benefits, it also presents some challenges that DeepSeek addresses through various techniques:\\nTraining Instability\\n: MoE models can be prone to routing collapses, where the same experts are repeatedly selected, hindering the learning process of others. DeepSeek mitigates this issue through its auxiliary-loss-free load balancing strategy and other training optimizations.\\nLoad Imbalance\\n: Uneven distribution of data among experts can negatively impact performance. DeepSeek\\'s Expert Choice routing algorithm and load balancing techniques address this challenge by ensuring an even distribution of data among experts.\\nHigh Memory Requirements\\n: All experts need to be loaded into memory, even if not actively used. This can be a limitation for resource-constrained environments. DeepSeek offers distilled versions of its models with reduced memory requirements to address this challenge.\\nGeneralization during fine-tuning\\n: MoE models can sometimes struggle to generalize during fine-tuning, leading to overfitting. DeepSeek employs various regularization techniques and training strategies to mitigate this issue.\\nLimitations of MoE inference\\n: MoE inference can face challenges such as high memory requirements and token overflow. DeepSeek addresses these limitations through optimizations in its architecture and inference process.\\nConclusion\\nDeepSeek\\'s innovative use of MoE has positioned it as a leading force in the world of open-source LLMs. By combining expert specialization with efficient resource utilization, DeepSeek achieves remarkable performance and scalability. Its open-source nature allows for community collaboration and customization, unlike proprietary models like GPT-4, democratizing AI development and making it more accessible. As DeepSeek continues to evolve, we can expect even more groundbreaking applications and advancements in the field of AI, particularly in areas that require advanced reasoning and problem-solving, such as education and scientific research.\\nKeywords\\nDeepSeek, Mixture of Experts, MoE, Large Language Model, LLM, AI, Artificial Intelligence, Deep Learning, Natural Language Processing, NLP, Code Generation, Business Processes, Education, Open Source, Efficiency, Scalability, Performance, Training Costs, Inference Speed, DeepSeek-V3, DeepSeekMoE, Multi-Token Prediction, MTP\\nCreate template\\nTemplates let you quickly answer FAQs or store snippets for re-use.\\nSubmit\\nPreview\\nDismiss\\nMahima Thacker\\nMahima Thacker\\nMahima Thacker\\nFollow\\n7+ Web3 Hackathon Wins | Full Stack Developer | Web3 Developer | Tech Educator | SheFi Graduate | Member Developer_DAO, H.E.R DAO\\nEmail\\nmahimathacker75@gmail.com\\nLocation\\nBhuj, Gujarat, India\\nEducation\\nI am a 3rd year B.Tech Student at GEC, Modasa\\nWork\\nI am working as a Full stack Web3 Developer at CodeCrunch Techlabas\\nJoined\\nDec 23, 2024\\n•\\nJan 29\\nCopy link\\nHide\\nVery informative. Thanks for sharing\\n@sayed_ali_alkamel\\n🫡\\nAre you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment\\'s\\npermalink\\n.\\nHide child comments as well\\nConfirm\\nFor further actions, you may consider blocking this person and/or\\nreporting abuse\\nRead next\\nesProc SPL & MongoDB: A Match Made in Data Heaven\\nBrett Hoffman -\\nFeb 24\\n11+ Best Websites to Download Free Next.js Templates\\nVinish Bhaskar -\\nFeb 24\\nClaude Code Agentic Code Demo\\nVijay Kodam -\\nFeb 24\\nAnthropic\\'s Claude Sonnet 3.7 is here!\\nJoaquin Diaz -\\nFeb 24\\nSayed Ali Alkamel\\nFollow\\n💡 Google Developer Expert | Building with ⚡ Flutter, 🤖 AI, ☁️ Cloud, & 💻 Backend (Java & Spring) | 10+ years crafting apps loved by millions | 🌍 Global Tech Speaker 🚀\\nLocation\\nBahrain 🇧🇭\\nJoined\\nJan 3, 2025\\nDeepSeek and the Power of Mixture of Experts (MoE)\\n#\\nai\\n#\\nmachinelearning\\n#\\nopensource\\n#\\ndeeplearning\\nWant to Unlock the True Potential of Google\\'s Gemini AI? This Guide Will Show You How!\\n#\\nai\\n#\\ngemini\\n#\\nmachinelearning\\n#\\nlargelanguagemodels\\nDeepSeek-R1: The Open-Source AI That\\'s Making Waves (on a Budget!)\\n#\\nai\\n#\\nopensource\\n#\\ndeepseek\\n#\\nnews\\nWe\\'re a place where coders share, stay up-to-date and grow their careers.\\nLog in\\nCreate account', 'image_urls': [], 'title': 'DeepSeek and the Power of Mixture of Experts (MoE) - DEV Community'}, {'url': 'https://medium.com/aimonks/deepseek-v3-efficient-and-scalable-ai-with-mixture-of-experts-8bd945b5ea3f', 'raw_content': 'DeepSeek-V3: Efficient and Scalable AI with Mixture-of-Experts | by My Social | 𝐀𝐈 𝐦𝐨𝐧𝐤𝐬.𝐢𝐨 | Dec, 2024 | Medium\\nOpen in app\\nSign up\\nSign in\\nWrite\\nSign up\\nSign in\\nDeepSeek-V3: Efficient and Scalable AI with Mixture-of-Experts\\nMy Social\\n·\\nFollow\\nPublished in\\n𝐀𝐈 𝐦𝐨𝐧𝐤𝐬.𝐢𝐨\\n·\\n8 min read\\n·\\nDec 31, 2024\\n--\\n3\\nListen\\nShare\\nPresentational View\\nIntroduction\\nScalable and efficient AI models are among the focal topics of the current artificial intelligence agenda. The purpose is to develop models that could solve more and more difficult problems and process ever larger amounts of data, while not demanding outrageous amounts of computational power for that. As of yesterday’s techniques of LLM like the transformer, though quite effective, sizable, in use, their computational costs are relatively high, making them relatively unusable.\\nThe use of a Mixture-of-Experts (MoE AI models) has come out as one of the best solutions to this challenge. MoE models split one model into multiple specific, smaller sub-networks, known as ‘experts’ where the model can greatly enhance its capacity without experiencing destructive escalations in computational expense. However, these models are not without their problems such as; imbalance distribution of data among experts and highly demanding computational resources during the training phase.\\nThese challenges are solved by DeepSeek-V3 Advanced approaches such as improvements in gating for dynamic routing and less consumption of attention in this MoE. All these enhance the equality of distribution of the specialists and performant computing, thereby offering advanced intelligent systems for paramount application in different fields.\\nWhat is DeepSeek-V3?\\nThe DeepSeek-V3 is a strong Mixture-of-Experts (MoE) large language model that was created by the DeepSeek AI. This architecture can make it achieve high performance with better efficiency and extensibility. It is available in varying sizes; it has the basic version in its list of offerings depending on the computation demands of the user.\\nKey Features of DeepSeek-V3\\nDeepSeek-V3 leverages its MoE architecture to achieve several key advantages:\\nEfficiency\\n: DeepSeek-V3 uses Mixture-of-Experts (MoE) by enabling a portion of its parameters say, 37B out of 671B, for any input. This selective activation reduces the computational costs considerably bringing out the ability to perform well while frugal with computation.\\nScalability\\n: The proposed MoE design enables effortless scalability by incorporating more specialized experts without focusing all the model. This modularity also renders DeepSeek-V3 easily scalable and ready for future improvements and the possibility of incorporating new forms of assessments without the need to carry out a new training process.\\nSpecialization\\n: Within MoE architecture, individual experts can be trained to perform specific domains to improve the performance in such areas. What DeepSeek-V3 lacks in general adaptability, it more than makes up for in specialized environments like coding and mathematics in which domain-specific knowledge is valuable.\\nImproved Inference Speed\\n: Because only a subset of the network is activated to solve a given problem, DeepSeek- V3 has even faster inference rates. This selective activation eliminates delays in managing responses and make interactions faster which is useful for real-time services.\\nCapabilities/Use Cases of DeepSeek-V3\\nEnhanced Code Generation and Debugging\\n: Since DeepSeek-V3 is built with MoE architecture, this makes it easy to generate experts focused on various programming languages, or coding styles. This targeted approach leads to more effective generation of code since the defects are targeted and thus coded in contrast to general purpose models where the defects could be haphazard. The agents’ differentiation allows the model to be more aware of the subtleties of different programming languages and provide less prone to errors of context.\\nAdvanced Mathematical Problem-Solving\\n: Easy comparisons: Aiming at grabbing the correct spot, the MoE architecture enables DeepSeek-V3 to use specialists precisely trained on mastery of mathematics to yield accuracy in this realm. Such individuals should be capable to solve difficult equations, logical proofs as well as most other qualitative mathematical problems with higher precision. Specialists in the model can improve mastery of mathematics both in content and method because specific workers will be assigned to mathematical tasks.\\nNext-Generation AI Assistants\\n: Because the DeepSeek-V3 system integrates specialists in various phases, it facilitates the development of improved AI companions. These assistants can offer balanced and contextually motivated solutions encompassing reasoning, encoding, and mathematical Reasoning abilities to bosses. The structural design of the MoE allows these assistants to change and better serve the users in a wide range of areas.\\nDeepSeek-V3 Architecture and Key Components\\nGood information flow is one of the main characteristics of the DeepSeek-V3 architecture. Input data pass through a number of ‘Transformer Blocks,’ as shown in figure below. Within each of the blocks, a Multi-Head Latent Attention module, allowed attention on different parts of the input sequence to be selectively computed to produce an Output Hidden ut. This output is then passed to the ‘DeepSeekMoE’ block which is the novel part of DeepSeek-V3 architecture . As can be seen in the figure below, the input passes through these key components.\\nsource —\\nhttps://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\\nThe DeepSeekMoE block involved a set of multiple ‘experts’ that are trained for a particular domain or a task. This makes it possible for the model to forward different subparts of the input to the relevant expert, thus supporting both optimization and integration of specialists. Rather than invoking all the experts in the network for any input received, DeepSeek-V3 calls only irrelevant ones, thus saving on costs, although with no compromise to efficiency. This dynamic routing is accompanied by an auxiliary-loss-free approach to load balancing that equally distributes load amongst the experts, thereby preventing congestion and improving the efficiency rate of the overall model. Load balancing is paramount in the scalability of the model and utilization of the available resources in the best way.\\nDeepSeek-V3 uses other innovations apart from MoE architecture and efficient routing, as specified below. Multi-Token Prediction (MTP) allows the training of such models with multiple future tokens at once enhancing learning and possible decoding efficiencies. The above methods of parallel processing during training also help in the process. In addition, DeepSeek-V3 also employs knowledge distillation technique that enables the transfer of reasoning ability from the DeepSeek-R1 series. The MoE architecture along with Multi-Token Prediction and load.\\nPerformance Evaluation with Other Models\\nBased on the strict comparison with other powerful language models, DeepSeek-V3’s great performance has been shown convincingly. Different benchmarks encompassing both English and necessary Chinese language tasks are used to compare DeepSeek-V3 to open-source competitors such as Qwen2.5 and LLaMA-3.1 and closed-source competitors such as GPT-4o and Claude-3.5-Sonnet. These benchmarks cover various crucial areas: general facts and knowledge (MMLU, MMLU-Pro), logical and rationality (DROP, LongBench v2), code writing (HumanEval-Mul, LiveCodeBench) and mathematical computation (AIME, MATH-500). Analyzing the results, it becomes apparent that DeepSeek-V3 is also among the best variant most of the time being on par with and sometimes outperforming the other open-source counterparts while almost always being on par with or better than the closed-source benchmarks.\\nsource —\\nhttps://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\\nIn addition to these comparative standards, several other test and experiments are performed to evaluate the ability of DeepSeek- V3. Among them there are, for example, ablation studies which shed the light on the contributions of particular architectural components of the model and training strategies. Tests on integrated reference recognition and sequential recall evaluate the performance of DeepSeek-V3 which can handle comprehension of a text sequence. More specifically, coding and mathematical reasoning tasks are specifically highlighted as beneficial from the new architecture of DeepSeek-V3 while the report credits knowledge distillation from DeepSeek-R1 as being particularly beneficial.\\nComparing DeepSeek-V3, Phi-4, and Llama 3.3\\nDeepSeek-V3, Phi-4, and Llama 3.3 have strengths in comparison as large language models. While DeepSeek-V3, due to its architecture being Mixture-of-Experts, and trained with a significantly higher amount of data, beats even closed-source versions on some specific benchmarks in maths, code, and Chinese languages, it falters significantly behind in other places, for instance, its poor performance with factual knowledge for English. Phi-4 is trained on a mix of synthesized and organic data, focusing more on reasoning, and gives outstanding performance in STEM Q&A and coding, sometimes even giving more accurate results than its teacher model GPT-4o. Its limitations include a lower context window and susceptibility to hallucinations.\\nLlama 3.3 places priority on multilingual dialogue and general language understanding, with a larger context window, suitable for processing extended text. Though it works well in multiple language tasks, it doesn’t have the focused strengths of Phi-4 on STEM or DeepSeek-V3 on Chinese.\\nThe choice of model depends on the specific application. Phi-4 is suitable for STEM use cases, Llama 3.3 for multilingual dialogue and long-context applications, and DeepSeek-V3 for math, code, and Chinese performance, although it is weak in English factual knowledge. Testing and safety assessments are important before deployment.\\nHow to Access and Use this model?\\nDeepSeek-V3 provides many ways to query and work with the model. Researches and developers can get different types of models such those of base model from Hugging Face for downloading. DeepSeek provides a chat demo that also demonstrates how the model functions. For more in-depth understanding of how the model works will find the source code and further resources in the GitHub repository of DeepSeek. At this stage, DeepSeek-V3 is primarily targeted to be used in research and development labs. Licensing may be required for commercial use.\\nLimitations and Future Work\\nDespite the high test accuracy, low time complexity, and satisfactory performance of DeepSeek-V3, this study has several shortcomings. Its large recommended deployment size may be problematic for lean teams as there are simply too many features to configure. While it outperforms its predecessor with regard to generation speed, there is still room for enhancement.\\nFuture work will concern further design optimization of architectures for enhanced training and inference performance, potential abandonment of the Transformer architecture, and ideal context size of infinite. Subsequent studies will also focus on enhancing few-shot learning, stable alignment approaches, and more effective reinforcement learning reward signals.\\nConclusion\\nDifferent stakeholders can benefit from DeepSeek-V3. For experts in AI, its MoE architecture and training schemes are the basis for research and a practical LLM implementation. Organizations continue to enjoy its flexibility and effectiveness making it easy to embark on large-scale implementation of complex NLP features such as conversational agents and code-generating models. For the general public, DeepSeek-V3 suggests advanced and adaptive AI tools in everyday utilization including a better search, translate, and virtual assistant features improving flow of information and simplifying everyday tasks.\\nSource\\nWebsite:\\nhttps://www.deepseek.com/\\nResearch paper:\\nhttps://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\\nGitHub Repo:https://github.com/deepseek-ai/DeepSeek-V3\\nDeepSeek-V3 model variant:\\nhttps://huggingface.co/deepseek-ai/DeepSeek-V3\\nDeepSeek-V3 base model variant:\\nhttps://huggingface.co/deepseek-ai/DeepSeek-V3-Base\\nTry model:\\nhttps://chat.deepseek.com/\\nDisclaimer\\n— This article is intended purely for informational purposes. It is not sponsored or endorsed by any company or organization, nor does it serve as an advertisement or promotion for any product or service. All information presented is based on publicly available resources and is subject to change. Readers are encouraged to conduct their own research and due diligence.\\nOriginally published at\\nhttps://socialviews81.blogspot.com\\n.\\nDeepseek V3\\nMixture Of Experts\\nDeepseek\\nOpen Source Ai\\nOpen Source Llm\\nPublished in\\n𝐀𝐈 𝐦𝐨𝐧𝐤𝐬.𝐢𝐨\\n4.2K Followers\\n·\\nLast published\\n16 hours ago\\nAImonks (\\nhttps://medium.com/aimonks\\n) is an AI-Educational Publication.\\nFollow\\nWritten by\\nMy Social\\n66 Followers\\n·\\n15 Following\\nTech Blogger | Emerging AI/ML Model Insights, its fascinating evolution as a technology, transformative role across diverse sectors, impact on our daily lives!\\nFollow\\nResponses (\\n3\\n)\\nSee all responses\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams', 'image_urls': [], 'title': 'DeepSeek-V3: Efficient and Scalable AI with Mixture-of-Experts | by My Social | 𝐀𝐈 𝐦𝐨𝐧𝐤𝐬.𝐢𝐨 | Dec, 2024 | Medium'}, {'url': 'https://adasci.org/deepseek-v3-explained-optimizing-efficiency-and-scale/', 'raw_content': \"DeepSeek-V3 Explained: Optimizing Efficiency and Scale\\nSkip to content\\n$\\n0.00\\n0\\nCart\\nMembers Area\\nDeep Dives\\nDeepSeek-V3 Explained: Optimizing Efficiency and Scale\\nExplore how DeepSeek-V3 redefines AI with groundbreaking architecture, efficient training, and impactful real-world applications in coding, education, and multilingual systems.\\nBy\\nAniruddha Shrikhande\\nPublished on\\nJanuary 2, 2025\\nExplore more from ADaSci\\nVisualizing Insights: Guide to Effective Data Storytelling\\nMachine learning approach to predict patient position for preventing bedsores\\nWhat are the Benefits of Chartered Data Scientist™\\nA case study on Credit Risk Analysis using Taiwanese Banking Data\\nHow to Leverage ADaSci Continuous Learning Program for a Generative AI Career?\\nEnhancing Taxpayer Risk Prediction through LLM- Driven Profile Tuning\\nA Hands-on Guide to Airtrain AI: A No-code Compute Platform\\nDeepSeek-V3 Explained: Optimizing Efficiency and Scale\\n8 Things That Will Set Apart A CDS™ From Other Data Scientists\\nWhy do Enterprises Love RAG?\\nDeepSeek-V3 marks a transformative advancement in the domain of large language models (\\nLLMs\\n), setting a new benchmark for open-source AI. As a Mixture-of-Experts (MoE) model with 671 billion parameters—37 billion of which are activated per token. Featuring innovations like Multi-Head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction, DeepSeek-V3 delivers unprecedented capabilities in coding, mathematics, and reasoning tasks. This article offers an in-depth exploration of its architecture, training strategies, innovations, and real-world applications.\\nTable of Content\\nWhat is DeepSeek-V3?\\nDeepSeek-V3 Architecture Unveiled\\nAdvanced Training and Deployment Strategies\\nKey Features and Innovations\\nReal-World Use Cases\\nWhat is DeepSeek-V3?\\nDeepSeek-V3 is an open-source large language model that leverages Mixture-of-Experts (MoE) architecture to achieve state-of-the-art performance in computational efficiency and accuracy. It features 671 billion parameters, with 37 billion activated per token, enabling it to handle complex tasks in coding, mathematics, and reasoning. Designed for scalability and cost-effectiveness, it incorporates innovative techniques like Multi-Head Latent Attention (MLA), FP8 mixed precision training, and a novel Multi-Token Prediction (MTP) objective.\\nDeepSeek-V3 Architecture Unveiled\\nAt its core, DeepSeek-V3 builds upon the Transformer framework but incorporates several advanced components to achieve its groundbreaking performance. Key elements of the architecture include:\\nDeepSeek-V3’s Architecture\\nMulti-Head Latent Attention (MLA)\\nMLA enhances inference efficiency by introducing low-rank joint compression for attention keys and values. This technique reduces memory overhead while maintaining high attention quality. By caching only compressed latent vectors, MLA minimizes key-value storage requirements during inference.\\nDeepSeekMoE\\nThe DeepSeekMixture-of-Experts mechanism employs finer-grained experts with innovative load-balancing techniques. Unlike traditional MoE architectures, It eliminates the need for auxiliary loss by using dynamic bias adjustments, ensuring balanced expert loads without performance trade-offs.\\nMulti-Token Prediction (MTP)\\nDeepSeek-V3 incorporates a novel MTP objective, allowing the model to predict multiple tokens at once. This densifies training signals and enables better pre-planning of token representations, boosting performance on complex benchmarks.\\nMulti-Token Prediction (MTP) Architecture\\nAdvanced Training and Deployment Strategies\\nEfficient Training Framework\\nDeepSeek-V3 achieves remarkable training efficiency through its FP8 mixed precision framework. By leveraging low-precision computation and storage, it reduces GPU memory usage and accelerates training. The model’s pre-training required only 2.788 million H800 GPU hours, translating to approximately $5.576 million in cost.\\nDualPipe Algorithm\\nThe DualPipe algorithm revolutionizes pipeline parallelism by overlapping computation and communication phases. This minimizes pipeline bubbles and ensures near-zero all-to-all communication overhead, enabling seamless scaling across multiple nodes.\\nDeployment Optimization\\nFor inference, It separates the prefilling and decoding stages, using modular deployment strategies to optimize GPU load and maintain low latency. Techniques like redundant expert hosting and dynamic routing further enhance computational efficiency.\\nKey Features and Innovations\\nAuxiliary-Loss-Free Load Balancing\\nTraditional MoE models rely on auxiliary loss to prevent expert overload, which often degrades performance. DeepSeek-V3 pioneers a bias-based dynamic adjustment strategy, achieving load balance without compromising accuracy.\\nFP8 Mixed Precision Framework\\nBy adopting FP8 precision for key computations, It reduces memory and computational costs. Fine-grained quantization and increased accumulation precision ensure numerical stability and training reliability.\\nMixed precision framework with FP8 data format\\nMulti-Token Prediction (MTP)\\nThe sequential prediction of multiple tokens not only improves training efficiency but also enhances inference capabilities, enabling faster and more accurate generation.\\nDeepSeek-V3-Base vs other open-source base models\\nDeepSeek-V3’s Key Features and Innovations\\nReal-World Use Cases\\nDeepSeek-V3’s versatility makes it an invaluable asset across various domains:\\nEducational Tools\\nAchieving 88.5 on the MMLU benchmark, It excels in answering complex educational queries and providing accurate, context-rich responses.\\nCoding Platforms\\nWith top-tier performance on coding benchmarks like LiveCodeBench, It is ideal for competitive programming platforms and code suggestion tools.\\nMathematical Applications\\nThe model’s state-of-the-art performance on MATH-500 highlights its ability to tackle advanced mathematical reasoning and problem-solving tasks.\\nMultilingual Knowledge Systems\\nDeepSeek-V3 demonstrates superior performance in multilingual benchmarks, making it a powerful tool for global knowledge management and translation.\\nFinal Words\\nDeepSeek-V3 represents a paradigm shift in open-source AI, delivering unmatched performance and efficiency. By integrating cutting-edge architectural innovations and training techniques, it narrows the gap between open-source and closed-source models. Its versatility across domains—from education to coding—underscores its potential as a transformative tool in the AI landscape. As the field advances, DeepSeek-V3’s innovations set a strong foundation for future developments.\\nReferences\\nDeepSeek-V3 GitHub Repository\\nDeepSeek-V3 Research Paper\\nAniruddha Shrikhande\\nAniruddha Shrikhande is an AI enthusiast and technical writer with a strong focus on Large Language Models (LLMs) and generative AI. Committed to demystifying complex AI concepts, he specializes in creating clear, accessible content that bridges the gap between technical innovation and practical application. Aniruddha's work explores cutting-edge AI solutions across various industries. Through his writing, Aniruddha aims to inspire and educate, contributing to the dynamic and rapidly expanding field of artificial intelligence.\\nThe Chartered Data Scientist\\nDesignation\\nAchieve the highest distinction in the data science profession.\\nRegister\\nElevate Your Team's AI Skills with our Proven Training Programs\\nStrengthen Critical AI Skills with Trusted Generative AI Training by Association of Data Scientists.\\nFind Out More\\nOur AI Courses\\nVisit AI Academy\\nSale!\\n[Upcoming Webinar] AI-Driven Risk Management in Derivatives Trading\\n$\\n20.00\\nOriginal price was: $20.00.\\n$\\n0.00\\nCurrent price is: $0.00.\\nAdd to cart\\n[Upcoming Hands-On Workshop] Building Agentic AI Workflows\\n$\\n19.99\\nAdd to cart\\nWebinar Recording – Unpacking Parallelism: Practical Strategies for Scaling AI Workflows\\n$\\n5.99\\nAdd to cart\\nOur Accreditations\\nGet global recognition for AI skills\\nChartered Data Scientist (CDS™)\\nThe highest distinction in the data science profession. Not just earn a charter, but use it as a designation.\\nLearn more\\nCertified Data Scientist - Associate Level\\nGlobal recognition of data science skills at the beginner level.\\nLearn more\\nCertified Generative AI Engineer\\nAn upskilling-linked certification initiative designed to recognize talent in generative AI and large language models\\nLearn more\\nJoin thousands of members and receive all benefits.\\nBecome Our Member\\nWe offer both Individual & Institutional Membership.\\nFind out more\", 'image_urls': [{'url': 'https://adasci.org/wp-content/uploads/2023/01/happy-indian-university-student-walking-with-mobil-2021-08-27-16-35-34-utc-scaled.jpg', 'score': 3}, {'url': 'https://adasci.org/wp-content/uploads/2023/01/indian-man-in-office-portrait-2022-11-06-23-14-38-utc-scaled.jpg', 'score': 3}, {'url': 'https://adasci.org/wp-content/uploads/2024/02/handsome-man-smiling-wearing-a-suit-in-a-conversat-2023-11-27-05-15-51-utc-scaled.jpg', 'score': 3}, {'url': 'https://adasci.org/wp-content/uploads/2022/12/ADASCI-15-1-1.png', 'score': 2}, {'url': 'https://adasci.org/wp-content/uploads/2024/12/Screenshot-2024-12-31-at-2.57.47-PM-1024x826.png', 'score': 2}, {'url': 'https://adasci.org/wp-content/uploads/2024/12/image-13.png', 'score': 1}, {'url': 'https://adasci.org/wp-content/uploads/2024/07/ADaSci-Agentic-AI-Workshop-1024x576.png', 'score': 1}], 'title': 'DeepSeek-V3 Explained: Optimizing Efficiency and Scale'}, {'url': 'https://medium.com/@amirhossein_dehghaniazar/deepseek-and-mixture-of-experts-revolutionizing-ai-efficiency-and-speed-1ce1f931e45c', 'raw_content': 'DeepSeek and Mixture of Experts (MoE): How AI is Becoming Faster and Smarter | Medium\\nOpen in app\\nSign up\\nSign in\\nWrite\\nSign up\\nSign in\\nDeepSeek and Mixture of Experts: Revolutionizing AI Efficiency and Speed\\nAmirhossein Dehghanazar\\n·\\nFollow\\n3 min read\\n·\\nFeb 1, 2025\\n--\\nListen\\nShare\\nIntroduction: The Quest for Smarter, Faster AI\\nAs artificial intelligence models grow larger and more complex, a critical challenge emerges: balancing performance with efficiency. Traditional models, while powerful, often struggle with computational demands. Enter\\nDeepSeek\\n— a cutting-edge AI framework leveraging the\\nMixture of Experts (MoE)\\narchitecture to redefine speed and scalability. This article explores how DeepSeek and MoE are transforming AI, making it faster, leaner, and more adaptable than ever before.\\nWhat is Mixture of Experts (MoE)?\\nMoE is a paradigm shift in neural network design. Instead of a monolithic model processing every input, MoE divides the workload among specialized subnetworks (“experts”) and a gating mechanism that dynamically routes tasks.\\nExperts\\n: Smaller neural networks, each excelling in specific data types or tasks (e.g., grammar, imagery, or numerical patterns).\\nGating Network\\n: A smart router that selects the most relevant experts for each input.\\nImagine a conference where specialists deliver targeted talks instead of a single speaker covering everything. MoE works similarly, activating only the necessary experts, slashing computational costs while maintaining high accuracy.\\nDeepSeek: Harnessing MoE for Breakthrough Performance\\nDeepSeek integrates MoE with innovative engineering to optimize AI workflows. Here’s how it stands out:\\nDynamic Computation\\nUnlike traditional models that activate all neurons for every input, DeepSeek’s gating network cherry-picks experts. This “conditional computation” reduces redundant processing, accelerating both training and inference.\\nScalability Without Sacrifice\\nAdding more experts scales DeepSeek’s capabilities linearly with resources, avoiding the quadratic cost spikes of dense models. This makes it ideal for applications requiring rapid adaptation, like real-time translation or autonomous systems.\\nEfficient Training\\nTraining MoE models can be challenging due to uneven expert utilization. DeepSeek tackles this with advanced load-balancing techniques and regularization, ensuring all experts contribute meaningfully without overfitting.\\nWhy DeepSeek and MoE Are Game-Changers\\nBlazing Speed\\nBy activating only 20–30% of its network per task, DeepSeek achieves inference speeds up to\\n5x faster\\nthan traditional models. This is critical for latency-sensitive applications like voice assistants or financial trading algorithms.\\nCost Efficiency\\nReduced computational load translates to lower cloud costs and energy consumption. Startups and enterprises alike can deploy powerful AI without breaking the bank.\\nVersatility\\nDeepSeek excels in diverse domains:\\nNatural Language Processing (NLP)\\n: Dynamic routing improves context-aware translations and sentiment analysis.\\nRecommendation Systems\\n: Experts tailor suggestions based on user behavior (e.g., Netflix’s personalized content).\\nHealthcare\\n: Specialized experts analyze medical images, genomics, and patient data with precision.\\nEdge Deployment\\nCompact, efficient models enable AI on edge devices — think smartphones, IoT sensors, and autonomous drones — without relying on cloud servers.\\nReal-World Impact: Where DeepSeek Shines\\nInstant Language Translation\\n: A traveler’s app using DeepSeek can translate street signs in real time, leveraging grammar and visual experts for accuracy.\\nSmart Retail\\n: MoE-powered systems analyze customer data to predict trends and optimize inventory.\\nClimate Modeling\\n: Experts specialize in atmospheric, oceanic, or geological data, accelerating simulations to combat climate change.\\nThe Future of AI is Efficient\\nDeepSeek and MoE represent more than a technical upgrade — they signal a shift toward sustainable, scalable AI. As models grow, efficiency will dictate their practicality. Innovations like sparse activation and dynamic routing are not just optimizations; they’re necessities for democratizing AI across industries.\\nLooking ahead, expect DeepSeek to push boundaries in federated learning, robotics, and personalized education. The era of “bigger is better” is evolving into “smarter and swifter,” thanks to MoE’s revolutionary architecture.\\nConclusion: Embracing the Speed Revolution\\nDeepSeek’s fusion of MoE and engineering excellence offers a blueprint for the future of AI. By prioritizing efficiency without compromising power, it unlocks new possibilities for developers and businesses. Whether you’re building the next-gen app or researching AI ethics, understanding MoE is key to staying ahead.\\nThe message is clear: in the race toward advanced AI, speed and intelligence are no longer trade-offs. With DeepSeek, they’re a unified force.\\nReady to explore DeepSeek?\\nDive into their open-source libraries or experiment with MoE frameworks like Google’s TensorFlow or PyTorch. The future of efficient AI is here—and it’s waiting for your contribution.\\nFollow me for more insights on AI breakthroughs, and let’s connect on LinkedIn/Twitter to discuss the next big thing in tech!\\nAI\\nDeep Learning\\nLlm\\nMoe\\nArtificial Intelligence\\nFollow\\nWritten by\\nAmirhossein Dehghanazar\\n116 Followers\\n·\\n471 Following\\nHi, I’m Amirhossein , a software engineer from Tehran, passionate about crafting seamless digital experiences and bringing ideas to life through code.\\nFollow\\nNo responses yet\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams', 'image_urls': [], 'title': 'DeepSeek and Mixture of Experts (MoE): How AI is Becoming Faster and Smarter | Medium'}, {'url': 'https://www.modular.com/ai-resources/exploring-deepseek-r1-s-mixture-of-experts-model-architecture', 'raw_content': \"Exploring DeepSeek-R1's Mixture-of-Experts Model Architecture - AI Resources\\nAIÂ\\xa0REsources Home\\nExploring DeepSeek-R1's Mixture-of-Experts Model Architecture\\nPopular\\nML Compiler Technical Primer\\nQuantization Technical Primer\\nMixtral of Experts\\nEfficient Memory Management for LLM Serving with PagedAttention\\nRoBERTa: A Robustly Optimized BERT Pretraining Approach\\n+ View more\\nCategories\\nMixture of Experts (MoE)\\nDeepSeek-R1\\nTest Time Compute\\nAMD MI300X\\nNVIDIA H100\\nNVIDIA H200\\nNVIDIA A100\\nEmbedding Models\\nOffline Batch Inference\\nText Embedding\\nPrometheus & Grafana\\nSpeculative Decoding\\nPrefix Caching\\nGGUF Models\\nFP8 with LLMs\\nLLM Serving\\nFunction Calling\\nStructured JSON\\nKV Cache\\nAI Foundations\\nResearch\\nIndustry\\nAgents\\nContext Windows\\nModels\\nML Systems\\nExploring DeepSeek-R1's Mixture-of-Experts Model Architecture\\nIntroduction\\nIn the rapidly evolving field of artificial intelligence, model efficiency and scalability are paramount. DeepSeek-R1, introduced in January 2025 by the Chinese AI startup DeepSeek, exemplifies these principles through its innovative Mixture-of-Experts (MoE) architecture. This article delves into the intricacies of DeepSeek-R1's MoE design, exploring its structure, advantages, and the broader implications for AI development.\\nUnderstanding Mixture-of-Experts Architecture\\nThe Mixture-of-Experts (MoE) architecture is a neural network design that incorporates multiple expert sub-models, each specializing in different aspects of data processing. A gating mechanism dynamically selects the most relevant experts for each input, enabling the model to allocate computational resources efficiently. This approach contrasts with traditional dense models, where all parameters are active during every computation, leading to higher resource consumption.\\nDeepSeek-R1's MoE Implementation\\nDeepSeek-R1 employs an MoE framework comprising 671 billion parameters. However, during any given forward pass, only 37 billion parameters are activated. This selective activation is achieved through a sophisticated gating mechanism that routes inputs to the most pertinent experts, thereby optimizing computational efficiency without compromising performance.\\nGating Mechanism\\nThe gating mechanism in DeepSeek-R1 evaluates incoming data and determines which experts should be engaged for processing. This dynamic routing ensures that only the most relevant experts are utilized, reducing unnecessary computations and enhancing processing speed.\\nExpert Specialization\\nEach expert within the MoE architecture is trained to specialize in specific data patterns or tasks. This specialization allows the model to handle a diverse range of inputs more effectively, as each expert can focus on mastering a particular subset of the data.\\nAdvantages of DeepSeek-R1's MoE Architecture\\nThe MoE architecture offers several notable benefits:\\n**Computational Efficiency**: By activating only a subset of experts during each forward pass, DeepSeek-R1 significantly reduces computational load, leading to faster processing times and lower energy consumption.\\n**Scalability**: The modular nature of the MoE framework allows for seamless scaling. New experts can be added to the model to enhance its capacity without necessitating a complete retraining of the entire system.\\n**Enhanced Performance**: Specialized experts improve the model's ability to handle complex and varied tasks, as each expert is fine-tuned for specific data patterns.\\nDeploying DeepSeek-R1 Using MAX Platform\\nFor developers aiming to implement DeepSeek-R1 or similar models, the Modular Accelerated Xecution (MAX) platform is an exceptional tool due to its ease of use, flexibility, and scalability. MAX supports PyTorch and HuggingFace models out of the box, enabling rapid development, testing, and deployment of large language models (LLMs).\\nPyTorch and HuggingFace Integration\\nThe MAX platform's compatibility with frameworks like PyTorch and HuggingFace ensures that developers can leverage existing models and tools, facilitating a smoother deployment process. This integration is particularly beneficial for those looking to implement advanced NLP models in their applications.\\nDeploying with MAX Platform\\nTo deploy a PyTorch model from HuggingFace using the MAX platform, follow these steps:\\nInstall the MAX CLI tool:\\nPython\\ncurl -ssL https://magic.modular.com | bash\\n&& magic global install max-pipelines\\nDeploy the model using the MAX CLI:\\nPython\\nmax-serve serve --huggingface-repo-id=deepseek-ai/DeepSeek-R1-Distill-Llama-8B\\n--weight-path=unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf\\nReplace 'model_name' with the specific model identifier from HuggingFace's model hub. This command will deploy the model with a high-performance serving endpoint, streamlining the deployment process.\\nConclusion\\nDeepSeek-R1 represents a significant advancement in AI development, showcasing China's growing capabilities in this field. Its efficient architecture, cost-effective training methodology, and impressive performance benchmarks position it as a formidable contender in the AI landscape. The integration with platforms like Modular's MAX further enhances its applicability, providing developers with the tools needed to deploy AI applications efficiently. As the AI field continues to evolve, models like DeepSeek-R1 exemplify the rapid advancements and the potential for innovation in this dynamic domain.\\nNext\\nDeepSeek-R1\\nDeepSeek-R1 vs. ChatGPT: A Comparative Analysis\\nDeepSeek-R1\\nDeepSeek-R1's Open-Source Approach: Benefits and Challenges\\nOn this page\\nLink 1\\nLink 2\\nDeploy Gen AI right now\\nGet started\\nView License\\nMAX for Enterprise\\nMixture of Experts (MoE)\\nDeepSeek-R1\\nMixture of Experts (MoE)\\nMixture of Experts (MoE)\\nMixture of Experts (MoE)\\nMixture of Experts (MoE)\\nMixture of Experts (MoE)\\nMixture of Experts (MoE)\\nDeepSeek-R1\\nDeepSeek-R1\\nDeepSeek-R1\\nDeepSeek-R1\\nDeepSeek-R1\\nDeepSeek-R1\\nTest Time Compute\\nDeepSeek-R1\\nDeepSeek-R1\\nTest Time Compute\\nTest Time Compute\\nAMD MI300X\\nAMD MI300X\\nAMD MI300X\\nAMD MI300X\\nAMD MI300X\\nNVIDIA H200\\nNVIDIA H200\\nNVIDIA H100\\nNVIDIA H200\\nNVIDIA H200\\nNVIDIA H100\\nNVIDIA H100\\nNVIDIA H200\\nNVIDIA H100\\nNVIDIA H100\\nNVIDIA A100\\nNVIDIA A100\\nNVIDIA A100\\nNVIDIA A100\\nFP8 with LLMs\\nFP8 with LLMs\\nFP8 with LLMs\\nFP8 with LLMs\\nFP8 with LLMs\\nGGUF Models\\nSpeculative Decoding\\nSpeculative Decoding\\nGGUF Models\\nGGUF Models\\nGGUF Models\\nGGUF Models\\nPrefix Caching\\nPrefix Caching\\nPrefix Caching\\nPrefix Caching\\nPrefix Caching\\nSpeculative Decoding\\nSpeculative Decoding\\nSpeculative Decoding\\nPrometheus & Grafana\\nPrometheus & Grafana\\nPrometheus & Grafana\\nPrometheus & Grafana\\nPrometheus & Grafana\\nText Embedding\\nText Embedding\\nText Embedding\\nText Embedding\\nText Embedding\\nOffline Batch Inference\\nOffline Batch Inference\\nOffline Batch Inference\\nOffline Batch Inference\\nEmbedding Models\\nOffline Batch Inference\\nEmbedding Models\\nEmbedding Models\\nEmbedding Models\\nEmbedding Models\\nLLM Serving\\nLLM Serving\\nLLM Serving\\nLLM Serving\\nLLM Serving\\nFunction Calling\\nFunction Calling\\nFunction Calling\\nFunction Calling\\nStructured JSON\\nStructured JSON\\nStructured JSON\\nFunction Calling\\nStructured JSON\\nStructured JSON\\nKV Cache\\nML Systems\\nModels\\nKV Cache\\nKV Cache\\nKV Cache\\nModels\", 'image_urls': [], 'title': \"Exploring DeepSeek-R1's Mixture-of-Experts Model Architecture - AI Resources\"}, {'url': 'https://www.analyticsvidhya.com/blog/2025/02/llm-optimization/', 'raw_content': 'A Deep Dive into LLM Optimization: From Policy Gradient to GRPO\\nMaster Generative AI with 10+ Real-world Projects in 2025!\\nDownload Projects\\nInterview Prep\\nCareer\\nGenAI\\nPrompt Engg\\nChatGPT\\nLLM\\nLangchain\\nRAG\\nAI Agents\\nMachine Learning\\nDeep Learning\\nGenAI Tools\\nLLMOps\\nPython\\nNLP\\nSQL\\nAIML Projects\\nFrom RL to LLMs: Optimizing AI with GRPO, PPO, and DPO for Better Fine-Tuning\\nNeil D\\nLast Updated : 18 Feb, 2025\\n22\\nmin read\\nFor decades,\\xa0Reinforcement Learning (RL)\\xa0has been the driving force behind breakthroughs in\\xa0robotics, game-playing AI (AlphaGo, OpenAI Five), and control systems. RL’s strength lies in its ability to optimize decision-making by\\xa0maximizing long-term rewards, making it ideal for problems requiring sequential reasoning. However,\\xa0large language models (LLMs)\\xa0initially relied on\\xa0supervised learning, where models were fine-tuned on static datasets. This approach lacked adaptability—while LLMs could mimic human text, they struggled with nuanced human preference alignment, leading to inconsistencies in conversational AI. The introduction of\\xa0RLHF (Reinforcement Learning with Human Feedback)\\xa0changed everything. By integrating RL into LLM fine-tuning, models like\\xa0ChatGPT, DeepSeek, Gemini, and Claude\\xa0could optimize (LLM Optimization) their responses based on user feedback.\\nHowever, standard\\xa0PPO-based RLHF\\xa0had inefficiencies, requiring expensive reward modeling and iterative training. Enter\\xa0DeepSeek’s Group Relative Policy Optimization (GRPO)—a breakthrough that\\xa0eliminated the need for explicit reward modeling\\xa0by directly optimizing\\xa0preference rankings. To fully grasp the significance of GRPO, we must first explore the\\xa0fundamental policy optimization techniques (LLM optimization)\\xa0that power modern reinforcement learning.\\nSource:\\nLink\\nLearning Objectives\\nUnderstand why RL-based techniques are crucial for optimizing LLMs like ChatGPT, DeepSeek, Claude, and Gemini.\\nLearn the fundamentals of policy optimization, including PG, TRPO, and PPO.Explore DPO and GRPO for preference-based LLM training without explicit reward models.\\nCompare PG, TRPO, PPO, DPO, and GRPO to determine the best approach for RL and LLM fine-tuning.\\nGain hands-on experience with Python implementations of policy optimization algorithms.\\nEvaluate fine-tuning impact using training loss curves and probability distributions.\\nApply DPO and GRPO to enhance LLM safety, alignment, and reliability.\\nThis article was published as a part of the\\nData Science Blogathon.\\nTable of contents\\nPrimer on Policy Optimization Techniques\\nMathematical Foundations (Required for All Methods)\\nPolicy Gradient (PG) – The Foundation\\nThe Policy Gradient Theorem\\nCode Example: REINFORCE Algorithm\\nTrust Region Policy Optimization (TRPO)\\nTRPO Algorithm & Key Mathematical Concepts\\nCode Example: TRPO Training Loop\\nProximal Policy Optimization (PPO)\\nPPO Algorithm & Key Mathematical Concept\\nCode Example: PPO Training Loop\\nDirect Preference Optimization (DPO) – Preference Learning for LLMs\\nCode Example: Direct Preference Optimization (DPO)\\nGRPO – Group Relative Policy Optimization (DeepSeek’s Approach)\\nMathematical Foundation of GRPO\\nData for GRPO Fine-Tuning\\nCode Implementation: Group-Based Preference Optimization\\nTraining Loop for GRPO\\nExpected Outcome and Results\\nFinal Model Insights: Why GRPO Excels in LLM Fine-Tuning\\nConclusion\\nFrequently Asked Questions\\nPrimer on Policy Optimization Techniques\\nBefore diving into DeepSeek’s GRPO, it’s crucial to understand the policy optimization techniques that form the foundation of reinforcement learning (RL) in both traditional control tasks and\\nLLM fine-tuning\\n. Policy optimization refers to the process of improving an AI agent’s decision-making strategy (policy) to maximize expected rewards. While early methods like vanilla\\npolicy gradient (PG)\\nlaid the groundwork, more sophisticated techniques such as TRPO, PPO, DPO, and GRPO evolved to address issues like stability, efficiency, and preference alignment.\\nWhat is Policy Optimization?\\nAt its core, policy optimization is about learning the optimal policy\\nπ_θ(a∣s)\\n, which maps a state\\ns\\nto an action\\na\\nwhile maximizing long-term rewards. The objective function in RL is typically formulated as:\\nWhere\\nR(τ)\\nis the total reward collected in a trajectory τ, and the expectation is taken over all possible trajectories following policy\\nπ_θ\\n.\\nThere are three major approaches to policy optimization:\\n1.\\xa0Gradient-Based Optimization (Policy Gradient Methods)\\nThese methods directly compute gradients of expected reward and update policy parameters using gradient ascent.\\nExample: REINFORCE algorithm (Vanilla Policy Gradient).\\nPros: Simple, works with continuous and discrete actions.\\nCons: High variance, requires tricks like baseline subtraction.\\n2.\\xa0Trust-Region Optimization (TRPO, PPO)\\nIntroduces constraints (KL divergence) to ensure policy updates are stable and not too drastic.\\nExample: TRPO ensures updates stay within a “trust region”; PPO simplifies this with clipping.\\nPros: More stable than raw policy gradients.\\nCons: Computationally expensive (TRPO), hyperparameter-sensitive (PPO).\\n3.\\xa0Preference-Based Optimization (DPO, GRPO)\\nOptimizes directly from ranked human preferences instead of rewards.\\nExample: DPO learns from preferred vs. rejected responses; GRPO generalizes to groups.\\nPros: Eliminates the need for reward models and better aligns LLMs with human intent.\\nCons: Requires high-quality preference data.\\nMathematical Foundations (Required for All Methods)\\nA. Markov Decision Process (MDP)\\nRL is typically formulated as a\\nMarkov Decision Process\\n(MDP), represented as:\\nwhere:\\nS\\nis the state space,\\nA\\nis the action space,\\nP(s′∣s,a)\\nis the transition probability to state s′,\\nR(s,a)\\nis the reward function,\\nγ\\nis the discount factor (how much future rewards are valued).\\nB. Expected Return J(θ)\\nThe\\nExpected Return\\n(ER) measures how much cumulative reward we expect from following policy\\nπ_θ\\n:\\nwhere\\nγ\\n(0 ≤\\nγ\\n≤ 1) determines how much future rewards contribute.\\nC. Policy Gradient Theorem\\nPolicy gradient (PG) methods update the policy using gradients of expected rewards. The key equation:\\nwhere:\\nA(s,a)\\nis the advantage function (how good action\\na\\nis compared to average actions in state\\ns\\n).\\nlogπ_θ\\u200b\\nensures we increase the probabilities of better actions.\\nD. Advantage Function A(s,a)\\nTo reduce variance in gradient estimates, we use the advantage function:\\nwhere:\\nQ(s,a)\\nis the expected return for taking action\\na\\nat state\\ns\\n.\\nV(s)\\nis the expected return following policy\\nπ\\nfrom\\ns\\n.\\nUsing\\nA(s,a)\\nhelps make updates more stable and efficient.\\nPolicy Gradient (PG) – The Foundation\\nThe Policy Gradient (PG) method is the most fundamental approach to reinforcement learning. Instead of learning a value function, PG directly parameterizes the policy\\nπ_θ(a∣s)\\nand updates it using gradient ascent. This allows learning in continuous action spaces, making it effective for tasks like robotics, game AI, and LLM fine-tuning.\\nHowever, PG methods suffer from high variance due to their reliance on sampling full trajectories. More advanced methods like TRPO, PPO, and GRPO build upon PG to improve stability.\\nThe Policy Gradient Theorem\\nThe goal of policy optimization is to find policy parameters\\nθ\\nthat maximize expected return:\\nUsing the log-derivative trick, we obtain the Policy Gradient Theorem:\\nwhere:\\n∇θ\\u200blogπθ\\u200b(a∣s)\\nis the gradient of the log-probability of taking action aaa.\\nA(s,a)\\n(Advantage function) determines how much better action aaa is compared to others.\\nWe perform gradient ascent to increase the probability of good actions.\\nCode Example: REINFORCE Algorithm\\nThe REINFORCE algorithm is the simplest form of PG. It samples trajectories, computes rewards, and updates the policy parameters. Below is the main training loop (only the key function is shown to limit the scope; the full notebook is\\nlinked\\n).\\ndef train_policy_gradient(env, policy, optimizer, num_episodes=500, gamma=0.99): \"\"\"Train a policy using the REINFORCE algorithm\"\"\" reward_history = [] for episode in range(num_episodes): state, _ = env.reset() log_probs = [] rewards = [] done = False while not done: state = torch.FloatTensor(state).unsqueeze(0) action_probs = policy(state) action_dist = torch.distributions.Categorical(action_probs) action = action_dist.sample() log_probs.append(action_dist.log_prob(action)) next_state, reward, done, _, _ = env.step(action.item()) rewards.append(reward) state = next_state # Compute discounted rewards returns = [] G = 0 for r in reversed(rewards): G = r + gamma * G returns.insert(0, G) returns = torch.tensor(returns) returns = (returns - returns.mean()) / (returns.std() + 1e-9) # Normalize for stability # Compute policy gradient loss loss = [] for log_prob, G in zip(log_probs, returns): loss.append(-log_prob * G) # Gradient ascent on expected return loss = torch.stack(loss).sum() # Optimize policy optimizer.zero_grad() loss.backward() optimizer.step() reward_history.append(sum(rewards)) return reward_history\\n🔗\\nFull implementation available here\\nCode Explanation\\nThe\\ntrain_policy_gradient\\nfunction implements the\\nREINFORCE\\nalgorithm, which optimizes policy parameters using Monte Carlo updates. The training begins by initializing the environment and iterating over multiple episodes, collecting state-action-reward trajectories. For each step in an episode, an action is sampled from the policy, executed in the environment, and its corresponding reward is stored. After completing an episode, the discounted rewards are computed using the\\ncompute_discounted_rewards\\nfunction, ensuring that future rewards contribute appropriately to policy updates. These rewards are then normalized to reduce variance, making training more stable. The policy loss is calculated by multiplying the log probabilities of actions by their respective discounted rewards. Finally, the policy is updated using gradient descent, which maximizes the expected return by reinforcing actions that led to higher rewards.\\nExpected Outcomes & Justification\\nThe training plot demonstrates how the total episode rewards evolve over 500 episodes. Initially, the agent performs poorly, as seen in the low reward values in early episodes (e.g., Episode 50: 20.0). However, as training progresses, the agent learns more effective strategies, leading to higher rewards (Episode 100: 134.0, Episode 150: 229.0). The performance peaks when the agent successfully balances the pole for the maximum time, reaching 500 rewards per episode (Episode 200, 350, and 450). However, instability is evident, as seen in the sharp reward drop in Episode 250 (26.0) and Episode 500 (9.0). This behaviour arises due to the high variance of PG methods, where updates can occasionally lead to suboptimal policies before stabilizing.\\nThe overall trend shows increasing average rewards, indicating that the policy is improving. However, fluctuations in rewards highlight the limitation of vanilla PG methods, which motivates the need for more stable techniques like TRPO and PPO.\\nTrust Region Policy Optimization (TRPO)\\nWhile Policy Gradient (PG) methods like REINFORCE are effective, they suffer from high variance and instability in updates. One bad update can drastically collapse the learned policy. TRPO (Trust Region Policy Optimization) improves upon PG by ensuring updates are constrained within a trust region, preventing abrupt changes that could harm performance.\\nInstead of using vanilla gradient descent, TRPO solves a constrained optimization problem:\\nThis KL-divergence constraint ensures that the new policy is not too far from the previous policy, leading to more stable updates.\\nTRPO Algorithm & Key Mathematical Concepts\\nTRPO optimizes the policy using Generalized Advantage Estimation (GAE) and Conjugate Gradient Descent.\\n1. Generalized Advantage Estimation (GAE):\\nComputes an advantage function to estimate how much better an action is compared to the expected return.\\nwhere\\nδ_t\\nis the TD error:\\n2.\\xa0Trust Region Constraint:\\nEnsures updates stay within a safe region using KL-divergence.\\nwhere\\nδ\\nis the maximum step size.\\n3. Conjugate Gradient Optimization:\\nInstead of directly computing the inverse Hessian, TRPO uses a conjugate gradient to find the optimal update direction efficiently.\\nCode Example: TRPO Training Loop\\nBelow is the main TRPO training function, where we apply trust region updates and compute the discounted rewards and advantages. (Only the key function is shown; the full notebook\\nlink\\n.)\\ndef train_trpo(env, policy, num_episodes=500, gamma=0.99): reward_history = [] for episode in range(num_episodes): state = env.reset() if isinstance(state, tuple): state = state[0] # Handle Gym versions that return (state, info) log_probs = [] states = [] actions = [] rewards = [] done = False while not done: state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0) probs = policy(state_tensor) action_dist = torch.distributions.Categorical(probs) action = action_dist.sample() step_result = env.step(action.item()) if len(step_result) == 5: next_state, reward, terminated, truncated, _ = step_result done = terminated or truncated # New Gym API else: next_state, reward, done, _ = step_result # Old Gym API log_probs.append(action_dist.log_prob(action)) states.append(state_tensor) actions.append(action) rewards.append(reward) state = next_state # Compute discounted rewards and advantages discounted_rewards = compute_discounted_rewards(rewards, gamma) discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9) # Convert lists to tensors states = torch.cat(states) actions = torch.tensor(actions) advantages = discounted_rewards # Copy old policy before updating old_policy = PolicyNetwork(env.observation_space.shape[0], env.action_space.n) old_policy.load_state_dict(policy.state_dict()) # Apply TRPO update trpo_step(policy, old_policy, states, actions, advantages) total_episode_reward = sum(rewards) reward_history.append(total_episode_reward) if (episode + 1) % 50 == 0: print(f\"Episode {episode+1}, Total Reward: {total_episode_reward}\") return reward_history\\n🔗\\nFull implementation available\\xa0here\\nCode Explanation\\nThe\\ntrain_trpo\\nfunction implements the Trust Region Policy Optimization update. The training loop initializes the environment and runs 500 episodes, collecting states, actions, and rewards for each step. The key difference from Policy Gradient (PG) is that TRPO maintains an old policy copy and updates the new policy while ensuring the update remains within a KL-divergence bound.\\nThe advantages are computed using discounted rewards and normalized to reduce variance. Finally, conjugate gradient descent is used to determine the optimal policy step direction. Unlike standard gradient updates, TRPO restricts step size to prevent drastic policy changes, leading to more stable performance.\\nExpected Outcomes & Justification\\nThe training curve for TRPO exhibits significant reward fluctuations, and the numerical results indicate that the policy does not consistently improve over time as shown below.\\nUnlike Policy Gradient (PG), which showed steady learning progress, TRPO struggles to maintain consistent improvements. Despite its theoretical advantages (trust region constraint preventing catastrophic updates), the actual results show high instability. The total rewards oscillate between low values (9-20), indicating that the agent fails to learn an optimal strategy efficiently.\\nThis is a known issue with TRPO—it requires careful tuning of KL divergence constraints, and in many cases, the update process is computationally expensive and prone to suboptimal convergence. The reward fluctuations suggest that the agent isn’t exploiting learned knowledge effectively, reinforcing the need for a more practical and robust policy optimization method.\\xa0PPO simplifies TRPO by approximating the trust region constraint using a clipped objective function, leading to faster and more efficient training.\\nProximal Policy Optimization (PPO)\\nTRPO ensures stable policy updates but is computationally expensive due to solving a constrained optimization problem at each step. PPO (Proximal Policy Optimization) simplifies this process by using a clipped objective function to restrict updates without requiring second-order optimization.\\nInstead of solving:\\nPPO modifies the objective function by introducing a clipped surrogate loss:\\nwhere:\\nr_t\\u200b(θ\\n)\\nis the probability ratio between new and old policies.\\nA_\\nt\\u200b\\nis the advantage estimate.\\nϵ\\nis a small constant (e.g., 0.2) that limits excessive policy updates.\\nThis prevents overshooting updates, making PPO more computationally efficient while retaining TRPO’s stability.\\nPPO Algorithm & Key Mathematical Concept\\n1. Advantage Estimation using GAE:\\nPPO improves TRPO by using Generalized Advantage Estimation (GAE) to compute stable gradients:\\nwhere\\nδ_t\\n=\\nr_t\\n+\\nγV(s_(t+1))\\n−\\nV(s_t)\\n.\\n2.\\nClipped Objective Function:\\nUnlike TRPO, which enforces a strict KL constraint, PPO approximates the constraint using clipping:\\nThis ensures that the update does not move too far, preventing policy collapse.\\n3.\\nMini-Batch Training:\\nInstead of updating the policy after each episode, PPO trains using mini-batches over multiple epochs, improving sample efficiency.\\nCode Example: PPO Training Loop\\nBelow is the main PPO training function, where we compute advantages, apply clipped policy updates, and use mini-batches for stable learning. (Only the key function is shown; full notebook\\nlink\\n.)\\ndef train_ppo(env, policy, optimizer, num_episodes=500, gamma=0.99, lambda_=0.95, epsilon=0.2, batch_size=32, epochs=5): reward_history = [] for episode in range(num_episodes): state = env.reset() if isinstance(state, tuple): state = state[0] # Handle Gym versions returning (state, info) log_probs = [] values = [] states = [] actions = [] rewards = [] done = False while not done: state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0) probs = policy(state_tensor) action_dist = torch.distributions.Categorical(probs) action = action_dist.sample() step_result = env.step(action.item()) # Handle different Gym API versions if len(step_result) == 5: next_state, reward, terminated, truncated, _ = step_result done = terminated or truncated # New API else: next_state, reward, done, _ = step_result # Old API log_probs.append(action_dist.log_prob(action)) states.append(state_tensor) actions.append(action) rewards.append(reward) state = next_state # Compute advantages values = [0] * len(rewards) # Placeholder for value estimates (since we use policy-only PPO) advantages = compute_advantages(rewards, values, gamma, lambda_) advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-9) # Normalize advantages # Convert lists to tensors states = torch.cat(states) actions = torch.tensor(actions) old_log_probs = torch.tensor(log_probs) # PPO Training Loop for _ in range(epochs): for i in range(0, len(states), batch_size): batch_indices = slice(i, i + batch_size) new_probs = policy(states[batch_indices]) new_action_dist = torch.distributions.Categorical(new_probs) new_log_probs = new_action_dist.log_prob(actions[batch_indices]) loss = ppo_loss(old_log_probs[batch_indices], new_log_probs, advantages[batch_indices], epsilon) optimizer.zero_grad() loss.backward() optimizer.step() total_episode_reward = sum(rewards) reward_history.append(total_episode_reward) if (episode + 1) % 50 == 0: print(f\"Episode {episode+1}, Total Reward: {total_episode_reward}\") return reward_history\\n🔗\\nFull implementation available here\\nCode Explanation\\nThe\\ntrain_ppo\\nfunction implements Proximal Policy Optimization (PPO) using a clipped surrogate loss and mini-batch updates. Unlike TRPO, which computes trust region constraints, PPO approximates them by clipping policy updates, making it much more efficient.\\nThe function begins by collecting episode trajectories (states, actions, log probabilities, and rewards).\\nAdvantage estimation is computed using Generalized Advantage Estimation (GAE).\\nMini-batches are used to update the policy over multiple epochs, improving sample efficiency.\\nInstead of a strict KL divergence constraint, PPO applies a clipped loss function to prevent destructive updates.\\nExpected Outcomes for PPO\\nThe PPO training curve and numerical results show a clear improvement in policy learning over time:\\nKey Observations:\\nStable Improvement:\\nThe early rewards (Ep 50-100) are low, indicating the agent is still exploring.\\nSteady Progress:\\nBy Episode 200, the total reward surpasses 200, showing the agent is learning a structured policy.\\nFluctuations Exist, But Recovery is Fast:\\nBetween Ep 300-400, rewards drop, but PPO stabilizes and quickly rebounds to peak performance (500).\\nFinal Convergence:\\nThe model reaches 500 rewards (max score) by Ep 500, confirming PPO effectively learns an optimal strategy.\\nCompared to TRPO, PPO exhibits:\\nLess noisy training\\nFaster convergence\\nMore efficient sample utilization: These improvements validate PPO’s clipped updates and mini-batch training as a superior approach to policy learning.\\nPPO is excellent for reward-based learning, but it struggles with preference-based fine-tuning in applications like LLMs (e.g., ChatGPT, DeepSeek, Claude, Gemini). DPO (Direct Preference Optimization) improves upon PPO by directly learning from human preference data instead of optimizing pure rewards.\\nDirect Preference Optimization (DPO) – Preference Learning for LLMs\\nTraditional reinforcement learning (RL) techniques are designed to optimize numerical reward-based objectives. However, Large Language Models (LLMs) like ChatGPT, DeepSeek, Claude, and Gemini require fine-tuning that aligns with human preferences rather than just maximizing a reward function. This is where Direct Preference Optimization (DPO) plays a crucial role. Unlike RL-based methods like PPO, which rely on an explicitly trained reward model, DPO optimizes models directly using human feedback. By leveraging preference pairs (where one response is preferred over another), DPO enables models to learn human-like responses efficiently.\\nDPO eliminates the need for a separate reward model, making it a simpler and more data-driven approach compared to Reinforcement Learning from Human Feedback (RLHF). Instead of reward-based fine-tuning, DPO updates the model parameters to increase the probability of preferred responses while decreasing the probability of rejected responses. This makes the training process more stable and avoids the complexities of RL algorithms like PPO, which involve constrained policy updates and KL penalties.\\nThe significance of DPO lies in its ability to fine-tune LLMs in a way that ensures better response alignment with human expectations. By removing explicit reward models, it prevents the instability often associated with RL-based fine-tuning. Moreover, DPO reduces the risk of harmful, misleading, or biased outputs, making LLMs safer and more reliable. This streamlined optimization process makes it a practical alternative to RL-based fine-tuning, especially when human preference data is available at scale.\\nThe DPO Training Dataset\\nFor DPO, we use human preference data, where each prompt has a preferred response and a rejected response.\\nExample Preference Dataset (Used for Fine-Tuning)\\npreference_data = [ {\"prompt\": \"What is the capital of France?\", \"preferred\": \"The capital of France is Paris.\", \"rejected\": \"France is a country in Europe.\"}, {\"prompt\": \"Who wrote Hamlet?\", \"preferred\": \"Hamlet was written by William Shakespeare.\", \"rejected\": \"Hamlet is an old book.\"}, {\"prompt\": \"Tell me a joke.\", \"preferred\": \"Why did the scarecrow win an award? Because he was outstanding in his field!\", \"rejected\": \"I don’t know any jokes.\"}, {\"prompt\": \"What is artificial intelligence?\", \"preferred\": \"Artificial intelligence is the simulation of human intelligence in machines.\", \"rejected\": \"AI is just robots.\"}, {\"prompt\": \"How to stay motivated?\", \"preferred\": \"Set clear goals, track progress, and reward yourself for achievements.\", \"rejected\": \"Just be motivated.\"},\\n]\\nThe preferred responses are accurate, informative, and well-structured, while the rejected responses are vague, incorrect, or unhelpful.\\nThe DPO Loss Function\\nDPO is formulated as a pairwise ranking problem between a preferred response and a rejected response for the same prompt. The goal is to increase the log probability of preferred responses while decreasing the probability of rejected ones.\\nMathematically, the DPO objective is:\\nWhere:\\ny^+\\nis the preferred response\\ny^-\\nis the rejected response\\nβ\\nis a scaling hyperparameter controlling preference strength\\nP_θ(y∣x)\\nis the log probability of generating a response given input\\nx\\nThis is similar to logistic regression, where the model maximizes separation between preferred and rejected responses.\\nCode Example: Direct Preference Optimization (DPO)\\nDPO fine-tunes LLMs by training on human-labeled preference pairs. The core logic of DPO training involves optimizing model weights based on preferred vs. rejected responses. The function below trains a transformer-based model to increase the likelihood of preferred responses while decreasing the likelihood of rejected ones. Below is the key function for computing the DPO loss and updating the model (only the main function is shown for scope; full notebook is\\nlinked\\n).\\ndef dpo_loss(preferred_log_probs, rejected_log_probs, beta=0.1): \"\"\"Computes the DPO loss function to optimize based on preferences\"\"\" return -torch.mean(torch.sigmoid(beta * (preferred_log_probs - rejected_log_probs))) def encode_text(prompt, response): \"\"\"Encodes the prompt + response into tokenized format with proper padding\"\"\" tokenizer.pad_token = tokenizer.eos_token # Fix padding issue input_text = f\"User: {prompt}\\\\nAssistant: {response}\" inputs = tokenizer( input_text, return_tensors=\"pt\", padding=True, # Enable padding truncation=True, # Truncate if too long max_length=512 # Set max length for safety ) return inputs[\"input_ids\"], inputs[\"attention_mask\"] loss_history = [] # Store loss values optimizer = optim.AdamW(model.parameters(), lr=5e-5) for epoch in range(10): # Train for 10 epochs total_loss = 0 for data in preference_data: prompt, preferred, rejected = data[\"prompt\"], data[\"preferred\"], data[\"rejected\"] # Encode preferred and rejected responses pref_input_ids, pref_attention_mask = encode_text(prompt, preferred) rej_input_ids, rej_attention_mask = encode_text(prompt, rejected) # Get log probabilities from the model preferred_logits = model(pref_input_ids, attention_mask= pref_attention_mask).logits[:, -1, :] rejected_logits = model(rej_input_ids, attention_mask=rej_attention_mask) .logits[:, -1, :] preferred_log_probs = preferred_logits.log_softmax(dim=-1) rejected_log_probs = rejected_logits.log_softmax(dim=-1) # Compute DPO loss loss = dpo_loss(preferred_log_probs, rejected_log_probs, beta=0.5) # Optimize the model optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss.item() loss_history.append(total_loss) # Store loss for visualization print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\\n🔗\\nFull implementation available here\\nExpected Output & Analysis\\nThe outcomes of Direct Preference Optimization (DPO) can be analyzed from multiple angles: loss convergence, probability shifts, and qualitative response improvements. The training loss curve shows a sharp drop in the initial epochs, followed by stabilization, indicating that the model quickly learns to align with human preferences. The plateau in loss suggests that further optimization yields diminishing improvements, confirming effective preference-based fine-tuning.\\nThe probability shift visualization reveals that preferred responses consistently achieve higher log probabilities than rejected ones. This confirms that DPO successfully adjusts the model’s behaviour, reinforcing the correct responses while suppressing undesired ones. Some variance in probability shifts suggests that certain prompts may still require fine-tuning for optimal alignment.\\nA direct comparison of model responses before and after DPO fine-tuning highlights clear improvements. Initially, the model fails to generate a joke, instead providing an irrelevant response. After fine-tuning, it attempts humor but still lacks coherence. This demonstrates that while DPO enhances preference alignment, additional refinements or complementary techniques may be required to generate high-quality, structured responses.\\nAlthough DPO effectively tunes LLMs without an explicit reward function, it lacks the structured policy learning of reinforcement learning-based methods. This is where General Reinforcement Pretraining Optimization (GRPO) by DeepSeek comes in, combining the strengths of DPO and PPO to enhance LLM fine-tuning further. The next section will explore how GRPO refines policy optimization for large-scale models.\\nGRPO – Group Relative Policy Optimization (DeepSeek’s Approach)\\nDeepSeek’s Group Relative Policy Optimization (GRPO) is an advanced preference optimization technique that extends Direct Preference Optimization (DPO) while incorporating elements from Proximal Policy Optimization (PPO). Unlike traditional policy optimization methods that operate on single preference pairs, GRPO leverages group-wise preference ranking, enabling better alignment with human feedback in large-scale LLM fine-tuning.\\nTraditional preference-based optimization methods, such as DPO (Direct Preference Optimization), operate on pairwise comparisons—one preferred and one rejected response. However, this approach fails to scale efficiently when optimizing on large datasets where multiple responses per prompt are ranked in order of preference. To address this limitation, DeepSeek introduced Group Relative Policy Optimization (GRPO), which allows group-based preference ranking rather than just single-pair preference updates. Instead of comparing two responses at a time, GRPO compares all ranked responses within a batch and optimizes the policy accordingly.\\nMathematically, GRPO extends DPO’s reward-free optimization by defining an ordered preference ranking among multiple completions and optimizing their relative likelihoods accordingly.\\nMathematical Foundation of GRPO\\nSince this is the main intent behind the blog, we will dive deep into the mathematics of this.\\n1. Expected Return in Preference Optimization\\nIn standard reinforcement learning, the expected return of a policy\\nπ_θ\\nis:\\nwhere\\nR(s_t,a_t)\\nis the reward at timestep\\nt\\n.\\nHowever, LLM fine-tuning does not operate in traditional reward-based RL. Instead, we optimize over human preferences, meaning that reward models are unnecessary.\\nInstead of learning a reward function, GRPO directly optimizes the model parameters to increase the likelihood of higher-ranked responses over lower-ranked ones.\\n2. Ranking-Based Probability Optimization\\nGiven a set of responses\\nr_1,r_2, …, r_n\\nranked in order of preference, we define a likelihood ratio:\\nwhere\\nx\\nis the input prompt, and\\nπ_θ\\nrepresents the policy (LLM) parameterized by\\nθ\\n. The key objective is to maximize the probability of higher-ranked responses while suppressing the probability of lower-ranked ones.\\nTo enforce relative preference constraints, GRPO optimizes the following pairwise ranking loss across all response pairs:\\nwhere:\\nσ(x)\\nis the sigmoid function ensuring probability normalization\\nβ\\nis a temperature scaling parameter controlling gradient magnitude.\\nπ_\\nθ\\u200b\\nis the policy (LLM).\\nThe sum iterates over all pairs (i, j) where\\nr_i\\n, is ranked higher than\\nr_j\\n.\\nThe KL-regularized version of GRPO adds a penalty term to prevent drastic shifts in model behaviour:\\nwhere\\nD_KL\\n\\u200b ensures conservative updates to prevent overfitting.\\nData for GRPO Fine-Tuning\\nBelow is an example dataset used to fine-tune an LLM using ranked preferences:\\ngrpo_preference_data = [ {\"prompt\": \"What is the capital of France?\", \"responses\": [ {\"text\": \"The capital of France is Paris.\", \"rank\": 1}, {\"text\": \"Paris is the largest city in France.\", \"rank\": 2}, {\"text\": \"Paris is in France.\", \"rank\": 3}, {\"text\": \"France is a country in Europe.\", \"rank\": 4} ]}, {\"prompt\": \"Tell me a joke.\", \"responses\": [ {\"text\": \"Why did the scarecrow win an award? Because he was outstanding in his field!\", \"rank\": 1}, {\"text\": \"Why did the chicken cross the road? To get to the other side.\", \"rank\": 2}, {\"text\": \"Jokes are funny.\", \"rank\": 3}, {\"text\": \"I don’t know any jokes.\", \"rank\": 4} ]}\\n]\\nEach prompt has multiple responses with assigned ranks. The model learns to increase the probability of higher-ranked responses while reducing the probability of lower-ranked ones.\\nCode Implementation: Group-Based Preference Optimization\\nBelow is the key function for computing the DPO loss and updating the model (only the main function is shown for scope; the full notebook is\\nlinked\\n). The GRPO training function processes multiple ranked responses per prompt, optimizing log-likelihood differences while enforcing KL constraints.\\ndef deepseek_grpo_loss(log_probs, rankings, input_ids, beta=1.0, kl_penalty=0.02, epsilon=1e-6): \"\"\"Computes DeepSeek GRPO loss with pairwise ranking and KL regularization.\"\"\" loss_terms = [] num_pairs = 0 log_probs = torch.clamp(log_probs, min=-10, max=10) # Prevent extreme values for i in range(len(rankings)): for j in range(i + 1, len(rankings)): if rankings[i] < rankings[j]: # Higher-ranked response should be preferred prob_diff = log_probs[i] - log_probs[j] pairwise_loss = -torch.log(torch.sigmoid(beta * prob_diff) + epsilon) # Avoid log(0) loss_terms.append(pairwise_loss) num_pairs += 1 loss = torch.stack(loss_terms).mean() if num_pairs > 0 else torch.tensor(0.0, device=log_probs.device) # KL regularization to prevent policy divergence old_logits = base_model(input_ids).logits[:, -1, :] old_log_probs = old_logits.log_softmax(dim=-1) kl_div = torch.nn.functional.kl_div(log_probs, old_log_probs.clamp(min=epsilon), reduction=\"batchmean\") return loss + (kl_penalty * kl_div.mean()) # Ensure single scalar\\nTraining Loop for GRPO\\nThe training loop processes ranked responses, computes loss, and updates the model while enforcing stability constraints.\\nloss_history = []\\nnum_epochs = 15 for epoch in range(num_epochs): total_loss = 0 for data in grpo_preference_data: prompt, responses = data[\"prompt\"], data[\"responses\"] input_ids, rankings = encode_text(prompt, responses) logits = model(input_ids).logits[:, -1, :] log_probs = logits.log_softmax(dim=-1) loss = deepseek_grpo_loss(log_probs, rankings, input_ids) if torch.isnan(loss): print(f\"Skipping update at epoch {epoch} due to NaN loss.\") continue optimizer.zero_grad() loss.backward() torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) optimizer.step() total_loss += loss.item() loss_history.append(total_loss) scheduler.step() print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\\n🔗\\nFull implementation available here\\nExpected Outcome and Results\\nThe expected outcomes of GRPO fine-tuning on the LLM, based on the provided outputs, highlight improvements in model optimization and preference-based ranking.\\nThe training loss curve shows a gradual and stable decline over 15 epochs, indicating that the model is learning effectively. Unlike conventional policy optimization methods, GRPO ensures that ranked responses improve without drastic fluctuations, suggesting smooth convergence.\\nThe loss value distribution over epochs presents a histogram where most values concentrate around a decreasing trend, showing that GRPO efficiently optimizes the model while maintaining stable loss updates. This distribution further indicates that loss values do not exhibit large variations, preventing instability in preference ranking.\\nThe log probability distribution before vs. after fine-tuning provides crucial insights into the model’s response generation. The shift in probability distribution suggests that after fine-tuning, the model assigns higher confidence to preferred responses. This shift results in responses that align better with human expectations and rankings.\\nOverall, the expected outcome of GRPO fine-tuning is a well-optimized model capable of generating high-quality responses ranked effectively based on preference learning. This demonstrates why GRPO is an effective alternative to traditional RL methods like PPO or DPO, offering a structured approach to optimizing LLMs without explicit reward models.\\nFinal Model Insights: Why GRPO Excels in LLM Fine-Tuning\\nUnlike pairwise DPO and trust-region PPO, GRPO allows LLMs to learn from multiple ranked completions per prompt, significantly improving response quality, stability, and human alignment.\\nMore scalable than pairwise methods → Learns from multiple ranked completions rather than just binary comparisons.\\nNo explicit reward modeling → Unlike RLHF, GRPO fine-tunes without requiring a trained reward model.\\nKL regularization stabilizes updates → Prevents catastrophic shifts in response distribution.\\nBetter generalization across prompts → Ensures the LLM produces high-quality, human-aligned responses.\\nWith reinforcement learning playing an increasingly central role in fine-tuning LLMs, GRPO stands out as the next step in AI preference learning, setting a new standard for human-aligned language modeling.\\nConclusion\\nPolicy optimization techniques play a critical role in reinforcement learning and LLM fine-tuning. Each method—Policy Gradient (PG), Trust Region Policy Optimization (TRPO), Proximal Policy Optimization (PPO), Direct Preference Optimization (DPO), and Group Relative Policy Optimization (GRPO)—offers unique advantages and trade-offs. PG serves as the foundation but suffers from high variance, while TRPO provides stability at the cost of computational complexity. PPO, being a refined version of TRPO, balances efficiency and robustness, making it widely used in RL applications. DPO, on the other hand, optimizes LLMs directly using preference data, eliminating the need for a reward model. Finally, GRPO, as introduced by DeepSeek, enhances preference-based fine-tuning by leveraging relative ranking in a structured manner.\\nBelow is a comparison of these LLM Optimization methods based on key aspects such as variance, stability, sample efficiency, and suitability for reinforcement learning versus LLM fine-tuning:\\nMethod\\nVariance\\nStability\\nSample Efficiency\\nBest for\\nLimitations\\nPG (REINFORCE)\\nHigh\\nLow\\nInefficient\\nSimple RL problems\\nHigh variance, slow convergence\\nTRPO\\nLow\\nHigh\\nModerate\\nHigh-stability RL tasks\\nComplex second-order updates, expensive\\nPPO\\nMedium\\nHigh\\nEfficient\\nGeneral RL tasks, Robotics, Games\\nMay require careful hyperparameter tuning\\nDPO\\nLow\\nHigh\\nHigh\\nLLM fine-tuning with human preferences\\nLacks explicit reinforcement learning framework\\nGRPO\\nLow\\nHigh\\nHigh\\nPreference-based LLM fine-tuning\\nNewer method, requires further empirical validation\\nFor practitioners, the choice depends on the task at hand. If optimizing reinforcement learning agents in games or robotics, PPO is the best choice due to its balance of efficiency and performance. If high-stability optimization is required, TRPO is preferred despite its computational cost. DPO and GRPO, however, are better suited for LLM fine-tuning, with GRPO providing an even stronger optimization framework based on relative preference ranking rather than just binary preference signals.\\nKey Takeaways\\nReinforcement learning (RL) plays a crucial role in both game-playing agents and LLM fine-tuning, but the optimization techniques vary significantly.\\nPG, TRPO, and PPO are fundamental in RL, with PPO being the most practical choice for its efficiency and performance balance.\\nDPO introduced a major shift in LLM fine-tuning by eliminating explicit reward models, making human preference alignment easier and more efficient.\\nGRPO, pioneered by DeepSeek, further refines LLM fine-tuning by optimizing for relative ranking rather than just binary comparisons, improving preference-based alignment.\\nFor RL tasks, PPO remains the dominant method, while for LLM fine-tuning, DPO and GRPO are superior choices due to their ability to fine-tune models using direct preference data without RL instability.\\nThis blog highlights how reinforcement learning and preference-based fine-tuning are converging, with new techniques like GRPO bridging the gap between structured optimization and real-world deployment of large-scale AI systems.\\nThe media shown in this article is not owned by Analytics Vidhya and is used at the Author’s discretion.\\nFrequently Asked Questions\\nQ1. What is the difference between PPO and DPO?\\nAns. PPO (Proximal Policy Optimization) is an RL-based optimization method that improves policies while maintaining stability using a clipping mechanism. It is widely used in reinforcement learning tasks such as robotics and game-playing AI. DPO (Direct Preference Optimization), on the other hand, is designed specifically for LLM fine-tuning, directly optimizing the model based on human preferences without requiring an explicit reward model. DPO is simpler and more efficient for aligning language models with human intent.\\nQ2. Why is GRPO better than DPO for preference-based fine-tuning?\\nAns. GRPO (Group Relative Policy Optimization) improves upon DPO by optimizing preferences in a ranked manner instead of binary preference signals. While DPO only differentiates between “preferred” and “rejected” responses, GRPO assigns relative rankings across multiple responses, capturing nuanced differences in preference. This allows LLMs to learn more refined distinctions and align better with human feedback.\\nQ3. When should I use TRPO over PPO?\\nAns. TRPO (Trust Region Policy Optimization) should be used when strict stability constraints are required, such as in high-stakes RL environments (e.g., robotics, autonomous driving). However, it is computationally expensive due to second-order optimization. PPO (Proximal Policy Optimization) provides a more efficient and scalable alternative by approximating TRPO’s constraints using a clipping mechanism, making it the preferred choice in most RL scenarios.\\nQ4. Why do LLMs need preference optimization techniques like DPO and GRPO?\\nAns. Traditional RL methods focus on maximizing numerical rewards, which do not always align with human expectations in language models. DPO and GRPO fine-tune LLMs based on human preference data, ensuring responses are helpful, honest, and harmless. Unlike Reinforcement Learning with Human Feedback (RLHF), these methods eliminate the need for a separate reward model, making fine-tuning more efficient and reducing potential biases from reward misalignment.\\nNeil D\\nAdvancing language model research by day and writing about my work online by night. I explore AI breakthroughs and transform complex studies into clear, engaging insights that empower professionals and enthusiasts alike.\\nThanks for stopping by my profile!\\nAdvanced\\nBest of Tech\\nLLMs\\nReinforcement Learning\\nFree Courses\\n4.7\\nGenerative AI - A Way of Life\\nExplore Generative AI for beginners: create text and images, use top AI tools, learn practical skills, and ethics.\\n4.5\\nGetting Started with Large Language Models\\nMaster Large Language Models (LLMs) with this course, offering clear guidance in NLP and model training made simple.\\n4.6\\nBuilding LLM Applications using Prompt Engineering\\nThis free course guides you on building LLM apps, mastering prompt engineering, and developing chatbots with enterprise data.\\n4.8\\nImproving Real World RAG Systems: Key Challenges & Practical Solutions\\nExplore practical solutions, advanced retrieval strategies, and agentic RAG systems to improve context, relevance, and accuracy in AI-driven applications.\\n4.7\\nMicrosoft Excel: Formulas & Functions\\nMaster MS Excel for data analysis with key formulas, functions, and LookUp tools in this comprehensive course.\\nResponses From Readers\\nCancel reply\\nClear\\nSubmit reply\\nΔ\\nWrite for us\\nWrite, captivate, and earn accolades and rewards for your work\\nReach a Global Audience\\nGet Expert Feedback\\nBuild Your Brand & Audience\\nCash In on Your Knowledge\\nJoin a Thriving Community\\nLevel Up Your Data Science Game\\nWe use cookies essential for this site to function well. Please click to help us improve its usefulness with additional cookies. Learn about our use of cookies in our\\nPrivacy Policy\\n&\\nCookies Policy\\n.\\nShow details\\nAccept all cookies\\nUse necessary cookies\\nPowered By\\nCookies\\nThis site uses cookies to ensure that you get the best experience possible. To learn more about how we use cookies, please refer to our\\nPrivacy Policy\\n&\\nCookies Policy\\n.\\nNecessary (2)\\nNecessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies.\\nAnalytics Vidhya (4)\\nlearn more about analytics vidhya privacy\\nbrahmaid\\nIt is needed for personalizing the website.\\nExpiry: Session\\nType: HTTP\\ncsrftoken\\nThis cookie is used to prevent Cross-site request forgery (often abbreviated as CSRF) attacks of the website\\nExpiry: Session\\nType: HTTPS\\nIdentityid\\nPreserves the login/logout state of users across the whole site.\\nExpiry: Session\\nType: HTTPS\\nsessionid\\nPreserves users\\' states across page requests.\\nExpiry: Session\\nType: HTTPS\\nGoogle (1)\\nlearn more about google privacy\\ng_state\\nGoogle One-Tap login adds this g_state cookie to set the user status on how they interact with the One-Tap modal.\\nExpiry: 365 days\\nType: HTTP\\nStatistics (4)\\nStatistic cookies help website owners to understand how visitors interact with websites by collecting and reporting information anonymously.\\nMicrosoft (7)\\nlearn more about microsoft policy\\nMUID\\nUsed by Microsoft Clarity, to store and track visits across websites.\\nExpiry: 1 Year\\nType: HTTP\\n_clck\\nUsed by Microsoft Clarity, Persists the Clarity User ID and preferences, unique to that site, on the browser. This ensures that behavior in subsequent visits to the same site will be attributed to the same user ID.\\nExpiry: 1 Year\\nType: HTTP\\n_clsk\\nUsed by Microsoft Clarity, Connects multiple page views by a user into a single Clarity session recording.\\nExpiry: 1 Day\\nType: HTTP\\nSRM_I\\nCollects user data is specifically adapted to the user or device. The user can also be followed outside of the loaded website, creating a picture of the visitor\\'s behavior.\\nExpiry: 2 Years\\nType: HTTP\\nSM\\nUse to measure the use of the website for internal analytics\\nExpiry: 1 Years\\nType: HTTP\\nCLID\\nThe cookie is set by embedded Microsoft Clarity scripts. The purpose of this cookie is for heatmap and session recording.\\nExpiry: 1 Year\\nType: HTTP\\nSRM_B\\nCollected user data is specifically adapted to the user or device. The user can also be followed outside of the loaded website, creating a picture of the visitor\\'s behavior.\\nExpiry: 2 Months\\nType: HTTP\\nGoogle (7)\\nlearn more about google privacy\\n_gid\\nThis cookie is installed by Google Analytics. The cookie is used to store information of how visitors use a website and helps in creating an analytics report of how the website is doing. The data collected includes the number of visitors, the source where they have come from, and the pages visited in an anonymous form.\\nExpiry: 399 Days\\nType: HTTP\\n_ga_#\\nUsed by Google Analytics, to store and count pageviews.\\nExpiry: 399 Days\\nType: HTTP\\n_gat_#\\nUsed by Google Analytics to collect data on the number of times a user has visited the website as well as dates for the first and most recent visit.\\nExpiry: 1 Day\\nType: HTTP\\ncollect\\nUsed to send data to Google Analytics about the visitor\\'s device and behavior. Tracks the visitor across devices and marketing channels.\\nExpiry: Session\\nType: PIXEL\\nAEC\\ncookies ensure that requests within a browsing session are made by the user, and not by other sites.\\nExpiry: 6 Months\\nType: HTTP\\nG_ENABLED_IDPS\\nuse the cookie when customers want to make a referral from their gmail contacts; it helps auth the gmail account.\\nExpiry: 2 Years\\nType: HTTP\\ntest_cookie\\nThis cookie is set by DoubleClick (which is owned by Google) to determine if the website visitor\\'s browser supports cookies.\\nExpiry: 1 Year\\nType: HTTP\\nWebengage (2)\\nLearn more about webengage privacy\\n_we_us\\nthis is used to send push notification using webengage.\\nExpiry: 1 Year\\nType: HTTP\\nWebKlipperAuth\\nused by webenage to track auth of webenagage.\\nExpiry: Session\\nType: HTTP\\nLinkedIn (16)\\nlearn more about linkedin privacy\\nln_or\\nLinkedin sets this cookie to registers statistical data on users\\' behavior on the website for internal analytics.\\nExpiry: 1 Day\\nType: HTTP\\nJSESSIONID\\nUse to maintain an anonymous user session by the server.\\nExpiry: 1 Year\\nType: HTTP\\nli_rm\\nUsed as part of the LinkedIn Remember Me feature and is set when a user clicks Remember Me on the device to make it easier for him or her to sign in to that device.\\nExpiry: 1 Year\\nType: HTTP\\nAnalyticsSyncHistory\\nUsed to store information about the time a sync with the lms_analytics cookie took place for users in the Designated Countries.\\nExpiry: 6 Months\\nType: HTTP\\nlms_analytics\\nUsed to store information about the time a sync with the AnalyticsSyncHistory cookie took place for users in the Designated Countries.\\nExpiry: 6 Months\\nType: HTTP\\nliap\\nCookie used for Sign-in with Linkedin and/or to allow for the Linkedin follow feature.\\nExpiry: 6 Months\\nType: HTTP\\nvisit\\nallow for the Linkedin follow feature.\\nExpiry: 1 Year\\nType: HTTP\\nli_at\\noften used to identify you, including your name, interests, and previous activity.\\nExpiry: 2 Months\\nType: HTTP\\ns_plt\\nTracks the time that the previous page took to load\\nExpiry: Session\\nType: HTTP\\nlang\\nUsed to remember a user\\'s language setting to ensure LinkedIn.com displays in the language selected by the user in their settings\\nExpiry: Session\\nType: HTTP\\ns_tp\\nTracks percent of page viewed\\nExpiry: Session\\nType: HTTP\\nAMCV_14215E3D5995C57C0A495C55%40AdobeOrg\\nIndicates the start of a session for Adobe Experience Cloud\\nExpiry: Session\\nType: HTTP\\ns_pltp\\nProvides page name value (URL) for use by Adobe Analytics\\nExpiry: Session\\nType: HTTP\\ns_tslv\\nUsed to retain and fetch time since last visit in Adobe Analytics\\nExpiry: 6 Months\\nType: HTTP\\nli_theme\\nRemembers a user\\'s display preference/theme setting\\nExpiry: 6 Months\\nType: HTTP\\nli_theme_set\\nRemembers which users have updated their display / theme preferences\\nExpiry: 6 Months\\nType: HTTP\\nPreferences (0)\\nPreference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in.\\nWe do not use cookies of this type.\\nMarketing (4)\\nMarketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers.\\nGoogle (11)\\nlearn more about google privacy\\n_gcl_au\\nUsed by Google Adsense, to store and track conversions.\\nExpiry: 3 Months\\nType: HTTP\\nSID\\nSave certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search.\\nExpiry: 2 Years\\nType: HTTP\\nSAPISID\\nSave certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search.\\nExpiry: 2 Years\\nType: HTTP\\n__Secure-#\\nSave certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search.\\nExpiry: 2 Years\\nType: HTTP\\nAPISID\\nSave certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search.\\nExpiry: 2 Years\\nType: HTTP\\nSSID\\nSave certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search.\\nExpiry: 2 Years\\nType: HTTP\\nHSID\\nSave certain preferences, for example the number of search results per page or activation of the SafeSearch Filter. Adjusts the ads that appear in Google Search.\\nExpiry: 2 Years\\nType: HTTP\\nDV\\nThese cookies are used for the purpose of targeted advertising.\\nExpiry: 6 Hours\\nType: HTTP\\nNID\\nThese cookies are used for the purpose of targeted advertising.\\nExpiry: 1 Month\\nType: HTTP\\n1P_JAR\\nThese cookies are used to gather website statistics, and track conversion rates.\\nExpiry: 1 Month\\nType: HTTP\\nOTZ\\nAggregate analysis of website visitors\\nExpiry: 6 Months\\nType: HTTP\\nFacebook (2)\\nlearn more about facebook privacy\\n_fbp\\nThis cookie is set by Facebook to deliver advertisements when they are on Facebook or a digital platform powered by Facebook advertising after visiting this website.\\nExpiry: 4 Months\\nType: HTTP\\nfr\\nContains a unique browser and user ID, used for targeted advertising.\\nExpiry: 2 Months\\nType: HTTP\\nLinkedIn (6)\\nLearn about linkedin policy\\nbscookie\\nUsed by LinkedIn to track the use of embedded services.\\nExpiry: 1 Year\\nType: HTTP\\nlidc\\nUsed by LinkedIn for tracking the use of embedded services.\\nExpiry: 1 Day\\nType: HTTP\\nbcookie\\nUsed by LinkedIn to track the use of embedded services.\\nExpiry: 6 Months\\nType: HTTP\\naam_uuid\\nUse these cookies to assign a unique ID when users visit a website.\\nExpiry: 6 Months\\nType: HTTP\\nUserMatchHistory\\nThese cookies are set by LinkedIn for advertising purposes, including: tracking visitors so that more relevant ads can be presented, allowing users to use the \\'Apply with LinkedIn\\' or the \\'Sign-in with LinkedIn\\' functions, collecting information about how visitors use the site, etc.\\nExpiry: 6 Months\\nType: HTTP\\nli_sugr\\nUsed to make a probabilistic match of a user\\'s identity outside the Designated Countries\\nExpiry: 90 Days\\nType: HTTP\\nMicrosoft (2)\\nLearn more about microsoft privacy.\\nMR\\nUsed to collect information for analytics purposes.\\nExpiry: 1 year\\nType: HTTP\\nANONCHK\\nUsed to store session ID for a users session to ensure that clicks from adverts on the Bing search engine are verified for reporting purposes and for personalisation\\nExpiry: 1 Day\\nType: HTTP\\nUnclassNameified (0)\\nUnclassNameified cookies are cookies that we are in the process of classNameifying, together with the providers of individual cookies.\\nWe do not use cookies of this type.\\nCookie declaration last updated on 24/03/2023 by Analytics Vidhya.\\nCookies are small text files that can be used by websites to make a user\\'s experience more efficient. The law states that we can store cookies on your device if they are strictly necessary for the operation of this site. For all other types of cookies, we need your permission. This site uses different types of cookies. Some cookies are placed by third-party services that appear on our pages. Learn more about who we are, how you can contact us, and how we process personal data in our\\nPrivacy Policy\\n.\\nAccept all cookies\\nUse necessary cookies\\nFlagship Courses\\nGenAI Pinnacle Program\\n|\\nGenAI Pinnacle Plus Program\\n|\\nAI/ML BlackBelt Courses\\n|\\nAgentic AI Pioneer Program\\nFree Courses\\nGenerative AI\\n|\\nLarge Language Models\\n|\\nBuilding LLM Applications using Prompt Engineering\\n|\\nBuilding Your first RAG System using LlamaIndex\\n|\\nStability.AI\\n|\\nMidJourney\\n|\\nBuilding Production Ready RAG systems using LlamaIndex\\n|\\nBuilding LLMs for Code\\n|\\nDeep Learning\\n|\\nPython\\n|\\nMicrosoft Excel\\n|\\nMachine Learning\\n|\\nDecision Trees\\n|\\nPandas for Data Analysis\\n|\\nEnsemble Learning\\n|\\nNLP\\n|\\nNLP using Deep Learning\\n|\\nNeural Networks\\n|\\nLoan Prediction Practice Problem\\n|\\nTime Series Forecasting\\n|\\nTableau\\n|\\nBusiness Analytics\\nPopular Categories\\nGenerative AI\\n|\\nPrompt Engineering\\n|\\nGenerative AI Application\\n|\\nNews\\n|\\nTechnical Guides\\n|\\nAI Tools\\n|\\nInterview Preparation\\n|\\nResearch Papers\\n|\\nSuccess Stories\\n|\\nQuiz\\n|\\nUse Cases\\n|\\nListicles\\nGenerative AI Tools and Techniques\\nGANs\\n|\\nVAEs\\n|\\nTransformers\\n|\\nStyleGAN\\n|\\nPix2Pix\\n|\\nAutoencoders\\n|\\nGPT\\n|\\nBERT\\n|\\nWord2Vec\\n|\\nLSTM\\n|\\nAttention Mechanisms\\n|\\nDiffusion Models\\n|\\nLLMs\\n|\\nSLMs\\n|\\nStyleGAN\\n|\\nEncoder Decoder Models\\n|\\nPrompt Engineering\\n|\\nLangChain\\n|\\nLlamaIndex\\n|\\nRAG\\n|\\nFine-tuning\\n|\\nLangChain AI Agent\\n|\\nMultimodal Models\\n|\\nRNNs\\n|\\nDCGAN\\n|\\nProGAN\\n|\\nText-to-Image Models\\n|\\nDDPM\\n|\\nDocument Question Answering\\n|\\nImagen\\n|\\nT5 (Text-to-Text Transfer Transformer)\\n|\\nSeq2seq Models\\n|\\nWaveNet\\n|\\nAttention Is All You Need (Transformer Architecture)\\nPopular GenAI Models\\nLlama 3.1\\n|\\nLlama 3\\n|\\nLlama 2\\n|\\nGPT 4o Mini\\n|\\nGPT 4o\\n|\\nGPT 3\\n|\\nClaude 3 Haiku\\n|\\nClaude 3.5 Sonnet\\n|\\nPhi 3.5\\n|\\nPhi 3\\n|\\nMistral Large 2\\n|\\nMistral NeMo\\n|\\nMistral-7b\\n|\\nGemini 1.5 Pro\\n|\\nGemini Flash 1.5\\n|\\nBedrock\\n|\\nVertex AI\\n|\\nDALL.E\\n|\\nMidjourney\\n|\\nStable Diffusion\\nData Science Tools and Techniques\\nPython\\n|\\nR\\n|\\nSQL\\n|\\nJupyter Notebooks\\n|\\nTensorFlow\\n|\\nScikit-learn\\n|\\nPyTorch\\n|\\nTableau\\n|\\nApache Spark\\n|\\nMatplotlib\\n|\\nSeaborn\\n|\\nPandas\\n|\\nHadoop\\n|\\nDocker\\n|\\nGit\\n|\\nKeras\\n|\\nApache Kafka\\n|\\nAWS\\n|\\nNLP\\n|\\nRandom Forest\\n|\\nComputer Vision\\n|\\nData Visualization\\n|\\nData Exploration\\n|\\nBig Data\\n|\\nCommon Machine Learning Algorithms\\n|\\nMachine Learning\\nSKIP\\nContinue your learning for FREE\\nLogin with Google\\nLogin with Email\\nForgot your password?\\nI accept the\\nTerms and Conditions\\nReceive updates on WhatsApp\\nEnter email address to continue\\nEmail address\\nGet OTP\\nEnter OTP sent to\\nEdit\\nEnter the OTP\\nResend OTP\\nResend OTP in\\n45s\\nVerify OTP', 'image_urls': [{'url': 'https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/image-7-2.webp', 'score': 2}, {'url': 'https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/DPO_OP4.webp', 'score': 2}, {'url': 'https://cdn.analyticsvidhya.com/wp-content/uploads/2025/02/GRPO_Eq4.webp', 'score': 1}], 'title': 'A Deep Dive into LLM Optimization: From Policy Gradient to GRPO'}, {'url': 'https://www.linkedin.com/pulse/grpo-game-changer-deepseeks-llms-sandeep-k-dzx2f', 'raw_content': 'GRPO: A Game-Changer for DeepSeek’s LLMs\\nAgree & Join LinkedIn\\nBy clicking Continue to join or sign in, you agree to LinkedIn’s\\nUser Agreement\\n,\\nPrivacy Policy\\n, and\\nCookie Policy\\n.\\nSign in to view more content\\nCreate your free account or sign in to continue your search\\nSign in\\nWelcome back\\nEmail or phone\\nPassword\\nShow\\nForgot password?\\nSign in\\nor\\nBy clicking Continue to join or sign in, you agree to LinkedIn’s\\nUser Agreement\\n,\\nPrivacy Policy\\n, and\\nCookie Policy\\n.\\nNew to LinkedIn?\\nJoin now\\nor\\nNew to LinkedIn?\\nJoin now\\nBy clicking Continue to join or sign in, you agree to LinkedIn’s\\nUser Agreement\\n,\\nPrivacy Policy\\n, and\\nCookie Policy\\n.\\nLinkedIn\\nLinkedIn is better on the app\\nDon’t have the app? Get it in the Microsoft Store.\\nOpen the app\\nSkip to main content\\nIntroduction\\nArtificial Intelligence (AI) has made incredible strides in recent years, particularly in the field of\\nLarge Language Models (LLMs)\\n. These models are trained on massive datasets and fine-tuned with sophisticated algorithms to improve reasoning, problem-solving, and understanding. However, a major challenge has been improving\\nreasoning capabilities\\nefficiently while minimizing training costs.\\nDeepSeek, a leading AI research team, has introduced a groundbreaking technique called\\nGroup Relative Policy Optimization (GRPO)\\n, which dramatically enhances LLM reasoning ability through reinforcement learning. GRPO powers\\nDeepSeek-R1-Zero\\nand\\nDeepSeek-R1\\n, two advanced reasoning models that rival industry leaders like OpenAI’s\\no1-1217\\nseries.\\nThis article will break down:\\nWhat GRPO is and why it matters\\nHow GRPO differs from other LLM training methods\\nHow GRPO impacts DeepSeek\\'s model performance\\nWhy GRPO could change the future of AI model training\\nWhat is Group Relative Policy Optimization (GRPO)?\\nGRPO is an advanced reinforcement learning (RL) technique designed to enhance LLMs\\' reasoning capabilities while reducing training costs\\n.\\nTraditional RL methods for LLMs, such as\\nProximal Policy Optimization (PPO)\\n, require a\\ncritic model\\n—a separate AI system that evaluates responses and assigns rewards. However, critic models are\\nas large as the policy model\\n, making training expensive and resource-intensive.\\nGRPO eliminates the need for a separate critic model.\\nInstead, it estimates rewards based on\\ngroups of responses\\n, making training more efficient and scalable.\\nHow GRPO Works (Explained Simply)\\nSampling\\n: The model generates multiple possible answers to a question.\\nGrouping\\n: These answers are grouped together and ranked based on correctness and clarity.\\nOptimization\\n: Instead of relying on a separate critic model, GRPO compares the responses within the group and adjusts the model\\'s policy accordingly.\\nThis approach enables the model to\\nself-learn and refine its reasoning skills\\nover multiple iterations without requiring\\nexternal human labeling or critic models\\n.\\nHow GRPO Works: A Technical Breakdown\\nFor those interested in the mathematics behind GRPO, the optimization function follows this principle:\\nJ_GRPO(θ) = E[ Σ ( min( π_θ(o_i | q) / π_θ_old(o_i | q)\\nA_i, clip( π_θ(o_i | q) / π_θ_old(o_i | q), 1 - ε, 1 + ε )\\nA_i ) - β D_KL( π_θ || π_ref ) ) ]\\nHere’s what the terms mean:\\nG is the number of generated responses.\\nπ_θ(o_i | q) is the probability of an output o_i given a question q.\\nA_i is the advantage function, computed as:\\nA_i = (r_i - mean({r_1, r_2, ... , r_G})) / std({r_1, r_2, ... , r_G})\\nRecommended by LinkedIn\\nArtificial General Intelligence: Breaking Down the…\\nKanerika Inc\\n3 weeks ago\\nAI-Powered Solutions: Beyond the Hype\\nMediusware - Global Outstaffing Partner\\n3 weeks ago\\nGenAI Tools and their Applications\\nAdvanceWorks\\n1 year ago\\nThis formula means that instead of requiring an external evaluator, the model\\ncompares its own generated responses within a group and updates itself\\nbased on their relative rankings.\\nHow Does GRPO Compare to Other Reinforcement Learning Methods?\\nCompared to traditional methods like PPO and RLHF, GRPO offers a unique balance between efficiency and effectiveness.\\nPPO (Proximal Policy Optimization) uses a separate critic model, making it computationally expensive. RLHF (Reinforcement Learning with Human Feedback) requires extensive human labeling, increasing training costs. Monte Carlo Tree Search (MCTS) can deliver excellent results but is extremely slow and resource-intensive.\\nGRPO, on the other hand, does not require a critic model or human feedback, significantly reducing training costs while improving reasoning ability. This makes it a highly scalable approach for training LLMs.\\nHow Much Does GRPO Improve Model Performance?\\nDeepSeek\\'s models trained with GRPO\\nachieve industry-leading results\\nin reasoning tasks. Here are some notable performance improvements:\\nOn the AIME 2024 math competition benchmark, DeepSeek-R1-Zero increased accuracy from\\n15.6% to 71.0%\\n, and after additional training, DeepSeek-R1 reached\\n79.8% accuracy\\n.\\nOn the MATH-500 benchmark, DeepSeek-R1 achieved\\n97.3% accuracy\\n, surpassing most other open models.\\nIn competitive coding, DeepSeek-R1 reached a\\n96.3 percentile ranking\\non Codeforces, a leading competitive programming platform.\\nFor general knowledge tasks like MMLU, DeepSeek-R1 achieved\\n90.8% accuracy\\n, making it one of the top-performing open models.\\nThese improvements are\\ngame-changing\\n. For example, GRPO boosted math performance from\\n15.6% to 71.0%\\n, a nearly\\n5x increase\\nin accuracy.\\nWhy GRPO is a Game-Changer for AI and LLMs\\nEnables Self-Learning AI Models\\nGRPO allows models to\\nself-improve\\nwithout needing human feedback or expensive training data. This is a major breakthrough for\\nautonomous reasoning\\n.\\nMakes Training More Efficient\\nSince GRPO doesn’t require a\\ncritic model\\n, it reduces computational costs. This means even smaller research teams can train powerful AI models.\\nProduces Smarter, More Flexible AI\\nDeepSeek-R1 models trained with GRPO show\\nemergent reasoning behaviors\\nlike\\nself-reflection and verification\\n. These behaviors are critical for developing\\nArtificial General Intelligence (AGI)\\n.\\nDemocratizes AI Research\\nDeepSeek has open-sourced\\nDeepSeek-R1-Zero\\nand\\ndistilled models (1.5B to 70B parameters)\\n, allowing developers worldwide to build on this breakthrough.\\nConclusion: The Future of AI with GRPO\\nGRPO represents a\\nmajor leap forward\\nin AI model training. By eliminating the need for expensive critic models, DeepSeek has built an\\nefficient, scalable, and powerful reasoning system\\n.\\nWith\\nDeepSeek-R1\\n, AI is moving closer to\\nautonomous problem-solving\\n, and GRPO is leading the way. If more AI researchers adopt this method, we could see a future where LLMs\\nself-train and evolve independently\\n, unlocking\\nnew levels of intelligence\\n.\\nWhat do you think about GRPO? Could it revolutionize AI model training? Let’s discuss!\\nFollow me for more AI insights and LLM breakdowns!\\nLike\\nComment\\nCopy\\nLinkedIn\\nFacebook\\nTwitter\\nShare\\n11\\n2 Comments\\nVISHWADHARANI EVR\\nStudent at Thiagarajar College of Engineering\\n5d\\nReport this comment\\nThanks for sharing it Sir , Really useful..\\nLike\\nReply\\n1\\xa0Reaction\\nGoutam Ghosh\\nSenior Technology Director/Senior Salesforce Technical Architect/ Business Process Re-Engineering Lead\\n4w\\nReport this comment\\nUseful tips\\nLike\\nReply\\n1\\xa0Reaction\\n2\\xa0Reactions\\nSee more comments\\nTo view or add a comment,\\nsign in\\nMore articles by Sandeep K\\nFrom CodeForces to IOI Gold: How AI is Winning the Coding Wars\\nFeb 12, 2025\\nFrom CodeForces to IOI Gold: How AI is Winning the Coding Wars\\nCompetitive programming has long been a gold standard for testing human intelligence in coding and problem-solving. But…\\n7\\nJamba: The AI Model That Could Redefine the Future of Large Language Models\\nFeb 4, 2025\\nJamba: The AI Model That Could Redefine the Future of Large Language Models\\nIntroduction: A New Era in AI The artificial intelligence landscape is evolving rapidly, and AI21 Labs has just…\\n6\\nWho is Beating DeepSeek and How?\\nJan 31, 2025\\nWho is Beating DeepSeek and How?\\nAlibaba’s Qwen 2.5-Max is making waves in the AI world, outperforming DeepSeek-V3 and even rivaling GPT-4o and Claude 3.\\n10\\n2 Comments\\nWhy DeepSeek costed so less?\\nJan 31, 2025\\nWhy DeepSeek costed so less?\\nIntroduction The training of large language models (LLMs) has traditionally been associated with exorbitant costs…\\n4\\nZero shot, few shot and RAG benchmaking of Deep Seek\\nJan 29, 2025\\nZero shot, few shot and RAG benchmaking of Deep Seek\\n1. Zero-Shot Learning Strengths: Autonomous Reasoning: DeepSeek-R1’s RL-driven training incentivizes self-generated…\\n19\\n1 Comment\\nDeepSeek: A Groundbreaking Leap in Reasoning-Driven AI\\nJan 28, 2025\\nDeepSeek: A Groundbreaking Leap in Reasoning-Driven AI\\nLarge Language Models (LLMs) have rapidly become foundational tools across industries, revolutionizing natural language…\\n22\\nKey Trends That Will Shape the Future of Generative AI in 2025:\\nNov 27, 2024\\nKey Trends That Will Shape the Future of Generative AI in 2025:\\nAs we move into 2025, Generative AI is set to redefine industries, workflows, and human-machine collaboration in ways…\\n9\\n1 Comment\\n\"AI Agents: Revolutionizing Automation with LLMs\"\\nOct 21, 2024\\n\"AI Agents: Revolutionizing Automation with LLMs\"\\nUnlocking the Power of AI Agents: How Large Language Models (LLMs) Transform Automation and What Companies Need to Know…\\n16\\n1 Comment\\nFrom Language Models to Artificial General Intelligence: The Transformative Pote\\nJul 14, 2024\\nFrom Language Models to Artificial General Intelligence: The Transformative Pote\\n𝐈𝐧𝐭𝐫𝐨𝐝𝐮𝐜𝐭𝐢𝐨𝐧: In the rapidly evolving field of artificial intelligence, the emergence of large language…\\n37\\n2 Comments\\nThe Art of Generative AI: A Playful Guide to Harnessing Innovation in Organizations\\nMar 26, 2024\\nThe Art of Generative AI: A Playful Guide to Harnessing Innovation in Organizations\\nIn the ever-evolving landscape of technology, Generative AI emerges as a whimsical wizard, ready to weave magic into…\\n4\\nShow more\\nSee all articles\\nSign in\\nStay updated on your professional world\\nSign in\\nBy clicking Continue to join or sign in, you agree to LinkedIn’s\\nUser Agreement\\n,\\nPrivacy Policy\\n, and\\nCookie Policy\\n.\\nNew to LinkedIn?\\nJoin now\\nInsights from the community\\nArtificial Intelligence\\nHow can you become an expert in explainable AI?\\nMachine Learning\\nHow can you explain AI to stakeholders without technical knowledge?\\nArtificial Intelligence\\nHow can you improve AI system accuracy?\\nTechnological Innovation\\nHow can you collaborate with AI experts and developers?\\nArtificial Intelligence\\nHere\\'s how you can present AI solutions to clients with confidence.\\nArtificial Intelligence\\nWhat are the best practices for using transformers to generate text in AI?\\nArtificial Intelligence\\nHow can you demonstrate creativity in AI?\\nArtificial Intelligence\\nHow can AI algorithms balance model complexity and interpretability?\\nGenerative AI\\nWhat are the best practices and tools for face swapping and voice cloning with generative AI?\\nArtificial Intelligence\\nYou’re trying to evaluate the effectiveness of an AI model. How do you know if it’s really working?\\nShow more\\nShow less\\nOthers also viewed\\nThe rise of machine learning: how generative AI works\\nQuantik\\n1y\\nDemystifying AI: What AI Is Not\\nEdgeMethods\\n1y\\nAhead of AI #6: TrAIn Differently\\nSebastian Raschka, PhD\\n1y\\nHow AI is Changing the World and How You Can Benefit from It\\nConstantech - AI For Business\\n1y\\nCracking the Code of GenAI: Insights from a Developer\\'s Lens\\nEngineer\\'s Planet\\n7mo\\nA leader\\'s guide to GenAI technicals - What is AI\\nSparkbit\\n10mo\\nWeekly Artificial Intelligence Newsletter\\nCo-one\\n2y\\nArtificial Intelligence: Understanding the Fundamentals\\nArbutus Infotech Private Limited\\n1y\\nYour Daily AI Research tl;dr | 2022-06-02\\nWhat\\'s AI by Louis-François Bouchard\\n2y\\nAgent Chaos: How AI Models Are Spiraling into Collapse\\nGanesh Raju\\n6mo\\nShow more\\nShow less\\nExplore topics\\nSales\\nMarketing\\nIT Services\\nBusiness Administration\\nHR Management\\nEngineering\\nSoft Skills\\nSee All', 'image_urls': [], 'title': 'GRPO: A Game-Changer for DeepSeek’s LLMs'}, {'url': 'https://medium.com/@yuvrajsagar117/understanding-grpo-key-ingredient-behind-deepseek-r1s-success-5f8f05494e6d', 'raw_content': 'Understanding GRPO: Key ingredient behind Deepseek-R1’s Success | by Yuvraj Sagar | Feb, 2025 | Medium\\nOpen in app\\nSign up\\nSign in\\nWrite\\nSign up\\nSign in\\nUnderstanding GRPO: Key ingredient behind Deepseek-R1’s Success\\nYuvraj Sagar\\n·\\nFollow\\n9 min read\\n·\\nFeb 12, 2025\\n--\\nListen\\nShare\\nIn my previous blog\\nDeepseek-R1 Explained\\n, I explored How\\nDeepseek-R1\\nwas developed, their innovation and techniques they utilized to train such a performant model. One of the key breakthroughs they made was\\nGroup Relative Policy Optimization (GRPO) —\\na large-scale reinforcement learning (RL) algorithm\\nspecifically designed to enhance reasoning capabilities in Large Language Models (LLMs). In this blog, we will be covering how GRPO works and how we can implement it for any given LLM to\\nenhance their reasoning capabilities\\n. If you want to find out how Deepseek team utilized this technique for their model, you can visit my\\nDeepseek-R1 Explained\\nblog to find out more. So, let’s get started with GRPO.\\nQuick Note:\\nFor the first part, I will be providing a quick recap about how Preference Optimizations are applied to any LLM, if you know about these, you can skip this part, and directly move to GRPO section.\\nHow LLMs are trained?\\nGenerally, there are three major stages in which LLMs are trained, as:\\nPretraining\\n— To educate the model from raw text data capturing patterns, grammar and general knowledge. At this stage, we only have base models serving as next-token predictors used for Chat-Completion purposes only.\\nSupervised Finetuning (SFT)\\n— Making base-models to perform well on downstream tasks using a labeled dataset — Helpful assistants.\\nPreference Alignment\\n— To shape these assistants’ responses to follow a desired format of answers to avoid bias, toxicity and harmfullness.\\nFig: How LLMs are trained —\\nSource\\nIf you’re interested in a deep dive into Finetuning and Preference Alignment, I highly recommend checking out\\nMaxime Labonne’s Blogs\\n. His work explores practical techniques for fine-tuning LLMs and how we can customize them for specific use cases, making them more effective and safer.\\nA Quick Look at Preference Alignment\\nPreference alignment is crucial for ensuring that LLMs generate responses that align with human expectations — making them more\\nhelpful, safe, and controllabl\\ne. This is typically achieved through\\nReinforcement Learning from Human Feedback (RLHF)\\nand its evolving alternatives like\\nDPO, IPO, ORPO and KTO.\\nRLHF\\nIn RLHF, we provide LLMs with multiple answers, which are ranked based on desired behaviors like helpfulness, safety, or low toxicity using human preferences. Instead of directly mimicking human preferences, RLHF uses a\\nreward model\\nto approximate human judgment, which then guides policy optimization via reinforcement learning algorithm like PPO. The model learns to prioritize the best-ranked responses, effectively mimicking the desired behavior. Often seen as a way to “censor” models.\\nRLHF got popularized for LLMs after the release of OpenAI’s paper\\nFine-Tuning Language Models from Human Preferences\\n. In the paper they provided a framework which involves:\\nTraining a\\nreward model\\nto approximate human feedback on model outputs.\\nUsing this reward model to fine-tune the LLM’s policy via reinforcement learning algorithms like\\nPPO (Proximal Policy Optimization)\\n.\\nFig: Training Process for Reward Model & Policy —\\nSource\\nWhile RLHF has been effective, it can be unstable, computationally expensive, and difficult to reproduce due to its sensitivity to hyperparameters and random seeds.\\nPPO\\nPPO is an\\nactor-critic algorithm\\nused in RLHF to fine-tune the LLM’s policy. It works by optimizing a policy\\n(the actor)\\nwhile using a value function\\n(the critic)\\nto estimate the expected rewards. It achieves it by maximizing a surrogate objective function, that is a combination of policy loss and value loss along, given by:\\nFig: Surrogate Loss Function for PPO\\nThe Actor-Critic algorithm is a reinforcement learning algorithm that uses two neural networks to improve an agent’s performance over time. The actor decided which action should be taken and critic inform the actor how good was the action and how it should adjust.\\nA nice illustration about PPO, give in GRPO paper is as follows\\nFig: PPO training —\\nSource\\nWe will talk about this diagram in contrast with GRPO.\\nDespite its effectiveness, PPO can still be unstable (e.g., loss divergence) and computationally demanding, which has led to the development of alternatives like\\nDPO\\n. For more details regarding PPO, refer to the\\nPPO paper\\n.\\nDPO\\nDirect Preference Optimization(DPO), another RL algorithm that gained popularity in the landscape of LLMs due to its effectiveness. It simplifies alignment by reframing RLHF as a\\nclassification problem\\n. Instead of training a separate reward model, DPO directly optimizes the policy using a\\nreference model\\n(a snapshot of the initial policy) and a binary cross-entropy loss.\\nDuring training, DPO ensures the trained model assigns higher probabilities to preferred answers and lower probabilities to rejected answers compared to the reference model. This is achieved using a\\nbinary cross-entropy objective\\n, which directly optimizes the model based on human preferences without requiring a separate reward model or extensive sampling.\\nDPO is more stable, efficient, and computationally less demanding than RLHF with PPO, making it an attractive alternative. For more details regarding DPO, refer to their paper\\nDirect Preference Optimization: Your Language Model is Secretly a Reward Model\\n. If you want to understand more about DPO and its variants like KTO, CPO, and IPO, give a read to this comprehensive paper\\nInsights into Alignment: Evaluating DPO and its Variants Across Multiple Tasks\\nby Amir Saeidi et. al 2024.\\nBuilding on PPO,\\nGRPO\\ntakes alignment a step further by refining the model’s reasoning capabilities. Let’s dive into GRPO.\\nGRPO\\nGRPO, an efficient and effective reinforcement learning (RL) algorithm built on Proximal Policy Optimization (PPO) (Schulman et al., 2017). GRPO foregoes the critic model and instead estimates the baseline from\\ngroup scores\\n, significantly reducing training resources compared to Proximal Policy Optimization (PPO). The following diagram from the paper illustrates it effectively.\\nFig: Comparison between PPO and GRPO’s key innovation\\nExplanation:\\nNo Need for a Value Model:\\nThe value model is eliminated. Instead, GRPO uses the\\naverage reward of multiple outputs\\ngenerated for the same question as the baseline.\\nThis simplifies the architecture and reduces resource requirements\\n. In PPO, a separate\\nvalue model (critic)\\nis required to estimate the expected cumulative reward for each state. This adds significant computational and memory overhead.\\nMultiple Outputs for a Given Question Processed Simultaneously:\\nFor each question,\\nmultiple outputs\\nare sampled simultaneously from the policy model. These outputs are processed as a\\ngroup\\n, and their rewards are used to compute a relative baseline. This approach aligns better with how reward models are trained (on comparisons between outputs) and provides a\\nmore robust estimate of performance\\n. In PPO, only one output is generated and evaluated at a time for a given question and the advantage\\nAi\\nis calculated based on the reward of that single output.\\nComputing Grouped Advantage Scores Using Normalization:\\nThe advantages <\\nA\\nt,i>\\nis computed\\nrelative to the group\\nof outputs as:\\nThe\\nrewards\\nof all outputs in the group are\\nnormalized\\n.\\nThe advantage for each output is calculated based on its\\nrelative performance\\nwithin the group.\\nThis\\ngroup-relative advantage\\nis simpler to compute and aligns well with the comparative nature of reward models.\\nIn PPO, the advantage <\\nA\\nt,i>\\nis calculated using\\nGeneralized Advantage Estimation (GAE)\\n, which relies on the value model’s predictions and can be complex to compute.\\nWhy these Changes Matter?\\nEfficiency\\n: By removing the value model and processing multiple outputs simultaneously, GRPO significantly reduces computational and memory costs.\\nAlignment with Reward Models\\n: GRPO’s group-based approach naturally fits how reward models are trained (on comparisons between outputs), making the training process more intuitive and effective.\\nSimplified Advantage Calculation\\n: The group-relative advantage computation is simpler and more robust, as it doesn’t rely on a separate value model.\\nHow GRPO Works:\\nNow, we will look how GRPO generates completions and helps to improve the model iteratively. An amazing illustration about this is given by HuggingFace Team\\nvisit here\\n. A brief summary is as follows:\\nGRPO\\nis an\\nonline learning algorithm\\nmeaning\\nit improves iteratively by using the data generated by the trained model itself during training\\n. It maximizes the advantage of generated completions while ensuring the model stays close to a reference policy. How GRPO works can be broken into four key steps as follows:\\nFig: How GRPO works? —\\nSource\\n1. Generating Completions:\\nAt each training step, a batch of prompts is sampled.\\nFor each prompt,\\nG completions\\nare generated using the current policy model. These completions are denoted as\\noi\\n\\u200b.\\n2. Computing the Advantage\\nFor each completion, a\\nreward\\nis computed using a reward model.\\nTo align with the comparative nature of reward models (trained on dataset of comparisons different output of a given question), the\\nadvantage\\nis calculated\\nrelative to the group of completions\\n. It is normalized as:\\nThis\\ngroup-relative advantage\\ngives GRPO its name.\\n3. Estimating KL Divergence\\nThe\\nKL divergence\\nbetween the current policy\\nπθ\\n\\u200b\\nand the reference policy\\nπref\\nis estimated using an unbiased approximator introduced by\\nSchulman et al. (2020)\\n, defined as follows:\\nThis ensures the policy doesn’t deviate too far from the reference model.\\n4. Computing the Loss\\nThe\\nGRPO loss\\ncombines the advantage and KL divergence:\\nThe first term maximizes the\\nscaled advantage\\n.\\nThe second term penalizes deviations from the reference policy using\\nKL divergence\\n.\\nTo ensure stable updates, the policy ratio (\\nπθ/\\nπθ\\nold)\\n\\u200b\\u200b is clipped between 1−\\nϵ\\nand 1+\\nϵ\\n.\\nGRPO in Action:\\nAs highlighted in my earlier blog,\\nDeepseek-R1\\nwhich uses GRPO as RL process and multi-stage finetuning steps, achieves performance comparable to\\nOpenAI-o1\\nacross\\nmath, code, and reasoning tasks\\n. Also see in the figure below how the results changed from\\nDeepseek-V3\\n(the base model used for creation of R1) and R1 model, we can see significant performance improvement as well, all thanks to GRPO and carefully curated chain-of-thought reasoning dataset by Deepseek-team.\\nFig: GRPO in action. Deepseek-R1 benchmark results —\\nSource\\nConclusion & Remarks:\\nReinforcement Learning has undeniably become the backbone of modern LLM training, evolving from the foundational RLHF framework to more nuanced approaches like PPO, DPO, and GRPO. Each algorithm refines the balance between stability, efficiency, and alignment in its own way:\\nRLHF with PPO\\nlaid the groundwork, using reward models and policy clipping to align models with human preferences.\\nDPO\\nreimagined alignment as a classification problem, bypassing reward models entirely for simplicity.\\nGRPO\\ntook PPO’s robustness further by eliminating the critic model and introducing\\ngroup-based reward normalization\\n— a stroke of elegance that mirrors how humans naturally compare outputs.\\nWhile innovations like GRPO push the boundaries of RL-driven alignment, it’s worth remembering that LLM performance also hinges on high-quality data, scalable infrastructure, and clever training strategies (topics for another day! 😊).\\nIf you enjoyed this deep dive, check out the references below or explore my earlier posts on RL fundamentals. Thanks for reading — feel free to leave a clap, a comment, or a critique. Let’s keep the conversation going! 🚀\\nReferences:\\n[1] —\\nDeepSeekMath:\\nPushing the Limits of Mathematical Reasoning in Open Language Models from\\narxiv\\n[2] —\\nDeepSeek-R1:\\nIncentivizing Reasoning Capability in LLMs via Reinforcement Learning from\\narxiv\\n[3] —\\nFine-tune Mistral-7b with Direct Preference Optimization by\\nMaxime Labonne\\n[4] —\\nFine-Tuning Language Models from Human Preferences from\\narxiv\\n[5] —\\nProximal Policy Optimization Algorithms from\\narxiv\\n[6] —\\nDirect Preference Optimization:\\nYour Language Model is Secretly a Reward Model from\\narxiv\\n[7] —\\nGRPOTrainer:\\nHow GRPO works from\\nHuggingFace TRL Docs\\nThank you for reading!\\nFeel free to connect with me on\\nLinkedIn\\nor\\nX\\n.\\nLarge Language Models\\nDeepseek R1\\nRlhf\\nGrpo\\nFollow\\nWritten by\\nYuvraj Sagar\\n11 Followers\\n·\\n9 Following\\nDeep Learning Engineer | Learning by sharing.\\nFollow\\nNo responses yet\\nHelp\\nStatus\\nAbout\\nCareers\\nPress\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams', 'image_urls': [], 'title': 'Understanding GRPO: Key ingredient behind Deepseek-R1’s Success | by Yuvraj Sagar | Feb, 2025 | Medium'}, {'url': 'https://opdeepseek.com/deepseek-moe/', 'raw_content': 'DeepSeek-MoE Power Of Mixture of Experts Framework\\nSkip to content\\nDeepSeek-MoE\\nis a large language model (LLM) with 16.4B parameters. It is based on the Mixture-of-Experts (MoE) architecture, which allows it to achieve high performance with relatively low computational cost. Inshort, It’s a model that leverages the Mixture of Experts (MoE) framework, which allows it to perform complex tasks efficiently. Curious about how it works? Well, the Mixture of Experts framework assigns tasks to specialized expert networks, each one focuses on a specific area.To give the most accurate output A gating mechanism decides which expert is best suited for a specific task. To better understand the concept we will go through this page and simplify the basic concept.\\nWhat is DeepSeek-MoE?\\nDeepSeek\\nis a large language model built on the Mixture of Experts framework. If we\\ncompare\\nit then It is not like other traditional models that activate all their parameters for every task, MoE selectively activates only the necessary experts for a given input. This makes it highly efficient in terms of computation and specialization. Essentially, DeepSeek optimizes performance while keeping resource consumption in check, a balance that’s difficult to achieve with conventional architectures.\\nThe backend working based on this division makes work faster, even smarter and capable of handling a number of inputs in a more efficient way.\\nBasic Concept Of Mixture Of Expert Framework\\nTo put it simply, Mixture of Experts is like having a team of specialists instead of a jack-of-all-trades. Instead of relying on a single model to process all kinds of information, MoE breaks tasks into smaller parts and assigns them to specialized sub-models called experts. A gating network acts as a decision-maker, directing inputs to the most relevant experts. This results in a more efficient and precise output.\\nThink of it like visiting a hospital. Instead of one doctor handling everything, you have cardiologists, neurologists, and orthopedic specialists who focus on their respective fields. This targeted approach ensures better diagnosis and treatment, just as MoE enhances AI performance. And the best part?\\nDeepSeek is free\\nto use, so you won’t be charged for any additional feature.\\nDeepSeek–MoE Architecture\\nDeepSeek AI\\nselectively activates only a few experts, significantly reducing computational overhead. The model also incorporates techniques to balance workload distribution across experts, preventing any single expert from being overburdened. This balance ensures that the system remains stable and doesn’t suffer from inefficiencies due to uneven task allocation.\\nApplications of DeepSeek-MoE\\nThe applications of DeepSeek-MoE span multiple domains. It excels in natural language processing, offering better text generation, summarization, and translation capabilities. In finance, it aids in predictive analytics, risk assessment, and fraud detection. Healthcare also benefits from this technology, as it helps in diagnosing diseases by analyzing vast medical datasets.\\nDeepSeek Mixture-of-expert\\narchitecture applies the MoE framework in a way that maximizes both scalability and accuracy. By leveraging specialized experts, it can perform a diverse range of tasks with remarkable precision. Whether it’s language modeling, data analysis, or predictive analytics, DeepSeek intelligently assigns tasks to the most capable expert networks.\\nThe beauty of this system is its\\nadaptability\\n. It can continuously learn and adjust, so that as new data emerges, the right experts are engaged. This means that\\nDeepSeek\\nisn’t just efficient, it’s also highly dynamic and responsive to different types of input provided to it.\\nBenefits of DeepSeek-MoE Architecture\\nEnhanced Parameter Efficiency\\nOne of the standout benefits of this architecture is its ability to utilize parameters efficiently. Unlike traditional models that activate all parameters, MoE selectively engages only the necessary ones. This means that computational resources are used more effectively, leading to faster processing speeds and reduced energy consumption.\\nMitigation of Redundancy\\nBecause experts are specialized, there is far less redundancy in DeepSeek compared to conventional AI models. Each expert focuses on a specific area, eliminating unnecessary computations and making the system more streamlined. This results in faster execution and more accurate outcomes.\\nHigher Expert Specialization\\nSpecialization is what makes DeepSeek so powerful. Each expert network is trained on specific types of data, ensuring that they become highly proficient in their domain. This focused learning approach leads to better performance, as opposed to models that try to do everything at once without true mastery in any area.\\nImproved Load Balancing\\nWith a well-structured gating network, DeepSeek ensures that computational loads are distributed evenly across experts. This prevents bottlenecks and optimizes performance, making the model both scalable and reliable.\\nFlexibility in Knowledge Acquisition\\nDeepSeek isn’t static, it evolves. The model can integrate new knowledge seamlessly, allowing it to adapt to changing information landscapes. Whether it’s learning new languages, adapting to new research findings, or improving predictive capabilities, DeepSeek MoE remains flexible and future-proof.\\nThe Future of DeepSeek-MoE\\nLooking ahead,\\nDeepSeek-MoE\\nhas the potential to redefine AI applications. As technology advances, we can expect even more efficient architectures, further reducing computational costs while improving accuracy. AI models like DeepSeek will likely become more integrated into everyday applications, making advanced AI accessible to businesses and individuals alike.\\nThe challenge will be refining MoE frameworks to further minimize latency and ensure fair expert utilization. But with continuous research and development, DeepSeek is poised to remain at the forefront of AI innovation.\\nYou can also explore the\\nkey features of DeepSeek\\nand if you are curious to know\\nis DeepSeek safe\\n.\\nFAQs\\nHow does DeepSeek differ from traditional AI models?\\nDeepSeek uses a Mixture of Experts framework, selectively activating only relevant experts instead of processing every input with a full model, making it more efficient.\\nWhat makes the DeepSeek-Mixture of Experts framework unique?\\nIt ensures specialization among different expert networks, reducing redundancy and improving precision while maintaining scalability.\\nWhat industries benefit the most from this AI model?\\nIndustries like finance, healthcare, research, and natural language processing benefit from its precision and efficiency.\\nHow does DeepSeek handle complex queries?\\nIts gating network assigns queries to the most suitable experts, ensuring high accuracy and reduced processing time.\\nLeave a Comment\\nCancel reply\\nComment\\nName\\nEmail\\nWebsite\\nSave my name, email, and website in this browser for the next time I comment.', 'image_urls': [{'url': 'https://opdeepseek.com/wp-content/uploads/2025/02/Add-a-subheading-9.webp', 'score': 2}, {'url': 'https://opdeepseek.com/wp-content/uploads/2025/02/unnamed-1024x696.webp', 'score': 1}, {'url': 'https://opdeepseek.com/wp-content/uploads/2025/02/Untitled-design-2025-02-03T235815.160-1024x576.webp', 'score': 1}], 'title': 'DeepSeek-MoE Power Of Mixture of Experts Framework'}, {'url': 'https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/', 'raw_content': \"Why GRPO is Important and How it Works\\nLast week on Arxiv Dives we dug into research\\nbehind DeepSeek-R1\\n, and uncovered that one of the techniques they use in the their training pipeline is called Group Relative Policy Optimization (GRPO).\\nAt it’s core, GRPO is a Reinforcement Learning (RL) algorithm that is aimed at improving the model’s reasoning ability. It was first introduced in their paper\\nDeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\\n, but was also used in the post-training of\\nDeepSeek-R1\\n.\\nThe process to go from DeepSeek’s base pre-trained language model to a reasoning model was laid out in detail in the\\nDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\\npaper.\\nLast week we didn’t get too deep into the math or process behind GRPO or look at any code, so today the goal is to fully understand what is going on in GRPO and help apply it to your own work.\\n💡\\nNote\\nGreg spends full workdays diving into research papers and writing the Arxiv Dives. He does this to empower engineers to hack on their own AI and use Oxen.AI to set up their data. If you like this blog, try\\nOxen.AI\\ntoday and we'd love to hear your feedback:)\\nRecap: How R1 Used GRPO\\nAs a recap - the full pipeline for improving DeepSeek’s base model to the reasoning model alternates between using Supervised Fine Tuning (SFT) and Group Relative Policy Optimization (GRPO).\\nSupervised Fine Tuning (SFT)\\nCold start the training with\\nhigh quality data\\nA couple thousand examples verified by humans\\nReinforcement Learning w/ GRPO\\nTrain the model to have reasoning traces <reasoning></reasoning>\\nDeterministic rewards for formatting, consistency, and correctness\\nSupervised Fine Tuning (SFT)\\nGenerate 800k Synthetic SFT data points and reject and filter\\nLLM As A Judge to filter incorrect responses\\nReinforcement Learning w/ GRPO\\nAlign the model to be helpful and harmless\\nIn this post we will dive into the details of GRPO to give you a sense of how it works and where you can apply it to training your own models. I’ve been doing some experiments training smaller models with GRPO and will follow up with a post on implementation details and code examples to help tie together all the concepts.\\nWhy is GRPO Important?\\nTLDR ~ The compute requirements drop significantly and it simplifies the RL. It just about cuts in half the compute requirements to do Reinforcement Learning from Human Feedback (RLHF) compared to what was used for ChatGPT (PPO). When you start considering LoRAs in the mix, this unlocks RL training for even the poorest of the GPU poor, and I got news for you. I tried it. And it works.\\nI was able to successfully turn a 1B parameter Llama 3.2 model into a reasoning model with 16GB of VRAM.\\nMore on that in a subsequent post where I’ll share the code and hardware requirements.\\n💡 I was able to successfully turn a 1B parameter Llama 3.2 model into a reasoning model with 16GB of VRAM.\\nBasically we can all train reasoning models from our garages by spending < $100 on cloud GPU services. Or essentially “for free” if you are talking smol models on your own hardware. How does this work in under the hood? The next section will talk through the evolution from PPO to GRPO.\\nFrom PPO to GRPO\\nThe Reinforcement Learning (RL) technique behind ChatGPT is rumored to be PPO which stands from Proximal Policy Optimization. The process was laid out in the\\nInstructGPT paper\\nto create a model that can follow instructions and go beyond simple predicting the next word.\\nThe training process requires you to collect a lot of labeled data. For a given user query, you have the model generate multiple candidate responses. Then you have a human or AI in the loop to label and rank the outputs form best to worst. This can then be used as training data for a “reward model” who’s job is to calculate a “reward” for a new prompt that it sees. The reward should represent how good this response is, given the user query.\\nAfter you have collected all this ranked and labeled data you can kick off PPO to train your LLM.\\nThe problem is that PPO can be pretty expensive to train. The diagram from the\\nGRPO paper\\nshows all the different LLMs that are in the mix during PPO and GRPO. There are 4 different LLMs in the blue and yellow boxes below.\\nTo help demystify some of the lingo above, here are my quick definitions:\\nPolicy Model - Fancy name for the current LLM you are training\\nReference Model - A frozen version of the original LLM you are training\\nReward Model - The model that was trained on human preferences (from the technique in InstructGPT above)\\nValue Model - A model that is trying to estimate the long term reward given certain actions\\nReducing Memory Usage with GRPO\\nIn PPO both the policy model and the value model have trainable parameters that need to be back-propagated through. Backprop requires a significant amount of memory. If you look at the diagram above, GRPO drops the value model.\\nPPO has 4 LLMs in the mix, which all require substantial memory and compute. The value and reward models are typically of a comparable parameter count to the LLM you are training. The reference model is usually a frozen copy of the initial language model.\\nNot only is this computationally expensive, there are a lot of moving parts and multiple models you are optimizing. The more moving parts, typically the harder it is to optimize. GRPO helps simplify things.\\nFor fun, I decided to try a bunch of different model sizes on an H100 and see how easy it was to fine tune with GRPO\\nIf you want all the technical details check them out here:\\n🧠 GRPO VRAM Requirements For the GPU Poor | Oxen.ai\\nSince the release of DeepSeek-R1, Group Relative Policy Optimization (GRPO) has become the talk of the town for Reinforcement Learning in Large Language Models due to its effectiveness and ease of training. The R1 paper demonstrated how you can use GRPO to go from a base instruction following LLM (DeepSeek-v3) to a reasoning model (DeepSeek-R1). To learn more about instruction following, reasoning models, and the full DeepSeek-R1 model, I suggest you checkout some of our other deep dives. How\\nOxen.ai\\nIf you understand where all the system requirements comes from, you can start to contribute to open source or help optimize your own libraries like this most recent PR I saw on the trl repo:\\nGroup Relative Advantages\\nThe main signal you are trying to get out of your LLMs during RL is represented by “A” which stands for the “Advantage”. This helps give direction to update the original LLM’s weights. If the the advantage is high, you want to encourage the model to keep doing the same actions. If it is low, you want to encourage the model to try something different.\\nIn PPO, the value model’s original job is to try to estimate how good the tokens that are generated are, or how likely they are to give a high reward. In order to do this well, it required you to train large LLM to make these value judgements. So how does GRPO remove the need for this?\\nThe first trick is that instead of generating one output per query, GRPO starts by generating multiple outputs.\\nConcretely if the question is a math question the model might try a few different approaches to solve it. For example, if the question is:\\nMr. Curtis has 325 chickens on his farm where 28 are roosters and the rest are hens. Twenty hens do not lay eggs while the rest of the hens do. How many egg-laying hens does Mr. Curtis have on his farm?\\nThe model might come up with a few different reasoning traces, some correct (answer=227), and some incorrect (answer=305).\\nCorrect Output\\n<reasoning>First, let's find out how many hens there are. The total number of chickens is 325, and 28 are roosters. So, the number of hens is 325 - 28 = 297. Of these 297 hens, 20 do not lay eggs, so the number of egg-laying hens is 297 - 20 = 277.</reasoning>\\n<answer>277</answer>\\nIncorrect Output\\n<reasoning>You need to subtract the 20 hens that do not lay eggs from the total number of hens to find the number of egg-laying hens. So, the number of egg-laying hens is 325 - 20 = 305.</reasoning>\\n<answer>305</answer>\\nThen for each output, we calculate a “reward” for how well that output answers the query. There can be multiple reward functions that evaluate different properties of the response. We will leave the reward functions as a black box for now, but just know that they return some numeric value that is higher if the response is good, and lower if the response is bad.\\nRewards may look like:\\nFormatting = 1.0\\nAnswer = 0.0\\nConsistency = 0.5\\nOnce we have our set of rewards (r) given our outputs, GRPO calculates our “advantage” (A) by simply looking at the mean and standard deviation of all the rewards.\\nThis equation is pretty handy for feature engineering in machine learning in general. It helps normalize arbitrary values to more a more learnable positive or negative signal. The intuition is “how many standard deviations from the mean is the data point?”\\nLet’s look at some examples.\\n# o_0 = <reasoning>I have some reasoning</reasoning><answer>12</answer>\\nr_0 = 1.0 # o_1 = <reasoning></reasoning><answer>12</answer>\\nr_1 = 0.5 # o_2 = The answer is 312\\nr_2 = 0.0 # o_3 = <reason>I did not have valid formatting or answer.\\nr_3 = 0.0\\nIn raw numpy it would look something like this:\\nimport numpy as np advantages = [(r_i - np.mean(r)) / np.std(r) for r_i in r]\\nLet’s try it with another set of numbers:\\nrewards = [4.0, 2.5, 0.5, 0.1]\\nadvantage = [(r_i - np.mean(r)) / np.std(r) for r_i in r]\\n[1.4137674241360643, 0.4606657898870322, -0.8101363891116772, -1.064296824911419]\\nYou’ll notice the values center around 0.0 and tell you how good or bad the score is relative to all the other ones. This gives us a sort of baseline of “given this prompt, how good are the average responses going to be?”. Reward the good outputs, and penalize the bad ones in this batch.\\nThis is pretty similar to what the value model was originally trying to do: estimate what our reward will be given a response. Since the policy model we are training is a language model, we can just tweak the temperature and have generate multiple possible completions instead. Then the average reward for all these generations is a good signal for how well the current model is doing, and if we should reinforce the behavior.\\nKL-Divergence\\nThe final piece of the equation is the KL Divergence term.\\nWithout getting too deep into the math, this is why we have been keeping around a “reference model” during the training. The idea is that we do not want to drift too far from the original model. For each token, we want to make sure the new predictions do not drift too far from the original ones.\\nThe intuition behind enforcing the KL Divergence is that the model we are starting with already knows how to write coherent sentences and follow instructions. We don’t want the new model to “reward hack” or exploit some sort of property in our reward signal that is not aligned with the original model. If it finds out that saying the word “pamplemousse” gets a high reward because it is a rarer word (and fun one to say) we don’t want it latching onto this behavior if it was not common in the pre-training.\\nPut all this together and you have this final equation!\\nOr as our trusty Ox Eric says…The math looks more complicated than it is…\\nThe Reward Signals\\nWhat’s super interesting about the DeepSeek-R1-Zero work is that they go even further to slash the memory usage because don’t use a “neural reward model”.\\nWhat does this mean? It means they are literally using regexes and string matching for reward signals. They argue that this helps with “reward hacking” and simplifies the whole training pipeline.\\nIf you took the definitions in\\nAccuracy Rewards\\nand\\nFormat Rewards\\nsections above and turned it into code, it would look like this:\\nreference:\\nGRPO Llama-1B\\nGRPO Llama-1B. GitHub Gist: instantly share code, notes, and snippets.\\nGist\\n262588213843476\\nYou don’t even need a full reward model LLM in the loop during training. This leaves us with the policy model and the reference model as the main memory requirements. Dropping the need of 4 LLMs to 2 gives us the huge reduction in GPU requirements.\\nIf your spidey senses are tingling and asking “do these reward function really generalize?” you would be right. They work well for whatever you specify in the rewards, but not much else. In other words, with these two rewards the model does get good at following the <reasoning></reasoning><answer></answer> format and does gets good at reasoning through math, but it fails at other useful tasks.\\nMy prediction is that\\n“The Bitter Lesson”\\nwill strike again here. Given enough compute and data, models just want to learn. The less we hand code rules, and more we let the model learn on it’s own, the better it will perform. GRPO rewards here feel a little to hand coded. Why wouldn’t you just have the model learn the weights of the reward signals?\\nThat being said, it is kind of fun playing with different rewards. What’s cool about GRPO is that as long as you can define it in code as a function that returns a value given responses, you can optimize against it. This could even be an external API call to another LLM. I have a feeling in the coming weeks / month people are going to start getting creative with what rewards are fed into GRPO because it is so accessible to train.\\nNext Up: Training a Rust Reasoner 🦀\\nWhat does this look like in practice? The\\n“Transformer Reinforcement Learning”\\nor\\ntrl\\nlibrary already has GRPO implemented, and it’s relatively easy to pass in your own reward functions.\\nSome of the folks in our community are going to try to train a smol 1B param LLM optimized for Rust. I think there’s a lot of juice we could squeeze out of low resource programming languages to get models we could run locally.\\nThe idea would be this:\\nWe have already started collecting a dataset in an oxen repository that is a combination of synthetic data and stack overflow questions that we plan on training on.\\nox/Rust/filtered_data.jsonl at main\\nThis is a dataset of rust questions and generated code created to fine tune small language models on rust.. Contribute to the ox/Rust repository by creating an account on Oxen.ai\\nIf you’d like to join us, let us know in the\\nDiscord\\n!\\nAlso if you are working on LLMs or setting up your data flywheel, check out\\nOxen.ai\\n.\", 'image_urls': [{'url': 'https://ghost.oxen.ai/content/images/2025/02/2.png', 'score': 3}, {'url': 'https://ghost.oxen.ai/content/images/2025/02/5.png', 'score': 3}, {'url': 'https://ghost.oxen.ai/content/images/2025/02/6.png', 'score': 3}, {'url': 'https://ghost.oxen.ai/content/images/2025/02/8.png', 'score': 3}, {'url': 'https://ghost.oxen.ai/content/images/2025/02/3.png', 'score': 2}, {'url': 'https://ghost.oxen.ai/content/images/2025/02/4.png', 'score': 2}, {'url': 'https://ghost.oxen.ai/content/images/2025/02/7.png', 'score': 2}, {'url': 'https://ghost.oxen.ai/content/images/2025/02/11.png', 'score': 2}, {'url': 'https://ghost.oxen.ai/content/images/2025/02/Screenshot-2025-02-06-at-4.58.47-PM.png', 'score': 2}, {'url': 'https://ghost.oxen.ai/content/images/2025/02/18-1.png', 'score': 2}], 'title': 'Why GRPO is Important and How it Works'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cUxOITWwq2yi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}